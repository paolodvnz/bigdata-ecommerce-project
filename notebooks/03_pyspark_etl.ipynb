{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark ETL\n",
    "\n",
    "L'obiettivo di questo Notebook e' mostrare l'utilizzo di PySpark per l'analisi distribuita di Big Data, dall'introduzione all'architettura fino a pipeline ETL complete con Delta Lake.\n",
    "\n",
    "- **Dataset**: 100M transazioni e-commerce\n",
    "- **Storage**: MinIO (S3-compatible)\n",
    "- **Output**: Dati ML-ready in Delta Lake per Notebook 4, dati Avro per Notebook 5 (Streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & Environment\n",
    "\n",
    "Configurazione ambiente Spark con Delta Lake support.\n",
    "\n",
    "1. Importare librerie necessarie\n",
    "2. Inizializzare SparkSession con configurazione MinIO\n",
    "3. Definire helper functions\n",
    "4. Verificare connessione Delta Lake\n",
    "\n",
    "### Configurazione Spark\n",
    "- **App Name**: Notebook-03-PySpark\n",
    "- **Delta Lake**: Abilitato (2.4.0)\n",
    "- **MinIO**: Configurato come S3-compatible storage\n",
    "- **Driver Memory**: 10GB (local mode, unica JVM)\n",
    "- **Spark UI**: http://localhost:4040\n",
    "\n",
    "### MinIO Storage\n",
    "- **MinIO Console**: http://localhost:9001\n",
    "- **User/Password**: minioadmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "#from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if \"notebooks\" in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import configurations\n",
    "from config.spark_config import get_spark_session\n",
    "from config.minio_config import get_s3a_path, BUCKET_NAME\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece59b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinIO paths configured:\n",
      "  Transactions: s3a://bigdata-ecommerce/raw/transactions\n",
      "  Customers: s3a://bigdata-ecommerce/raw/customers.parquet\n",
      "  Products: s3a://bigdata-ecommerce/raw/products.parquet\n",
      "  User-Item Interactions for ML: s3a://bigdata-ecommerce/ml_data/user_item_interactions\n",
      "  Customer Features for ML: s3a://bigdata-ecommerce/ml_data/customer_features\n",
      "  Streaming Source: s3a://bigdata-ecommerce/streaming/source/transactions_avro\n"
     ]
    }
   ],
   "source": [
    "# PATHS CONFIGURATION\n",
    "\n",
    "# Output directories\n",
    "RESULTS_DIR = project_root / \"docs\" / \"results\"\n",
    "GRAPH_DIR = project_root / \"docs\" / \"graphs\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "GRAPH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# MinIO paths  \n",
    "transactions_path = get_s3a_path(\"raw/\", \"transactions\")\n",
    "customers_path = get_s3a_path(\"raw/\", \"customers.parquet\")\n",
    "products_path = get_s3a_path(\"raw/\", \"products.parquet\")\n",
    "als_delta_path = get_s3a_path(\"ml_data/\", \"user_item_interactions\")\n",
    "rf_delta_path = get_s3a_path(\"ml_data/\", \"customer_features\")\n",
    "avro_output = get_s3a_path(\"streaming/source/\", \"transactions_avro\")\n",
    "\n",
    "print(\"MinIO paths configured:\")\n",
    "print(f\"  Transactions: {transactions_path}\")\n",
    "print(f\"  Customers: {customers_path}\")\n",
    "print(f\"  Products: {products_path}\")\n",
    "print(f\"  User-Item Interactions for ML: {als_delta_path}\")\n",
    "print(f\"  Customer Features for ML: {rf_delta_path}\")\n",
    "print(f\"  Streaming Source: {avro_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6283643e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "def timer_decorator(func):\n",
    "    \"\"\"\n",
    "    Decorator to measure execution time of functions.\n",
    "\n",
    "    Args:\n",
    "        func: Function to time.\n",
    "\n",
    "    Returns:\n",
    "        Wrapped function with timing output.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"\\n    {func.__name__} completed in {duration:.2f} seconds\")\n",
    "        return result, duration\n",
    "    return wrapper\n",
    "\n",
    "def count_with_time(df, description=\"Operation\"):\n",
    "    \"\"\"\n",
    "    Count DataFrame rows with timing.\n",
    "\n",
    "    Args:\n",
    "        df: Spark DataFrame.\n",
    "        description: Operation description for display.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (count, duration_seconds)\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    n = df.count()\n",
    "    duration = time.time() - start\n",
    "    print(f\"    {description}: {n:,} rows in {duration:.2f}s\")\n",
    "    return n, duration\n",
    "\n",
    "def show_partitions(df, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Display partition information.\n",
    "\n",
    "    Args:\n",
    "        df: Spark DataFrame.\n",
    "        name: DataFrame name for display.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of partitions.\n",
    "    \"\"\"\n",
    "    num_partitions = df.rdd.getNumPartitions()\n",
    "    print(f\"{name} partitions: {num_partitions}\")\n",
    "    return num_partitions\n",
    "\n",
    "def save_results(data, filename):\n",
    "    \"\"\"\n",
    "    Save results to CSV in docs/results/.\n",
    "\n",
    "    Args:\n",
    "        data: Dictionary or pandas DataFrame.\n",
    "        filename: Output filename.\n",
    "    \"\"\"\n",
    "    filepath = RESULTS_DIR / filename\n",
    "    if isinstance(data, dict):\n",
    "        pd.DataFrame([data]).to_csv(filepath, index=False)\n",
    "    else:\n",
    "        data.to_csv(filepath, index=False)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 JAR files\n",
      "Spark Session created: Notebook-03-PySpark\n",
      "Spark Version: 3.4.1\n",
      "Spark UI: http://localhost:4040\n",
      "Delta Lake + MinIO: Enabled\n",
      "\n",
      "Key Spark Configurations:\n",
      "  spark.app.name: Notebook-03-PySpark\n",
      "  spark.driver.memory: 10g\n",
      "  spark.sql.shuffle.partitions: 24\n",
      "  spark.default.parallelism: 12\n",
      "  spark.memory.fraction: 0.6\n",
      "  spark.sql.adaptive.enabled: true\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE SPARK SESSION\n",
    "\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "\n",
    "spark = get_spark_session(\n",
    "    app_name=\"Notebook-03-PySpark\",\n",
    "    enable_delta=True\n",
    ")\n",
    "\n",
    "# Display key configuration\n",
    "print(\"\\nKey Spark Configurations:\")\n",
    "conf = spark.sparkContext.getConf()\n",
    "important_configs = [\n",
    "    \"spark.app.name\",\n",
    "    \"spark.driver.memory\",\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.default.parallelism\",\n",
    "    \"spark.memory.fraction\",\n",
    "    \"spark.sql.adaptive.enabled\"\n",
    "]\n",
    "for config in important_configs:\n",
    "    print(f\"  {config}: {conf.get(config, 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinIO Storage Screenshots\n",
    "\n",
    "**MinIO Data**\n",
    "\n",
    "<img src=\"./screenshots/03_minio_data_1.png\" alt=\"MinIo Data\" >\n",
    "\n",
    "**Partizioni Transaction**\n",
    "\n",
    "<img src=\"./screenshots/03_minio_data_2.png\" alt=\"MinIo Data Transaction\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architettura PySpark\n",
    "\n",
    "PySpark utilizza la **lazy evaluation**: le trasformazioni non vengono eseguite immediatamente ma vengono registrate in un **DAG** (Directed Acyclic Graph). Il **Catalyst Optimizer** analizza il grafo per trovare il piano di esecuzione piu' efficiente, che viene avviato solo quando si invoca un'action.\n",
    "\n",
    "**Transformations**\n",
    "\n",
    "Creano nuovi DataFrame senza eseguire calcoli immediati. Si dividono in due categorie:\n",
    "\n",
    "1. **Narrow transformations**: operano su singole partizioni, non richiedono shuffle e network\n",
    "    - `select()`, `filter()`, `withColumn()`\n",
    "\n",
    "2. **Wide transformations**: riorganizzano dati tra partizioni, richiedono shuffle e network\n",
    "    - `groupBy()`, `join()`, `orderBy()`\n",
    "\n",
    "**Actions**\n",
    "\n",
    "Triggherano l'esecuzione del piano computazionale e ritornano risultati concreti all'utente.\n",
    "- `count()`, `collect()`, `show()`\n",
    "\n",
    "\n",
    "Due principi fondamentali caratterizzano PySpark:\n",
    "\n",
    "1. **Immutability**: ogni DataFrame e' immutabile e ogni trasformazione genera un nuovo DataFrame\n",
    "\n",
    "2. **Distributed Computing**: partiziona i dati su piu' executors permettendo l'esecuzione parallela delle operazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOAD DATA\n",
      "======================================================================\n",
      "\n",
      "Customers: 1,000,000 rows\n",
      "Products: 50,000 rows\n",
      "Transactions: 100,000,000 rows\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOAD DATA\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "customers = spark.read.parquet(customers_path)\n",
    "products = spark.read.parquet(products_path)\n",
    "transactions = spark.read.parquet(transactions_path)\n",
    "\n",
    "print(f\"Customers: {customers.count():,} rows\")\n",
    "print(f\"Products: {products.count():,} rows\")\n",
    "print(f\"Transactions: {transactions.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LAZY EVALUATION DEMO\n",
      "======================================================================\n",
      "\n",
      "1. Creating transformation (lazy)...\n",
      "   Transformation defined (no execution yet)\n",
      "\n",
      "2. Triggering action (execution happens now)...\n",
      "   Executed! Found 8,233,328 high-value transactions\n",
      "   Execution time: 3.79s\n"
     ]
    }
   ],
   "source": [
    "# LAZY EVALUATION DEMO\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LAZY EVALUATION DEMO\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Create transformation (no execution yet)\n",
    "print(\"1. Creating transformation (lazy)...\")\n",
    "filtered_transactions = transactions.filter(F.col(\"final_amount\") > 1000)\n",
    "print(\"   Transformation defined (no execution yet)\")\n",
    "\n",
    "# Trigger action (execution happens now)\n",
    "print(\"\\n2. Triggering action (execution happens now)...\")\n",
    "start = time.time()\n",
    "result_count = filtered_transactions.count()\n",
    "duration = time.time() - start\n",
    "\n",
    "print(f\"   Executed! Found {result_count:,} high-value transactions\")\n",
    "print(f\"   Execution time: {duration:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRANSFORMATION EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "1. SELECT transformation:\n",
      "   Selected 3 columns from customers\n",
      "\n",
      "2. FILTER transformation:\n",
      "   Filtered VIP customers (lazy)\n",
      "\n",
      "3. WITHCOLUMN transformation:\n",
      "   Added margin_pct column (lazy)\n",
      "\n",
      "4. GROUPBY transformation (with agg):\n",
      "   Grouped by segment (lazy)\n",
      "\n",
      "======================================================================\n",
      "ACTIONS EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "1. COUNT action:\n",
      "   VIP customers: 50,000 (executed in 0.20s)\n",
      "\n",
      "2. SHOW action:\n",
      "   First 5 VIP customers:\n",
      "+-----------+--------------------+---+\n",
      "|customer_id|                name|age|\n",
      "+-----------+--------------------+---+\n",
      "|  C00000017|       Aria Giannini| 26|\n",
      "|  C00000032|Gelsomina Renzi-P...| 26|\n",
      "|  C00000042|  Sig.ra Michela Emo| 63|\n",
      "|  C00000062|  Gustavo Spanevello| 31|\n",
      "|  C00000066|Sig.ra Lara Randazzo| 67|\n",
      "+-----------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "3. COLLECT action:\n",
      "   Collected 3 rows to driver\n",
      "   - C00000017: Aria Giannini\n",
      "   - C00000032: Gelsomina Renzi-Piane\n",
      "   - C00000042: Sig.ra Michela Emo\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMATION & ACTIONS EXAMPLES\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRANSFORMATION EXAMPLES\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# 1. Select\n",
    "print(\"1. SELECT transformation:\")\n",
    "customer_basics = customers.select(\"customer_id\", \"name\", \"customer_segment\")\n",
    "print(\"   Selected 3 columns from customers\")\n",
    "\n",
    "# 2. Filter\n",
    "print(\"\\n2. FILTER transformation:\")\n",
    "vip_customers = customers.filter(F.col(\"customer_segment\") == \"VIP\")\n",
    "print(\"   Filtered VIP customers (lazy)\")\n",
    "\n",
    "# 3. WithColumn\n",
    "print(\"\\n3. WITHCOLUMN transformation:\")\n",
    "transactions_with_margin = transactions.withColumn(\n",
    "    \"margin_pct\",\n",
    "    F.round((F.col(\"final_amount\") / F.col(\"total_amount\")) * 100, 2)\n",
    ")\n",
    "print(\"   Added margin_pct column (lazy)\")\n",
    "\n",
    "# 4. GroupBy (transformation, not action!)\n",
    "print(\"\\n4. GROUPBY transformation (with agg):\")\n",
    "segment_stats = customers.groupBy(\"customer_segment\").agg(\n",
    "    F.count(\"*\").alias(\"count\"),\n",
    "    F.avg(\"age\").alias(\"avg_age\")\n",
    ")\n",
    "print(\"   Grouped by segment (lazy)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ACTIONS EXAMPLES\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# 1. Count\n",
    "print(\"1. COUNT action:\")\n",
    "start = time.time()\n",
    "vip_count = vip_customers.count()\n",
    "duration = time.time() - start\n",
    "print(f\"   VIP customers: {vip_count:,} (executed in {duration:.2f}s)\")\n",
    "\n",
    "# 2. Show\n",
    "print(\"\\n2. SHOW action:\")\n",
    "print(\"   First 5 VIP customers:\")\n",
    "vip_customers.select(\"customer_id\", \"name\", \"age\").show(5)\n",
    "\n",
    "# 3. Collect\n",
    "print(\"\\n3. COLLECT action:\")\n",
    "top_3_vip = vip_customers.select(\"customer_id\", \"name\").limit(3).collect()\n",
    "print(f\"   Collected {len(top_3_vip)} rows to driver\")\n",
    "for row in top_3_vip:\n",
    "    print(f\"   - {row['customer_id']}: {row['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATAFRAME OPERATIONS\n",
      "======================================================================\n",
      "\n",
      "1. DataFrame Info:\n",
      "   Columns: 13\n",
      "   Column names: ['customer_id', 'name', 'email', 'phone', 'address', 'city', 'region', 'country', 'postal_code', 'registration_date', 'customer_segment', 'age', 'gender']\n",
      "   Partitions: 12\n",
      "\n",
      "2. Schema:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- registration_date: date (nullable = true)\n",
      " |-- customer_segment: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "\n",
      "3. Describe (summary statistics):\n",
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|           1000000|\n",
      "|   mean|         46.506383|\n",
      "| stddev|16.733604371767242|\n",
      "|    min|                18|\n",
      "|    max|                75|\n",
      "+-------+------------------+\n",
      "\n",
      "\n",
      "4. Distinct values:\n",
      "   Customer segments:\n",
      "+----------------+\n",
      "|customer_segment|\n",
      "+----------------+\n",
      "|         Regular|\n",
      "|      Occasional|\n",
      "|             VIP|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DATAFRAME OPERATIONS\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATAFRAME OPERATIONS\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Basic info\n",
    "print(\"1. DataFrame Info:\")\n",
    "print(f\"   Columns: {len(customers.columns)}\")\n",
    "print(f\"   Column names: {customers.columns}\")\n",
    "print(f\"   Partitions: {customers.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Schema\n",
    "print(\"\\n2. Schema:\")\n",
    "customers.printSchema()\n",
    "\n",
    "# Describe\n",
    "print(\"\\n3. Describe (summary statistics):\")\n",
    "customers.select(\"age\").describe().show()\n",
    "\n",
    "# Distinct\n",
    "print(\"\\n4. Distinct values:\")\n",
    "segments = customers.select(\"customer_segment\").distinct()\n",
    "print(\"   Customer segments:\")\n",
    "segments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUERY EXECUTION PLAN\n",
      "======================================================================\n",
      "\n",
      "Chained operations:\n",
      "  1. Filter completed transactions\n",
      "  2. Filter amount > 500\n",
      "  3. Select specific columns\n",
      "  4. Add high_value column\n",
      "  5. Order by amount\n",
      "  6. Limit to top 10\n",
      "\n",
      "Result:\n",
      "+--------------+-----------+------------+----------------+----------+\n",
      "|transaction_id|customer_id|final_amount|transaction_date|high_value|\n",
      "+--------------+-----------+------------+----------------+----------+\n",
      "|   T0098657043|  C00472985|     8997.85|      2024-04-29|       Yes|\n",
      "|   T0025697854|  C00291574|     8994.05|      2024-09-18|       Yes|\n",
      "|   T0075933682|  C00968650|     8991.83|      2025-07-31|       Yes|\n",
      "|   T0088081874|  C00633627|     8982.91|      2025-03-26|       Yes|\n",
      "|   T0084912906|  C00797292|     8979.11|      2025-10-28|       Yes|\n",
      "|   T0077918434|  C00794352|     8975.02|      2025-08-03|       Yes|\n",
      "|   T0057242121|  C00663478|     8974.71|      2024-03-19|       Yes|\n",
      "|   T0030245510|  C00101347|     8964.92|      2024-12-09|       Yes|\n",
      "|   T0000997695|  C00138589|     8958.15|      2024-04-04|       Yes|\n",
      "|   T0029100685|  C00123143|     8956.97|      2024-01-30|       Yes|\n",
      "+--------------+-----------+------------+----------------+----------+\n",
      "\n",
      "\n",
      "Spark optimizes this chain before execution (Catalyst optimizer)\n",
      "\n",
      "PHYSICAL PLAN (what to do):\n",
      "----------------------------------------------------------------------\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=10, orderBy=[final_amount#56 DESC NULLS LAST], output=[transaction_id#48,customer_id#49,final_amount#56,transaction_date#58,high_value#361])\n",
      "+- *(1) Project [transaction_id#48, customer_id#49, final_amount#56, transaction_date#58, CASE WHEN (final_amount#56 > 1000.0) THEN Yes ELSE No END AS high_value#361]\n",
      "   +- *(1) Filter (((isnotnull(status#61) AND isnotnull(final_amount#56)) AND (status#61 = completed)) AND (final_amount#56 > 500.0))\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [transaction_id#48,customer_id#49,final_amount#56,transaction_date#58,status#61,year#62,month#63] Batched: true, DataFilters: [isnotnull(status#61), isnotnull(final_amount#56), (status#61 = completed), (final_amount#56 > 50..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://bigdata-ecommerce/raw/transactions], PartitionFilters: [], PushedFilters: [IsNotNull(status), IsNotNull(final_amount), EqualTo(status,completed), GreaterThan(final_amount,..., ReadSchema: struct<transaction_id:string,customer_id:string,final_amount:double,transaction_date:date,status:...\n",
      "\n",
      "\n",
      "\n",
      "LOGICAL PLAN (how to do it):\n",
      "----------------------------------------------------------------------\n",
      "== Parsed Logical Plan ==\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [final_amount#56 DESC NULLS LAST], true\n",
      "      +- Project [transaction_id#48, customer_id#49, final_amount#56, transaction_date#58, CASE WHEN (final_amount#56 > cast(1000 as double)) THEN Yes ELSE No END AS high_value#361]\n",
      "         +- Project [transaction_id#48, customer_id#49, final_amount#56, transaction_date#58]\n",
      "            +- Filter (final_amount#56 > cast(500 as double))\n",
      "               +- Filter (status#61 = completed)\n",
      "                  +- Relation [transaction_id#48,customer_id#49,product_id#50,quantity#51L,unit_price#52,total_amount#53,discount_pct#54,discount_amount#55,final_amount#56,shipping_cost#57,transaction_date#58,transaction_timestamp#59,payment_method#60,status#61,year#62,month#63] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "transaction_id: string, customer_id: string, final_amount: double, transaction_date: date, high_value: string\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [final_amount#56 DESC NULLS LAST], true\n",
      "      +- Project [transaction_id#48, customer_id#49, final_amount#56, transaction_date#58, CASE WHEN (final_amount#56 > cast(1000 as double)) THEN Yes ELSE No END AS high_value#361]\n",
      "         +- Project [transaction_id#48, customer_id#49, final_amount#56, transaction_date#58]\n",
      "            +- Filter (final_amount#56 > cast(500 as double))\n",
      "               +- Filter (status#61 = completed)\n",
      "                  +- Relation [transaction_id#48,customer_id#49,product_id#50,quantity#51L,unit_price#52,total_amount#53,discount_pct#54,discount_amount#55,final_amount#56,shipping_cost#57,transaction_date#58,transaction_timestamp#59,payment_method#60,status#61,year#62,month#63] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [final_amount#56 DESC NULLS LAST], true\n",
      "      +- Project [transaction_id#48, customer_id#49, final_amount#56, transaction_date#58, CASE WHEN (final_amount#56 > 1000.0) THEN Yes ELSE No END AS high_value#361]\n",
      "         +- Filter ((isnotnull(status#61) AND isnotnull(final_amount#56)) AND ((status#61 = completed) AND (final_amount#56 > 500.0)))\n",
      "            +- Relation [transaction_id#48,customer_id#49,product_id#50,quantity#51L,unit_price#52,total_amount#53,discount_pct#54,discount_amount#55,final_amount#56,shipping_cost#57,transaction_date#58,transaction_timestamp#59,payment_method#60,status#61,year#62,month#63] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=10, orderBy=[final_amount#56 DESC NULLS LAST], output=[transaction_id#48,customer_id#49,final_amount#56,transaction_date#58,high_value#361])\n",
      "+- *(1) Project [transaction_id#48, customer_id#49, final_amount#56, transaction_date#58, CASE WHEN (final_amount#56 > 1000.0) THEN Yes ELSE No END AS high_value#361]\n",
      "   +- *(1) Filter (((isnotnull(status#61) AND isnotnull(final_amount#56)) AND (status#61 = completed)) AND (final_amount#56 > 500.0))\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [transaction_id#48,customer_id#49,final_amount#56,transaction_date#58,status#61,year#62,month#63] Batched: true, DataFilters: [isnotnull(status#61), isnotnull(final_amount#56), (status#61 = completed), (final_amount#56 > 50..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://bigdata-ecommerce/raw/transactions], PartitionFilters: [], PushedFilters: [IsNotNull(status), IsNotNull(final_amount), EqualTo(status,completed), GreaterThan(final_amount,..., ReadSchema: struct<transaction_id:string,customer_id:string,final_amount:double,transaction_date:date,status:...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# QUERY EXECUTION PLAN\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"QUERY EXECUTION PLAN\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Complex chain of transformations\n",
    "result = (\n",
    "    transactions\n",
    "    .filter(F.col(\"status\") == \"completed\")\n",
    "    .filter(F.col(\"final_amount\") > 500)\n",
    "    .select(\n",
    "        \"transaction_id\",\n",
    "        \"customer_id\",\n",
    "        \"final_amount\",\n",
    "        \"transaction_date\"\n",
    "    )\n",
    "    .withColumn(\"high_value\", F.when(F.col(\"final_amount\") > 1000, \"Yes\").otherwise(\"No\"))\n",
    "    .orderBy(F.desc(\"final_amount\"))\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "print(\"Chained operations:\")\n",
    "print(\"  1. Filter completed transactions\")\n",
    "print(\"  2. Filter amount > 500\")\n",
    "print(\"  3. Select specific columns\")\n",
    "print(\"  4. Add high_value column\")\n",
    "print(\"  5. Order by amount\")\n",
    "print(\"  6. Limit to top 10\")\n",
    "\n",
    "print(\"\\nResult:\")\n",
    "result.show()\n",
    "\n",
    "print(\"\\nSpark optimizes this chain before execution (Catalyst optimizer)\")\n",
    "\n",
    "# Show physical plan\n",
    "print(\"\\nPHYSICAL PLAN (what to do):\")\n",
    "print(\"-\" * 70)\n",
    "result.explain(extended=False)\n",
    "\n",
    "# Show logical plan\n",
    "print(\"\\nLOGICAL PLAN (how to do it):\")\n",
    "print(\"-\" * 70)\n",
    "result.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL & Joins\n",
    "\n",
    "Spark SQL permette di eseguire query SQL distribuite su DataFrame con ottimizzazione automatica tramite Catalyst.\n",
    "\n",
    "**Temporary Views** - per registrare DataFrame come tabelle SQL:\n",
    "- **Temp View**: disponibile solo nella SparkSession corrente, si cancella al termine della sessione\n",
    "- **Global Temp View**: accessibile a tutte le sessioni Spark nell'applicazione, utile per condividere dati tra notebook\n",
    "\n",
    "**Operazioni JOIN**:\n",
    "- Supporta tutte le tipologie standard: `inner`, `left`, `right`, `outer`\n",
    "- Join standard richiedono shuffle (riorganizzazione dati tra partizioni) ed e' dunque lenta\n",
    "\n",
    "**Broadcast Join Optimization**:\n",
    "Quando una tabella e' piccola (< 10MB default), Spark invia una copia completa della tabella a tutti gli executors. Il join avviene localmente su ogni executor senza shuffle della tabella grande, risultando molto piu' veloce. Evitare di utilizzarlo con tabelle >100MB (rischio OOM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "REGISTERING TEMPORARY VIEWS\n",
      "======================================================================\n",
      "\n",
      "Registered views:\n",
      "   - transactions\n",
      "   - customers\n",
      "   - products\n",
      "\n",
      "Available tables:\n",
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|         |   customers|      false|\n",
      "|         |    products|      false|\n",
      "|         |transactions|      false|\n",
      "+---------+------------+-----------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SIMPLE SQL QUERIES\n",
      "======================================================================\n",
      "\n",
      "1. Top 10 high-value transactions:\n",
      "+--------------+-----------+------------+----------------+\n",
      "|transaction_id|customer_id|final_amount|transaction_date|\n",
      "+--------------+-----------+------------+----------------+\n",
      "|   T0098657043|  C00472985|     8997.85|      2024-04-29|\n",
      "|   T0025697854|  C00291574|     8994.05|      2024-09-18|\n",
      "|   T0075933682|  C00968650|     8991.83|      2025-07-31|\n",
      "|   T0088081874|  C00633627|     8982.91|      2025-03-26|\n",
      "|   T0084912906|  C00797292|     8979.11|      2025-10-28|\n",
      "|   T0077918434|  C00794352|     8975.02|      2025-08-03|\n",
      "|   T0057242121|  C00663478|     8974.71|      2024-03-19|\n",
      "|   T0030245510|  C00101347|     8964.92|      2024-12-09|\n",
      "|   T0000997695|  C00138589|     8958.15|      2024-04-04|\n",
      "|   T0029100685|  C00123143|     8956.97|      2024-01-30|\n",
      "+--------------+-----------+------------+----------------+\n",
      "\n",
      "\n",
      "2. Transactions by status:\n",
      "+---------+--------+--------------------+-----------------+\n",
      "|   status|   count|       total_revenue|       avg_amount|\n",
      "+---------+--------+--------------------+-----------------+\n",
      "|completed|95001380|4.114988022378023E10|433.1503418558786|\n",
      "| refunded| 2000246|                 0.0|              0.0|\n",
      "|cancelled| 2998374|                 0.0|              0.0|\n",
      "+---------+--------+--------------------+-----------------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "GROUPBY QUERIES\n",
      "======================================================================\n",
      "\n",
      "1. Analysis by customer segment:\n",
      "+----------------+-------------+-----------------+-----------+\n",
      "|customer_segment|num_customers|          avg_age|num_regions|\n",
      "+----------------+-------------+-----------------+-----------+\n",
      "|      Occasional|       600000|         46.50292|         20|\n",
      "|         Regular|       350000|46.51125428571429|         20|\n",
      "|             VIP|        50000|         46.51384|         20|\n",
      "+----------------+-------------+-----------------+-----------+\n",
      "\n",
      "\n",
      "2. Sales by product category:\n",
      "+--------------+----------------+--------------------+---------------------+\n",
      "|      category|num_transactions|       total_revenue|avg_transaction_value|\n",
      "+--------------+----------------+--------------------+---------------------+\n",
      "|   Electronics|         8440996|1.808401946341007E10|   2142.4035106058654|\n",
      "|        Beauty|        18830251| 4.941475243440018E9|   262.42216545281406|\n",
      "|       Fashion|        11114326| 4.485214480169998E9|     403.552539323572|\n",
      "|        Sports|         8003459| 3.843931032420014E9|    480.2837163806317|\n",
      "|        Health|        10168864|2.6793183183899865E9|    263.4825599388473|\n",
      "|Home & Kitchen|        11388343| 2.557262712840004E9|   224.55090374780633|\n",
      "|          Toys|         7011573| 1.941004102529983E9|   276.82862355280093|\n",
      "|    Automotive|         6101310|1.6060144092299993E9|    263.2245221485221|\n",
      "|          Food|         7470202| 6.596727886099981E8|    88.30722229599657|\n",
      "|         Books|         6472056| 3.519676727400006E8|    54.38266800225471|\n",
      "+--------------+----------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL QUERIES\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"REGISTERING TEMPORARY VIEWS\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "transactions.createOrReplaceTempView(\"transactions\")\n",
    "customers.createOrReplaceTempView(\"customers\")\n",
    "products.createOrReplaceTempView(\"products\")\n",
    "\n",
    "print(\"Registered views:\")\n",
    "print(\"   - transactions\")\n",
    "print(\"   - customers\")\n",
    "print(\"   - products\")\n",
    "\n",
    "print(\"\\nAvailable tables:\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SIMPLE SQL QUERIES\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Query 1: SELECT with WHERE\n",
    "print(\"1. Top 10 high-value transactions:\")\n",
    "query1 = \"\"\"\n",
    "SELECT\n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    final_amount,\n",
    "    transaction_date\n",
    "FROM transactions\n",
    "WHERE status = 'completed' AND final_amount > 1000\n",
    "ORDER BY final_amount DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "result1 = spark.sql(query1)\n",
    "result1.show()\n",
    "\n",
    "# Query 2: Aggregation\n",
    "print(\"\\n2. Transactions by status:\")\n",
    "query2 = \"\"\"\n",
    "SELECT\n",
    "    status,\n",
    "    COUNT(*) as count,\n",
    "    SUM(final_amount) as total_revenue,\n",
    "    AVG(final_amount) as avg_amount\n",
    "FROM transactions\n",
    "GROUP BY status\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "result2 = spark.sql(query2)\n",
    "result2.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GROUPBY QUERIES\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Customer segments analysis\n",
    "print(\"1. Analysis by customer segment:\")\n",
    "query3 = \"\"\"\n",
    "SELECT\n",
    "    customer_segment,\n",
    "    COUNT(*) as num_customers,\n",
    "    AVG(age) as avg_age,\n",
    "    COUNT(DISTINCT region) as num_regions\n",
    "FROM customers\n",
    "GROUP BY customer_segment\n",
    "ORDER BY num_customers DESC\n",
    "\"\"\"\n",
    "result3 = spark.sql(query3)\n",
    "result3.show()\n",
    "\n",
    "# Product categories\n",
    "print(\"\\n2. Sales by product category:\")\n",
    "query4 = \"\"\"\n",
    "SELECT\n",
    "    p.category,\n",
    "    COUNT(DISTINCT t.transaction_id) as num_transactions,\n",
    "    SUM(t.final_amount) as total_revenue,\n",
    "    AVG(t.final_amount) as avg_transaction_value\n",
    "FROM transactions t\n",
    "JOIN products p ON t.product_id = p.product_id\n",
    "WHERE t.status = 'completed'\n",
    "GROUP BY p.category\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "result4 = spark.sql(query4)\n",
    "result4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INNER JOIN\n",
      "======================================================================\n",
      "\n",
      "Transactions with customer information:\n",
      "+--------------+------------+--------------------+----------------+-------+\n",
      "|transaction_id|final_amount|       customer_name|customer_segment| region|\n",
      "+--------------+------------+--------------------+----------------+-------+\n",
      "|   T0032651422|      177.37|Sig.ra Clelia Ian...|      Occasional|Toscana|\n",
      "|   T0016231588|      182.02|Sig.ra Clelia Ian...|      Occasional|Toscana|\n",
      "|   T0090010818|       250.2|Sig.ra Clelia Ian...|      Occasional|Toscana|\n",
      "|   T0085437320|      112.35|Sig.ra Clelia Ian...|      Occasional|Toscana|\n",
      "|   T0087918645|      329.85|Sig.ra Clelia Ian...|      Occasional|Toscana|\n",
      "|   T0010783655|      225.89|Sig.ra Clelia Ian...|      Occasional|Toscana|\n",
      "|   T0063166968|      806.97|Sig.ra Clelia Ian...|      Occasional|Toscana|\n",
      "|   T0067240728|      279.13|Sig.ra Clelia Ian...|      Occasional|Toscana|\n",
      "|   T0018171683|      677.49|Sig.ra Clelia Ian...|      Occasional|Toscana|\n",
      "|   T0018427078|       344.0|Sig.ra Clelia Ian...|      Occasional|Toscana|\n",
      "+--------------+------------+--------------------+----------------+-------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "LEFT JOIN\n",
      "======================================================================\n",
      "\n",
      "Top 20 customers by total spent:\n",
      "+-----------+--------------------+----------------+----------------+------------------+\n",
      "|customer_id|                name|customer_segment|num_transactions|       total_spent|\n",
      "+-----------+--------------------+----------------+----------------+------------------+\n",
      "|  C00271911|  Sig.ra Pina Golino|             VIP|             349|279918.64999999997|\n",
      "|  C00387531|     Lodovico Fusani|             VIP|             323|260226.26999999987|\n",
      "|  C00531659|     Costanzo Nordio|             VIP|             318|256164.09000000003|\n",
      "|  C00458014|Nicolò Mezzetta-B...|             VIP|             304|251168.44999999987|\n",
      "|  C00343149|Dott. Fortunata B...|             VIP|             334| 249635.8800000003|\n",
      "|  C00941483|   Valerio Tomaselli|             VIP|             327|248252.66000000024|\n",
      "|  C00984397|      Uberto Fagotto|             VIP|             332|247968.63000000012|\n",
      "|  C00371245|Sig.ra Aurora Lol...|             VIP|             343|247284.47000000003|\n",
      "|  C00479841|         Elisa Verri|             VIP|             347|246091.26999999993|\n",
      "|  C00665141|     Durante Pedroni|             VIP|             340|245274.05999999994|\n",
      "|  C00480031|   Sig.ra Dina Blasi|             VIP|             307| 244540.3899999998|\n",
      "|  C00529766|     Amleto Sabatini|             VIP|             313|243205.15999999986|\n",
      "|  C00357562|   Carmelo Tamborini|             VIP|             337|243151.69999999978|\n",
      "|  C00973129|      Stefano Ciampi|             VIP|             337|         242805.41|\n",
      "|  C00579353|Lidia Battaglia-C...|             VIP|             339|241701.30999999988|\n",
      "|  C00658949|       Virgilio Muti|             VIP|             317|241646.07999999964|\n",
      "|  C00642954|Dott. Gabriella S...|             VIP|             314|241130.11000000002|\n",
      "|  C00903647|        Priscilla Fo|             VIP|             333|241124.07999999993|\n",
      "|  C00989351|       Giulio Trotta|             VIP|             294|241079.78999999998|\n",
      "|  C00010850|Evangelista Manac...|             VIP|             304| 240939.7799999999|\n",
      "+-----------+--------------------+----------------+----------------+------------------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "BROADCAST JOIN OPTIMIZATION\n",
      "======================================================================\n",
      "\n",
      "Products table size: 50,000 rows\n",
      "Transactions table size: 100,000,000 rows\n",
      "\n",
      "1. Standard Join:\n",
      "+--------------+------------+--------------------+-----------+\n",
      "|transaction_id|final_amount|        product_name|   category|\n",
      "+--------------+------------+--------------------+-----------+\n",
      "|   T0050000090|      218.13|L'Oréal Fragrance...|     Beauty|\n",
      "|   T0050000116|      258.41|Nature's Bounty S...|     Health|\n",
      "|   T0050000120|      593.12|Zara Accessories ...|    Fashion|\n",
      "|   T0050000126|       130.3|Under Armour Acce...|     Sports|\n",
      "|   T0050000134|     1667.78|HP Accessories Oc...|Electronics|\n",
      "|   T0050000148|      612.97|Mattel Board Game...|       Toys|\n",
      "|   T0050000164|      221.57|L'Oréal Fragrance...|     Beauty|\n",
      "|   T0050000187|      206.82|Nature's Bounty W...|     Health|\n",
      "|   T0050000241|         0.0|Mattel Action Fig...|       Toys|\n",
      "|   T0050000331|       21.87|       3M Parts Unde| Automotive|\n",
      "+--------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "2. Broadcast Join:\n",
      "+--------------+------------+--------------------+-----------+\n",
      "|transaction_id|final_amount|        product_name|   category|\n",
      "+--------------+------------+--------------------+-----------+\n",
      "|   T0050000090|      218.13|L'Oréal Fragrance...|     Beauty|\n",
      "|   T0050000116|      258.41|Nature's Bounty S...|     Health|\n",
      "|   T0050000120|      593.12|Zara Accessories ...|    Fashion|\n",
      "|   T0050000126|       130.3|Under Armour Acce...|     Sports|\n",
      "|   T0050000134|     1667.78|HP Accessories Oc...|Electronics|\n",
      "|   T0050000148|      612.97|Mattel Board Game...|       Toys|\n",
      "|   T0050000164|      221.57|L'Oréal Fragrance...|     Beauty|\n",
      "|   T0050000187|      206.82|Nature's Bounty W...|     Health|\n",
      "|   T0050000241|         0.0|Mattel Action Fig...|       Toys|\n",
      "|   T0050000331|       21.87|       3M Parts Unde| Automotive|\n",
      "+--------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Broadcast join speedup: 2.24x\n",
      "   Standard join: 0.48s\n",
      "   Broadcast join: 0.21s\n",
      "Saved: 03_join_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# JOIN OPERATIONS\n",
    "\n",
    "# Inner Join\n",
    "print(\"=\" * 70)\n",
    "print(\"INNER JOIN\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "query5 = \"\"\"\n",
    "SELECT\n",
    "    t.transaction_id,\n",
    "    t.final_amount,\n",
    "    c.name as customer_name,\n",
    "    c.customer_segment,\n",
    "    c.region\n",
    "FROM transactions t\n",
    "INNER JOIN customers c ON t.customer_id = c.customer_id\n",
    "WHERE t.final_amount > 100\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "result5 = spark.sql(query5)\n",
    "print(\"Transactions with customer information:\")\n",
    "result5.show()\n",
    "\n",
    "# Left Join\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LEFT JOIN\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "query6 = \"\"\"\n",
    "SELECT\n",
    "    c.customer_id,\n",
    "    c.name,\n",
    "    c.customer_segment,\n",
    "    COUNT(t.transaction_id) as num_transactions,\n",
    "    COALESCE(SUM(t.final_amount), 0) as total_spent\n",
    "FROM customers c\n",
    "LEFT JOIN transactions t ON c.customer_id = t.customer_id\n",
    "    AND t.status = 'completed'\n",
    "GROUP BY c.customer_id, c.name, c.customer_segment\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "result6 = spark.sql(query6)\n",
    "print(\"Top 20 customers by total spent:\")\n",
    "result6.show()\n",
    "\n",
    "# Broadcast Join Optimization\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BROADCAST JOIN OPTIMIZATION\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "print(f\"Products table size: {products.count():,} rows\")\n",
    "print(f\"Transactions table size: {transactions.count():,} rows\")\n",
    "\n",
    "# Standard join (without broadcast)\n",
    "print(\"\\n1. Standard Join:\")\n",
    "standard_join = transactions.join(\n",
    "    products,\n",
    "    transactions.product_id == products.product_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    transactions.transaction_id,\n",
    "    transactions.final_amount,\n",
    "    products.product_name,\n",
    "    products.category\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "standard_join.show(10)\n",
    "standard_time = time.time() - start\n",
    "\n",
    "# Broadcast join\n",
    "print(\"\\n2. Broadcast Join:\")\n",
    "broadcast_join = transactions.join(\n",
    "    F.broadcast(products),\n",
    "    transactions.product_id == products.product_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    transactions.transaction_id,\n",
    "    transactions.final_amount,\n",
    "    products.product_name,\n",
    "    products.category\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "broadcast_join.show(10)\n",
    "broadcast_time = time.time() - start\n",
    "\n",
    "# Compare\n",
    "speedup = (standard_time / broadcast_time) if broadcast_time > 0 else 1\n",
    "print(f\"\\nBroadcast join speedup: {speedup:.2f}x\")\n",
    "print(f\"   Standard join: {standard_time:.2f}s\")\n",
    "print(f\"   Broadcast join: {broadcast_time:.2f}s\")\n",
    "\n",
    "# Save comparison\n",
    "join_comparison = pd.DataFrame({\n",
    "    \"Join_Type\": [\"Standard\", \"Broadcast\"],\n",
    "    \"Time_s\": [standard_time, broadcast_time]\n",
    "})\n",
    "save_results(join_comparison, \"03_join_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Tuning & Monitoring\n",
    "\n",
    "L'ottimizzazione delle performance in PySpark si basa su quattro tecniche principali: **partitioning** per distribuire i dati in modo ottimale, **caching** per evitare ricomputazioni, **broadcast join** per ridurre lo shuffle, e **configuration tuning** per ottimizzare i parametri Spark.\n",
    "\n",
    "## Partitioning\n",
    "\n",
    "**Partition Size** - regola generale 128MB per partition (standard Hadoop):\n",
    "- Partizioni troppo piccole --> overhead eccessivo\n",
    "- Partizioni troppo grandi --> memory pressure e stragglers\n",
    "\n",
    "**Partition Count**:\n",
    "- Minimum: `num_cores * 2`\n",
    "- Optimal: `num_cores * 4`\n",
    "\n",
    "**Repartition vs Coalesce**:\n",
    "- `repartition(n)`: esegue shuffle, utile per aumentare/ridurre partizioni e load balancing\n",
    "- `coalesce(n)`: no shuffle, solo per ridurre partizioni, piu' veloce\n",
    "\n",
    "## Caching\n",
    "\n",
    "**Quando cachare**:\n",
    "- DataFrame usato multiple volte nella stessa sessione\n",
    "- Dopo operazioni costose (join, aggregazioni complesse)\n",
    "- Algoritmi iterativi (machine learning)\n",
    "\n",
    "**Quando non cachare**:\n",
    "- DataFrame usato una sola volta\n",
    "- Dati gia' ottimizzati in memoria (es. Parquet con columnar pruning)\n",
    "- Dataset troppo grande per la memoria disponibile (rischio spill to disk)\n",
    "\n",
    "## Spark UI\n",
    "\n",
    "Accessibile su **http://localhost:4040** durante una SparkSession attiva.\n",
    "\n",
    "**Tabs principali**:\n",
    "- **Jobs**: tutte le jobs eseguite, durata, stages, tasks, status success/failure\n",
    "- **Stages**: breakdown dettagliato di ogni job, metriche a livello task, shuffle read/write\n",
    "- **Storage**: RDD/DataFrame cached, utilizzo memoria, frazione cached vs spilled to disk\n",
    "- **Environment**: Spark properties, system properties, classpath\n",
    "- **Executors**: metriche per executor, memory usage, GC time, distribuzione task\n",
    "- **SQL**: execution plans (physical/logical), metriche per operatore, visualizzazione DAG\n",
    "\n",
    "**Spark UI Jobs**\n",
    "\n",
    "<img src=\"./screenshots/03_spark_jobs.png\" alt=\"Spark UI Jobs\" >\n",
    "\n",
    "**Spark UI Executors**\n",
    "\n",
    "<img src=\"./screenshots/03_spark_executors.png\" alt=\"Spark UI Executors\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OPTIMIZATIONS\n",
      "======================================================================\n",
      "\n",
      "Running baseline query...\n",
      "\n",
      "Result:\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "|customer_segment|      category|num_transactions|       total_revenue|   avg_transaction|\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "|         Regular|   Electronics|         4547002| 9.862931500629993E9| 2169.106479528708|\n",
      "|      Occasional|   Electronics|         2595077| 4.460162046650009E9|1718.7012357051483|\n",
      "|             VIP|   Electronics|         1298917| 3.760925916129992E9|2895.4320531103926|\n",
      "|         Regular|        Beauty|        10136890|2.6945622244399986E9| 265.8174474064529|\n",
      "|         Regular|       Fashion|         5985500|2.4465271903900003E9|408.74232568540646|\n",
      "|         Regular|        Sports|         4307190| 2.095215886780005E9| 486.4461253810501|\n",
      "|         Regular|        Health|         5477055|1.4617503961900039E9| 266.8862000089471|\n",
      "|         Regular|Home & Kitchen|         6133733| 1.395528803750005E9|227.51704447357017|\n",
      "|      Occasional|        Beauty|         5795895| 1.219338762650004E9|210.37971920643903|\n",
      "|      Occasional|       Fashion|         3419255|1.1064785241299996E9| 323.6022244991964|\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Baseline completed:\n",
      "Time: 35.65s\n",
      "\n",
      "======================================================================\n",
      "PARTITIONING & CACHING\n",
      "======================================================================\n",
      "\n",
      "Current partitions:\n",
      "  Transactions: 35\n",
      "  Customers:    12\n",
      "  Products:     1\n",
      "\n",
      "Repartitioning transactions to 24 partitions & caching...\n",
      "  Repartitioned in     149.31s\n",
      "  New partitions:      24\n",
      "  Transactions cached: 100,000,000 rows\n",
      "\n",
      "Caching Customers & Products tables...\n",
      "  Customers cached: 1,000,000 rows\n",
      "  Products cached: 50,000 rows\n",
      "\n",
      "======================================================================\n",
      "OPTIMIZED PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Running optimized query...\n",
      "\n",
      "Result:\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "|customer_segment|      category|num_transactions|       total_revenue|   avg_transaction|\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "|         Regular|   Electronics|         4547002| 9.862931500630074E9|2169.1064795287252|\n",
      "|      Occasional|   Electronics|         2595077| 4.460162046650007E9|1718.7012357051476|\n",
      "|             VIP|   Electronics|         1298917|3.7609259161300006E9|2895.4320531103995|\n",
      "|         Regular|        Beauty|        10136890| 2.694562224440001E9|265.81744740645314|\n",
      "|         Regular|       Fashion|         5985500|2.4465271903900013E9| 408.7423256854066|\n",
      "|         Regular|        Sports|         4307190|2.0952158867800114E9|486.44612538105156|\n",
      "|         Regular|        Health|         5477055| 1.461750396190001E9| 266.8862000089466|\n",
      "|         Regular|Home & Kitchen|         6133733|1.3955288037500005E9|227.51704447356943|\n",
      "|      Occasional|        Beauty|         5795895|1.2193387626500049E9|210.37971920643918|\n",
      "|      Occasional|       Fashion|         3419255|1.1064785241299994E9|323.60222449919627|\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Performance Improvement:\n",
      "  Baseline: 35.65s\n",
      "  Optimized: 18.75s\n",
      "  Speedup: 1.90x\n",
      "\n",
      "Releasing cached DataFrames...\n",
      "  Cache released\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZATIONS\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OPTIMIZATIONS\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Clear any previous cache\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Baseline: Complex aggregation without optimization\n",
    "print(\"Running baseline query...\")\n",
    "start = time.time()\n",
    "result_baseline = (\n",
    "    transactions\n",
    "    .filter(F.col(\"status\") == \"completed\")\n",
    "    .join(customers, \"customer_id\")\n",
    "    .join(products, \"product_id\")\n",
    "    .groupBy(\"customer_segment\", \"category\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_transactions\"),\n",
    "        F.sum(\"final_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"final_amount\").alias(\"avg_transaction\")\n",
    "    )\n",
    "    .orderBy(F.col(\"total_revenue\").desc())\n",
    ")\n",
    "\n",
    "print(\"\\nResult:\")\n",
    "result_baseline.show(10)\n",
    "baseline_time = time.time() - start\n",
    "\n",
    "print(f\"Baseline completed:\")\n",
    "print(f\"Time: {baseline_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PARTITIONING & CACHING\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Check current partitions\n",
    "print(\"Current partitions:\")\n",
    "print(f\"  Transactions: {transactions.rdd.getNumPartitions()}\")\n",
    "print(f\"  Customers:    {customers.rdd.getNumPartitions()}\")\n",
    "print(f\"  Products:     {products.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Optimal partitions: 2x physical cores for local mode\n",
    "optimal_partitions = 24\n",
    "\n",
    "print(f\"\\nRepartitioning transactions to {optimal_partitions} partitions & caching...\")\n",
    "start = time.time()\n",
    "transactions_optimized = transactions.repartition(optimal_partitions)\n",
    "transactions_optimized.cache()\n",
    "transactions_count = transactions_optimized.count()\n",
    "repart_time = time.time() - start\n",
    "\n",
    "print(f\"  Repartitioned in     {repart_time:.2f}s\")\n",
    "print(f\"  New partitions:      {transactions_optimized.rdd.getNumPartitions()}\")\n",
    "print(f\"  Transactions cached: {transactions_count:,} rows\")\n",
    "\n",
    "# Cache dimension tables (small, frequently joined)\n",
    "print(\"\\nCaching Customers & Products tables...\")\n",
    "\n",
    "customers_optimized = customers.cache()\n",
    "customers_count = customers_optimized.count()\n",
    "print(f\"  Customers cached: {customers_count:,} rows\")\n",
    "\n",
    "products_optimized = products.cache()\n",
    "products_count = products_optimized.count()\n",
    "print(f\"  Products cached: {products_count:,} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMIZED PERFORMANCE\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Same query with cached + repartitioned + broadcast\n",
    "print(\"Running optimized query...\")\n",
    "start = time.time()\n",
    "result_optimized = (\n",
    "    transactions_optimized\n",
    "    .filter(F.col(\"status\") == \"completed\")\n",
    "    .join(F.broadcast(customers_optimized), \"customer_id\")\n",
    "    .join(F.broadcast(products_optimized), \"product_id\")\n",
    "    .groupBy(\"customer_segment\", \"category\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_transactions\"),\n",
    "        F.sum(\"final_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"final_amount\").alias(\"avg_transaction\")\n",
    "    )\n",
    "    .orderBy(F.col(\"total_revenue\").desc())\n",
    ")\n",
    "\n",
    "print(\"\\nResult:\")\n",
    "result_optimized.show(10)\n",
    "optimized_time = time.time() - start\n",
    "\n",
    "# Calculate improvement\n",
    "speedup = baseline_time / optimized_time if optimized_time > 0 else 1\n",
    "print(f\"\\nPerformance Improvement:\")\n",
    "print(f\"  Baseline: {baseline_time:.2f}s\")\n",
    "print(f\"  Optimized: {optimized_time:.2f}s\")\n",
    "print(f\"  Speedup: {speedup:.2f}x\")\n",
    "\n",
    "\n",
    "# Release cached DataFrames\n",
    "print(\"\\nReleasing cached DataFrames...\")\n",
    "transactions_optimized.unpersist()\n",
    "spark.catalog.clearCache()\n",
    "print(\"  Cache released\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline & Delta Lake\n",
    "\n",
    "In questa sezione costruiremo 2 pipeline ETL complete:\n",
    "1. pipeline ETL completa che prepara dati ML-ready (per Notebook 4)\n",
    "2. pipeline ETL completa che converte le transizioni in formato **Avro** (per Notebook 5)\n",
    "\n",
    "## Target Output per ML\n",
    "\n",
    "**ALS Data** (`user_item_interactions`) - per recommendation system:\n",
    "- Campi: `customer_id`, `product_id`, `rating`\n",
    "- Rating implicito derivato dal comportamento d'acquisto:\n",
    "\n",
    "    ```\n",
    "    rating = log(1 + purchase_count) * 2 + log(1 + total_spent) / 2\n",
    "    ```\n",
    "\n",
    "**Random Forest Data** (`customer_features`) - per classificazione:\n",
    "1. **Features Engineering**:\n",
    "    - **RFM**: Recency, Frequency, Monetary\n",
    "    - **Behavioral**: avg_transaction, unique_products\n",
    "    - **Temporal**: days_since_last_purchase\n",
    "    - **Demographic**: age, region\n",
    "\n",
    "2. **Labels**:\n",
    "    - `customer_segment` (multiclass)\n",
    "\n",
    "## Target Output per Streaming\n",
    "\n",
    "La pipeline produce una conversione delle transazioni in **Avro** il quale e' un formato di serializzazione dati row-based, ideale per pipeline di streaming e write-intensive workloads, con le seguenti caratteristiche:\n",
    "\n",
    "- **Schema embedded**: lo schema e' incluso nel file stesso, garantendo auto-descrizione dei dati\n",
    "- **Row-based**: ottimizzato per scritture sequenziali e streaming, a differenza di Parquet che e' columnar\n",
    "- **Schema evolution**: supporto nativo per aggiunta/rimozione di campi con backward/forward compatibility\n",
    "- **Compatto e veloce**: serializzazione binaria efficiente\n",
    "\n",
    "Leggiamo le transazioni dal Parquet partizionato su MinIO e le riscriviamo in formato Avro, mantenendo la partizione per `year`/`month`. Il risultato sara' il source per la Bronze layer del Notebook 5.\n",
    "\n",
    "## Delta Lake: ACID su Data Lake\n",
    "\n",
    "Delta Lake e' uno **storage layer costruito su Parquet** che aggiunge transazioni ACID, versioning e schema evolution ai data lake. Inoltre supporta sia batch che streaming in modo unificato.\n",
    "\n",
    "**Problemi dei Data Lake tradizionali**:\n",
    "- Assenza di transazioni ACID\n",
    "- Nessun versionamento dei dati\n",
    "- Inconsistenze nello schema\n",
    "- Impossibilita' di rollback\n",
    "- Mancanza di audit trail\n",
    "\n",
    "**Soluzioni offerte da Delta Lake**:\n",
    "- **ACID transactions**: atomicita' e consistenza garantite\n",
    "- **Time Travel**: accesso a versioni precedenti dei dati\n",
    "- **Schema enforcement & evolution**: validazione e evoluzione controllata dello schema\n",
    "- **Audit history**: transaction log completo di tutte le operazioni\n",
    "- **Unified batch & streaming**: stessa API per entrambe le modalita' di elaborazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ETL PIPELINE FOR ML\n",
      "======================================================================\n",
      "\n",
      "Pipeline Goal: Prepare data for ML models in Notebook 4\n",
      "\n",
      "Target Models:\n",
      "  1. ALS (Recommendation System)\n",
      "     - Need: user_id, item_id, rating (implicit)\n",
      "  2. Random Forest (Classification)\n",
      "     - Need: customer features + label\n",
      "\n",
      "ETL Stages:\n",
      "  EXTRACT: Load raw data from MinIO\n",
      "  TRANSFORM: Create ML-ready features\n",
      "  LOAD: Save to Delta Lake tables\n",
      "\n",
      "======================================================================\n",
      "EXTRACT\n",
      "======================================================================\n",
      "\n",
      "Loading data for ETL pipeline...\n",
      "  Transactions: 100,000,000 rows\n",
      "  Customers:    1,000,000 rows\n",
      "  Products:     50,000 rows\n",
      "\n",
      "Extract phase completed!\n",
      "\n",
      "======================================================================\n",
      "TRANSFORM - ALS DATA PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "Creating user-item interaction matrix...\n",
      "Schema after conversion:\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- purchase_count: long (nullable = false)\n",
      " |-- total_spent: double (nullable = true)\n",
      "\n",
      "Created 73,874,174 user-item interactions\n",
      "\n",
      "Sample interactions:\n",
      "+-------+-------+------+-----------+----------+--------------+------------------+\n",
      "|user_id|item_id|rating|customer_id|product_id|purchase_count|       total_spent|\n",
      "+-------+-------+------+-----------+----------+--------------+------------------+\n",
      "| 304524|      1| 12.63|  C00304524| P00000001|            49|14807.519999999999|\n",
      "| 713095|      1| 12.56|  C00713095| P00000001|            48|14162.890000000001|\n",
      "| 681654|      1| 12.54|  C00681654| P00000001|            48|          13455.59|\n",
      "| 139826|      1| 12.51|  C00139826| P00000001|            46|15117.879999999997|\n",
      "|   3361|      1| 12.48|  C00003361| P00000001|            46|          14047.31|\n",
      "| 766483|      1| 12.46|  C00766483| P00000001|            46|          13546.52|\n",
      "| 154263|      1| 12.42|  C00154263| P00000001|            46|12566.040000000003|\n",
      "| 537479|      1| 12.42|  C00537479| P00000001|            45|13779.419999999998|\n",
      "| 847118|      1| 12.41|  C00847118| P00000001|            45|13327.879999999997|\n",
      "| 843125|      1|  12.4|  C00843125| P00000001|            44|14234.350000000002|\n",
      "+-------+-------+------+-----------+----------+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TRANSFORM - RANDOM FOREST DATA PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "Creating customer features...\n",
      "\n",
      "Customer features prepared\n",
      "Schema: ['customer_id', 'recency', 'frequency', 'monetary', 'avg_transaction', 'unique_products', 'customer_segment', 'age', 'region']\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- recency: integer (nullable = true)\n",
      " |-- frequency: long (nullable = false)\n",
      " |-- monetary: double (nullable = true)\n",
      " |-- avg_transaction: double (nullable = true)\n",
      " |-- unique_products: long (nullable = false)\n",
      " |-- customer_segment: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample data:\n",
      "+-----------+-------+---------+------------------+------------------+---------------+----------------+---+---------------------+\n",
      "|customer_id|recency|frequency|monetary          |avg_transaction   |unique_products|customer_segment|age|region               |\n",
      "+-----------+-------+---------+------------------+------------------+---------------+----------------+---+---------------------+\n",
      "|C00000001  |60     |54       |15084.21          |279.3372222222222 |46             |Occasional      |61 |Calabria             |\n",
      "|C00000008  |50     |49       |19393.65          |395.7887755102041 |45             |Occasional      |21 |Puglia               |\n",
      "|C00000030  |58     |46       |14398.919999999998|313.02            |34             |Occasional      |67 |Toscana              |\n",
      "|C00000040  |48     |49       |19277.690000000002|393.4222448979592 |43             |Occasional      |70 |Marche               |\n",
      "|C00000046  |43     |50       |10729.470000000003|214.58940000000007|44             |Occasional      |48 |Friuli-Venezia Giulia|\n",
      "|C00000047  |47     |51       |15117.28          |296.4172549019608 |43             |Occasional      |64 |Calabria             |\n",
      "|C00000048  |39     |135      |87433.02          |647.652           |100            |Regular         |54 |Puglia               |\n",
      "|C00000060  |41     |138      |60466.74000000001 |438.16478260869576|95             |Regular         |34 |Toscana              |\n",
      "|C00000072  |55     |53       |19114.059999999998|360.64264150943393|43             |Occasional      |31 |Veneto               |\n",
      "|C00000090  |39     |148      |76821.32999999999 |519.0630405405404 |114            |Regular         |46 |Veneto               |\n",
      "+-----------+-------+---------+------------------+------------------+---------------+----------------+---+---------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Class distribution (customer_segment):\n",
      "+----------------+------+\n",
      "|customer_segment| count|\n",
      "+----------------+------+\n",
      "|      Occasional|600000|\n",
      "|         Regular|350000|\n",
      "|             VIP| 50000|\n",
      "+----------------+------+\n",
      "\n",
      "\n",
      "RFM Statistics:\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|summary|           recency|        frequency|          monetary|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|           1000000|          1000000|           1000000|\n",
      "|   mean|         49.379389|         95.00138|41149.880223780034|\n",
      "| stddev|13.029851985487852|65.13704533466566|  38042.4465340215|\n",
      "|    min|                39|               20|3468.3899999999994|\n",
      "|    max|               239|              368| 279918.6499999999|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "LOAD\n",
      "======================================================================\n",
      "\n",
      "Delta Lake paths:\n",
      "  ALS data: s3a://bigdata-ecommerce/ml_data/user_item_interactions\n",
      "  RF data: s3a://bigdata-ecommerce/ml_data/customer_features\n",
      "\n",
      "1. Writing ALS user-item interactions to Delta...\n",
      "  Written in 61.97s\n",
      "\n",
      "2. Writing RF customer features to Delta...\n",
      "  Written in 94.98s\n",
      "\n",
      "ML data loaded to Delta Lake successfully!\n"
     ]
    }
   ],
   "source": [
    "# ETL PIPELINE FOR ML\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ETL PIPELINE FOR ML\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "print(\"Pipeline Goal: Prepare data for ML models in Notebook 4\")\n",
    "print(\"\\nTarget Models:\")\n",
    "print(\"  1. ALS (Recommendation System)\")\n",
    "print(\"     - Need: user_id, item_id, rating (implicit)\")\n",
    "print(\"  2. Random Forest (Classification)\")\n",
    "print(\"     - Need: customer features + label\")\n",
    "\n",
    "print(\"\\nETL Stages:\")\n",
    "print(\"  EXTRACT: Load raw data from MinIO\")\n",
    "print(\"  TRANSFORM: Create ML-ready features\")\n",
    "print(\"  LOAD: Save to Delta Lake tables\")\n",
    "\n",
    "# EXTRACT\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXTRACT\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "print(\"Loading data for ETL pipeline...\")\n",
    "print(f\"  Transactions: {transactions_count:,} rows\")\n",
    "print(f\"  Customers:    {customers_count:,} rows\")\n",
    "print(f\"  Products:     {products_count:,} rows\")\n",
    "print(\"\\nExtract phase completed!\")\n",
    "\n",
    "# TRANSFORM: ALS\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRANSFORM - ALS DATA PREPROCESSING\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "print(\"Creating user-item interaction matrix...\")\n",
    "\n",
    "user_item_interactions = (\n",
    "    transactions\n",
    "    .filter(F.col(\"status\") == \"completed\")\n",
    "    .groupBy(\"customer_id\", \"product_id\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"purchase_count\"),\n",
    "        F.sum(\"final_amount\").alias(\"total_spent\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"rating\",\n",
    "        F.round(\n",
    "            F.expr(\"log(1 + purchase_count) * 2 + log(1 + total_spent) / 2\"),\n",
    "            2\n",
    "        )\n",
    "    )\n",
    "    # Convert string IDs to numeric for ALS compatibility\n",
    "    .withColumn(\"user_id\", F.regexp_extract(F.col(\"customer_id\"), r\"(\\d+)\", 1).cast(\"int\"))\n",
    "    .withColumn(\"item_id\", F.regexp_extract(F.col(\"product_id\"), r\"(\\d+)\", 1).cast(\"int\"))\n",
    "    .select(\n",
    "        \"user_id\",\n",
    "        \"item_id\",\n",
    "        \"rating\",\n",
    "        \"customer_id\",\n",
    "        \"product_id\",\n",
    "        \"purchase_count\",\n",
    "        \"total_spent\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Schema after conversion:\")\n",
    "user_item_interactions.printSchema()\n",
    "\n",
    "als_count = user_item_interactions.count()\n",
    "print(f\"Created {als_count:,} user-item interactions\")\n",
    "print(\"\\nSample interactions:\")\n",
    "user_item_interactions.orderBy(F.desc(\"rating\")).show(10)\n",
    "\n",
    "# TRANSFORM: Random Forest\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRANSFORM - RANDOM FOREST DATA PREPROCESSING\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "print(\"Creating customer features...\")\n",
    "\n",
    "# RFM model + behavioral features\n",
    "customer_features = (\n",
    "    transactions_optimized\n",
    "    .filter(F.col(\"status\") == \"completed\")\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"frequency\"),\n",
    "        F.sum(\"final_amount\").alias(\"monetary\"),\n",
    "        F.avg(\"final_amount\").alias(\"avg_transaction\"),\n",
    "        F.expr(\"datediff(current_date(), max(transaction_date))\").alias(\"recency\"),\n",
    "        F.expr(\"count(distinct product_id)\").alias(\"unique_products\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join with demographics\n",
    "customer_features = customer_features.join(\n",
    "    customers_optimized.select(\"customer_id\", \"customer_segment\", \"age\", \"region\"),\n",
    "    \"customer_id\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Select and cast to correct types\n",
    "customer_features = (\n",
    "    customer_features\n",
    "    .select(\n",
    "        \"customer_id\",\n",
    "        \"recency\",\n",
    "        \"frequency\",\n",
    "        \"monetary\",\n",
    "        \"avg_transaction\",\n",
    "        \"unique_products\",\n",
    "        \"customer_segment\",\n",
    "        \"age\",\n",
    "        \"region\",\n",
    "    )\n",
    "    .withColumn(\"recency\", F.col(\"recency\").cast(\"int\"))\n",
    "    .withColumn(\"frequency\", F.col(\"frequency\").cast(\"long\"))\n",
    "    .withColumn(\"monetary\", F.col(\"monetary\").cast(\"double\"))\n",
    "    .withColumn(\"avg_transaction\", F.col(\"avg_transaction\").cast(\"double\"))\n",
    "    .withColumn(\"unique_products\", F.col(\"unique_products\").cast(\"long\"))\n",
    "    .withColumn(\"age\", F.col(\"age\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "print(\"\\nCustomer features prepared\")\n",
    "print(f\"Schema: {customer_features.columns}\")\n",
    "customer_features.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "customer_features.show(10, truncate=False)\n",
    "print(\"\\nClass distribution (customer_segment):\")\n",
    "customer_features.groupBy(\"customer_segment\").count().orderBy(\"count\", ascending=False).show()\n",
    "print(\"\\nRFM Statistics:\")\n",
    "customer_features.select(\"recency\", \"frequency\", \"monetary\").describe().show()\n",
    "\n",
    "# LOAD\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LOAD\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "print(\"Delta Lake paths:\")\n",
    "print(f\"  ALS data: {als_delta_path}\")\n",
    "print(f\"  RF data: {rf_delta_path}\")\n",
    "\n",
    "# Write ALS data\n",
    "print(\"\\n1. Writing ALS user-item interactions to Delta...\")\n",
    "start = time.time()\n",
    "user_item_interactions.write.format(\"delta\").mode(\"overwrite\").save(als_delta_path)\n",
    "als_write_time = time.time() - start\n",
    "print(f\"  Written in {als_write_time:.2f}s\")\n",
    "\n",
    "# Write RF data\n",
    "print(\"\\n2. Writing RF customer features to Delta...\")\n",
    "start = time.time()\n",
    "customer_features.write.format(\"delta\").mode(\"overwrite\").save(rf_delta_path)\n",
    "rf_write_time = time.time() - start\n",
    "print(f\"  Written in {rf_write_time:.2f}s\")\n",
    "\n",
    "print(\"\\nML data loaded to Delta Lake successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ETL PIPELINE FOR STREAMING\n",
      "======================================================================\n",
      "\n",
      "Pipeline Goal: Prepare data for Streaming in Notebook 5\n",
      "\n",
      "ETL Stages:\n",
      "  EXTRACT: Load raw data from MinIO\n",
      "  TRANSFORM: Convert in Avro format\n",
      "  LOAD: Save to Delta Lake tables\n",
      "\n",
      "======================================================================\n",
      "EXTRACT\n",
      "======================================================================\n",
      "\n",
      "Source (Parquet): s3a://bigdata-ecommerce/raw/transactions\n",
      "Output (Avro):    s3a://bigdata-ecommerce/streaming/source/transactions_avro\n",
      "\\Reading transactions from Parquet...\n",
      "  Read 100,000,000 records\n",
      "\n",
      "======================================================================\n",
      "TRANSFORM\n",
      "======================================================================\n",
      "\n",
      "Add partitioning columns...\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- discount_pct: double (nullable = true)\n",
      " |-- discount_amount: double (nullable = true)\n",
      " |-- final_amount: double (nullable = true)\n",
      " |-- shipping_cost: double (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- transaction_timestamp: timestamp_ntz (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "LOAD\n",
      "======================================================================\n",
      "\n",
      "Writing as Avro (partitioned by year/month)...\n",
      "  Written in 158.0s\n",
      "\n",
      "Verifying Avro output...\n",
      "Avro records: 100,000,000\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- discount_pct: double (nullable = true)\n",
      " |-- discount_amount: double (nullable = true)\n",
      " |-- final_amount: double (nullable = true)\n",
      " |-- shipping_cost: double (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- transaction_timestamp: timestamp_ntz (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "\n",
      "Sample data:\n",
      "+--------------+-----------+----------+--------+----------+------------+------------+---------------+------------+-------------+----------------+---------------------+----------------+---------+----+-----+\n",
      "|transaction_id|customer_id|product_id|quantity|unit_price|total_amount|discount_pct|discount_amount|final_amount|shipping_cost|transaction_date|transaction_timestamp|  payment_method|   status|year|month|\n",
      "+--------------+-----------+----------+--------+----------+------------+------------+---------------+------------+-------------+----------------+---------------------+----------------+---------+----+-----+\n",
      "|   T0030000080|  C00483720| P00008835|       2|     218.0|       436.0|        0.03|          11.03|      424.97|          0.0|      2024-02-15|  2024-02-15 08:32:41|          paypal|completed|2024|    2|\n",
      "|   T0030000085|  C00376825| P00002798|       1|     20.67|       20.67|        0.04|           0.76|       19.91|        10.59|      2024-02-16|  2024-02-16 14:51:25|     credit_card|completed|2024|    2|\n",
      "|   T0030000088|  C00943728| P00000254|       3|    101.95|      305.85|         0.1|          29.33|      276.52|          0.0|      2024-02-16|  2024-02-16 04:50:41|          paypal|completed|2024|    2|\n",
      "|   T0030000097|  C00175777| P00001527|       3|    204.59|      613.77|        0.07|          43.95|      569.82|          0.0|      2024-02-27|  2024-02-27 22:56:41|cash_on_delivery|completed|2024|    2|\n",
      "|   T0030000140|  C00377989| P00002851|       3|     122.9|       368.7|        0.12|          45.31|      323.39|          0.0|      2024-02-29|  2024-02-29 22:05:35|      debit_card|completed|2024|    2|\n",
      "+--------------+-----------+----------+--------+----------+------------+------------+---------------+------------+-------------+----------------+---------------------+----------------+---------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ETL PIPELINE FOR STREAMING\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ETL PIPELINE FOR STREAMING\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "print(\"Pipeline Goal: Prepare data for Streaming in Notebook 5\")\n",
    "\n",
    "print(\"\\nETL Stages:\")\n",
    "print(\"  EXTRACT: Load raw data from MinIO\")\n",
    "print(\"  TRANSFORM: Convert in Avro format\")\n",
    "print(\"  LOAD: Save to Delta Lake tables\")\n",
    "\n",
    "# EXTRACT\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXTRACT\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "print(f\"Source (Parquet): {transactions_path}\")\n",
    "print(f\"Output (Avro):    {avro_output}\")\n",
    "\n",
    "# Read transactions from Parquet\n",
    "print(\"\\Reading transactions from Parquet...\")\n",
    "\n",
    "transactions_for_avro = spark.read.parquet(transactions_path)\n",
    "total_records = transactions_for_avro.count()\n",
    "print(f\"  Read {total_records:,} records\")\n",
    "\n",
    "# TRANSFORM\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRANSFORM\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Add partitioning columns\n",
    "print(\"Add partitioning columns...\")\n",
    "transactions_for_avro = (\n",
    "    transactions_for_avro\n",
    "    .withColumn(\"year\", F.year(F.col(\"transaction_date\")))\n",
    "    .withColumn(\"month\", F.month(F.col(\"transaction_date\")))\n",
    ")\n",
    "print(\"\\nSchema:\")\n",
    "transactions_for_avro.printSchema()\n",
    "\n",
    "# LOAD\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LOAD\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Write as Avro partitioned by year/month\n",
    "print(\"Writing as Avro (partitioned by year/month)...\")\n",
    "start = time.time()\n",
    "(\n",
    "    transactions_for_avro\n",
    "    .write\n",
    "    .format(\"avro\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .save(avro_output)\n",
    ")\n",
    "write_time = time.time() - start\n",
    "print(f\"  Written in {write_time:.1f}s\")\n",
    "\n",
    "# Verify: read back a sample\n",
    "print(\"\\nVerifying Avro output...\")\n",
    "avro_check = spark.read.format(\"avro\").load(avro_output)\n",
    "avro_count = avro_check.count()\n",
    "print(f\"Avro records: {avro_count:,}\")\n",
    "print(f\"\\nSchema:\")\n",
    "avro_check.printSchema()\n",
    "print(f\"\\nSample data:\")\n",
    "avro_check.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Releasing cached DataFrames...\n",
      "  All cache released\n",
      "\n",
      "Stopping Spark session...\n",
      "  Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "# CLEANUP\n",
    "\n",
    "# Release all cached DataFrames\n",
    "print(\"Releasing cached DataFrames...\")\n",
    "spark.catalog.clearCache()\n",
    "print(\"  All cache released\")\n",
    "\n",
    "# Stop Spark\n",
    "print(\"\\nStopping Spark session...\")\n",
    "spark.stop()\n",
    "print(\"  Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinIO Storage Screenshots\n",
    "\n",
    "**User-Item Interactions**\n",
    "\n",
    "<img src=\"./screenshots/03_minio_delta_als.png\" alt=\"MinIo Delta ALS\" >\n",
    "\n",
    "**Customer feature**\n",
    "\n",
    "<img src=\"./screenshots/03_minio_delta_rf.png\" alt=\"MinIo Delta RF\" >\n",
    "\n",
    "**Streaming Source**\n",
    "\n",
    "<img src=\"./screenshots/03_minio_streaming_source.png\" alt=\"MinIo Streaming Source\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
