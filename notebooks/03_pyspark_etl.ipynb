{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed1787c",
   "metadata": {},
   "source": [
    "# PySpark ETL\n",
    "\n",
    "L'obiettivo di questo Notebook è mostrare l'utilizzo di PySpark per l'analisi distribuita di Big Data, dall'introduzione all'architettura fino a pipeline ETL complete con Delta Lake.\n",
    "\n",
    "- **Dataset**: 100M transazioni e-commerce\n",
    "- **Storage**: MinIO (S3-compatible)\n",
    "- **Output**: Dati ML-ready in Delta Lake per Notebook 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16c978b",
   "metadata": {},
   "source": [
    "# Setup & Environment\n",
    "\n",
    "Configurazione ambiente Spark con Delta Lake support.\n",
    "\n",
    "1. Importare librerie necessarie\n",
    "2. Inizializzare SparkSession con configurazione MinIO\n",
    "3. Definire helper functions\n",
    "4. Verificare connessione Delta Lake\n",
    "\n",
    "### Configurazione Spark\n",
    "- **App Name**: Notebook-03-PySpark\n",
    "- **Delta Lake**: Abilitato (2.4.0)\n",
    "- **MinIO**: Configurato come S3-compatible storage\n",
    "- **Driver Memory**: 6GB\n",
    "- **Executor Memory**: 8GB\n",
    "- **Spark UI**: http://localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90871c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count as spark_count, sum as spark_sum, avg, max as spark_max, min as spark_min,\n",
    "    when, lit, round as spark_round, desc, asc, row_number, rank, dense_rank,\n",
    "    expr, broadcast, current_timestamp, regexp_extract\n",
    ")\n",
    "\n",
    "#from pyspark.sql.window import Window\n",
    "#from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType\n",
    "#from delta import DeltaTable\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import configurations\n",
    "from config.spark_config import get_spark_session\n",
    "from config.minio_config import get_s3a_path, BUCKET_NAME\n",
    "\n",
    "# Silence Warning Log\n",
    "import logging\n",
    "logging.getLogger(\"org\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340d8144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 JAR files\n",
      "Spark Session created: Notebook-03-PySpark\n",
      "Spark Version: 3.4.1\n",
      "Spark UI: http://localhost:4040\n",
      "Delta Lake + MinIO: Enabled\n",
      "\n",
      "Key Spark Configurations:\n",
      "  spark.app.name: Notebook-03-PySpark\n",
      "  spark.driver.memory: 6g\n",
      "  spark.executor.memory: 8g\n",
      "  spark.sql.shuffle.partitions: 50\n",
      "  spark.sql.adaptive.enabled: true\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE SPARK SESSION + DELTA LAKE\n",
    "\n",
    "# Local IP configuration\n",
    "os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'\n",
    "\n",
    "spark = get_spark_session(\n",
    "    app_name=\"Notebook-03-PySpark\",\n",
    "    enable_delta=True\n",
    ")\n",
    "\n",
    "# Silence Warnings Log\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Display Spark configuration\n",
    "print(\"\\nKey Spark Configurations:\")\n",
    "conf = spark.sparkContext.getConf()\n",
    "important_configs = [\n",
    "    'spark.app.name',\n",
    "    'spark.driver.memory',\n",
    "    'spark.executor.memory',\n",
    "    'spark.sql.shuffle.partitions',\n",
    "    'spark.sql.adaptive.enabled'\n",
    "]\n",
    "for config in important_configs:\n",
    "    print(f\"  {config}: {conf.get(config, 'Not set')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69d59b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "def timer_decorator(func):\n",
    "    \"\"\"\n",
    "    Decorator to measure execution time of functions\n",
    "    \n",
    "    Args:\n",
    "        func: Function to time\n",
    "        \n",
    "    Returns:\n",
    "        Wrapped function with timing\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"\\n    {func.__name__} completed in {duration:.2f} seconds\")\n",
    "        return result, duration\n",
    "    return wrapper\n",
    "\n",
    "def count_with_time(df, description=\"Operation\"):\n",
    "    \"\"\"\n",
    "    Count DataFrame rows with timing\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        description: Operation description\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (count, duration)\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    count = df.count()\n",
    "    duration = time.time() - start\n",
    "    print(f\"    {description}: {count:,} rows in {duration:.2f}s\")\n",
    "    return count, duration\n",
    "\n",
    "def show_partitions(df, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Display partition information\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        name: DataFrame name for display\n",
    "    \"\"\"\n",
    "    num_partitions = df.rdd.getNumPartitions()\n",
    "    print(f\"{name} partitions: {num_partitions}\")\n",
    "    return num_partitions\n",
    "\n",
    "def save_results(data, filename):\n",
    "    \"\"\"\n",
    "    Save results to CSV in results directory\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary or DataFrame\n",
    "        filename: Output filename\n",
    "    \"\"\"\n",
    "    results_dir = project_root / \"results\"\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    filepath = results_dir / filename\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        pd.DataFrame([data]).to_csv(filepath, index=False)\n",
    "    else:\n",
    "        data.to_csv(filepath, index=False)\n",
    "    \n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c742168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data paths configured:\n",
      "  Transactions: s3a://bigdata-ecommerce/raw/transactions\n",
      "  Customers: s3a://bigdata-ecommerce/raw/customers.parquet\n",
      "  Products: s3a://bigdata-ecommerce/raw/products.parquet\n",
      "\n",
      "Testing Delta Lake on MinIO...\n",
      "  Delta write successful to: s3a://bigdata-ecommerce/delta-lake-test/test_table\n",
      "  Delta read successful: 2 rows\n",
      "\n",
      "Delta Lake on MinIO is working!\n"
     ]
    }
   ],
   "source": [
    "# VERIFY PATHS & DELTA LAKE\n",
    "\n",
    "# Check MinIO paths\n",
    "transactions_path = get_s3a_path(\"raw/\", \"transactions\")\n",
    "customers_path = get_s3a_path(\"raw/\", \"customers.parquet\")\n",
    "products_path = get_s3a_path(\"raw/\", \"products.parquet\")\n",
    "\n",
    "print(\"Data paths configured:\")\n",
    "print(f\"  Transactions: {transactions_path}\")\n",
    "print(f\"  Customers: {customers_path}\")\n",
    "print(f\"  Products: {products_path}\")\n",
    "\n",
    "\n",
    "# Quick test to ensure Delta Lake works with MinIO\n",
    "print(\"\\nTesting Delta Lake on MinIO...\")\n",
    "\n",
    "test_delta_path = get_s3a_path(\"delta-lake-test/\", \"test_table\")\n",
    "\n",
    "try:\n",
    "    # Create simple test DataFrame\n",
    "    test_data = [(1, \"test\", 100), (2, \"test2\", 200)]\n",
    "    test_df = spark.createDataFrame(test_data, [\"id\", \"name\", \"value\"])\n",
    "    \n",
    "    # Write as Delta\n",
    "    test_df.write.format(\"delta\").mode(\"overwrite\").save(test_delta_path)\n",
    "    print(f\"  Delta write successful to: {test_delta_path}\")\n",
    "    \n",
    "    # Read back\n",
    "    read_df = spark.read.format(\"delta\").load(test_delta_path)\n",
    "    read_count = read_df.count()\n",
    "    print(f\"  Delta read successful: {read_count} rows\")\n",
    "    \n",
    "    print(\"\\nDelta Lake on MinIO is working!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nDelta Lake test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd47d34",
   "metadata": {},
   "source": [
    "**Interfaccia MinIO**\n",
    "\n",
    "<img src=\"./screenshots/03_minio_interface.png\" alt=\"MinIo Interface\" >\n",
    "\n",
    "**MinIO Storage**\n",
    "\n",
    "<img src=\"./screenshots/03_minio_data.png\" alt=\"MinIo Data\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb819026",
   "metadata": {},
   "source": [
    "# Architettura PySpark\n",
    "\n",
    "PySpark utilizza la **lazy evaluation**: le trasformazioni non vengono eseguite immediatamente ma vengono registrate in un **DAG** (Directed Acyclic Graph). Il **Catalyst Optimizer** analizza il grafo per trovare il piano di esecuzione più efficiente, che viene avviato solo quando si invoca un'action.\n",
    "\n",
    "**Transformations**\n",
    "\n",
    "Creano nuovi DataFrame senza eseguire calcoli immediati. Si dividono in due categorie:\n",
    "\n",
    "1. **Narrow transformations**: operano su singole partizioni, non richiedono shuffle e network\n",
    "    - `select()`, `filter()`, `withColumn()`\n",
    "\n",
    "2. **Wide transformations**: riorganizzano dati tra partizioni, richiedono shuffle e network\n",
    "    - `groupBy()`, `join()`, `orderBy()`\n",
    "\n",
    "**Actions**\n",
    "\n",
    "Triggherano l'esecuzione del piano computazionale e ritornano risultati concreti all'utente.\n",
    "- `count()`, `collect()`, `show()`\n",
    "\n",
    "\n",
    "Due principi fondamentali caratterizzano PySpark:\n",
    "\n",
    "1. **Immutability**: ogni DataFrame è immutabile e ogni trasformazione genera un nuovo DataFrame\n",
    "\n",
    "2. **Distributed Computing**: partiziona i dati su più executors permettendo l'esecuzione parallela delle operazioni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "061095c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOAD DATA\n",
      "======================================================================\n",
      "\n",
      "Sample Customers: 1,000,000 rows\n",
      "Sample Products: 50,000 rows\n",
      "Sample Transactions: 100,000,000 rows\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOAD DATA\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Load data\n",
    "customers = spark.read.parquet(customers_path)\n",
    "products = spark.read.parquet(products_path)\n",
    "transactions = spark.read.parquet(transactions_path)\n",
    "\n",
    "print(f\"Sample Customers: {customers.count():,} rows\")\n",
    "print(f\"Sample Products: {products.count():,} rows\")\n",
    "print(f\"Sample Transactions: {transactions.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf92523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LAZY EVALUATION DEMO\n",
      "======================================================================\n",
      "\n",
      "1. Creating transformation (lazy)...\n",
      "   Transformation defined (no execution yet)\n",
      "\n",
      "2. Triggering action (execution happens now)...\n",
      "   Executed! Found 8,233,328 high-value transactions\n",
      "   Execution time: 3.05s\n"
     ]
    }
   ],
   "source": [
    "# LAZY EVALUATION DEMO\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LAZY EVALUATION DEMO\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Create transformation (no execution yet)\n",
    "print(\"1. Creating transformation (lazy)...\")\n",
    "filtered_transactions = transactions.filter(col(\"final_amount\") > 1000)\n",
    "print(\"   Transformation defined (no execution yet)\")\n",
    "\n",
    "# Trigger action (execution happens now)\n",
    "print(\"\\n2. Triggering action (execution happens now)...\")\n",
    "start = time.time()\n",
    "result_count = filtered_transactions.count()\n",
    "duration = time.time() - start\n",
    "\n",
    "print(f\"   Executed! Found {result_count:,} high-value transactions\")\n",
    "print(f\"   Execution time: {duration:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4844dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRANSFORMATION EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "1. SELECT transformation:\n",
      "   Selected 3 columns from customers\n",
      "\n",
      "2. FILTER transformation:\n",
      "   Filtered VIP customers (lazy)\n",
      "\n",
      "3. WITHCOLUMN transformation:\n",
      "   Added margin_pct column (lazy)\n",
      "\n",
      "4. GROUPBY transformation (with agg):\n",
      "   Grouped by segment (lazy)\n",
      "\n",
      "======================================================================\n",
      "ACTIONS EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "1. COUNT action:\n",
      "   VIP customers: 50,000 (executed in 0.26s)\n",
      "\n",
      "2. SHOW action:\n",
      "   First 5 VIP customers:\n",
      "+-----------+--------------------+---+\n",
      "|customer_id|                name|age|\n",
      "+-----------+--------------------+---+\n",
      "|  C00000017|       Aria Giannini| 26|\n",
      "|  C00000032|Gelsomina Renzi-P...| 26|\n",
      "|  C00000042|  Sig.ra Michela Emo| 63|\n",
      "|  C00000062|  Gustavo Spanevello| 31|\n",
      "|  C00000066|Sig.ra Lara Randazzo| 67|\n",
      "+-----------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "3. COLLECT action:\n",
      "   Collected 3 rows to driver\n",
      "   - C00000017: Aria Giannini\n",
      "   - C00000032: Gelsomina Renzi-Piane\n",
      "   - C00000042: Sig.ra Michela Emo\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMATION & ACTIONS EXAMPLES\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRANSFORMATION EXAMPLES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1. Select\n",
    "print(\"1. SELECT transformation:\")\n",
    "customer_basics = customers.select(\"customer_id\", \"name\", \"customer_segment\")\n",
    "print(f\"   Selected 3 columns from customers\")\n",
    "\n",
    "# 2. Filter\n",
    "print(\"\\n2. FILTER transformation:\")\n",
    "vip_customers = customers.filter(col(\"customer_segment\") == \"VIP\")\n",
    "print(f\"   Filtered VIP customers (lazy)\")\n",
    "\n",
    "# 3. WithColumn\n",
    "print(\"\\n3. WITHCOLUMN transformation:\")\n",
    "transactions_with_margin = transactions.withColumn(\n",
    "    \"margin_pct\",\n",
    "    spark_round((col(\"final_amount\") / col(\"total_amount\")) * 100, 2)\n",
    ")\n",
    "print(f\"   Added margin_pct column (lazy)\")\n",
    "\n",
    "# 4. GroupBy (transformation, not action!)\n",
    "print(\"\\n4. GROUPBY transformation (with agg):\")\n",
    "segment_stats = customers.groupBy(\"customer_segment\").agg(\n",
    "    spark_count(\"*\").alias(\"count\"),\n",
    "    avg(\"age\").alias(\"avg_age\")\n",
    ")\n",
    "print(f\"   Grouped by segment (lazy)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACTIONS EXAMPLES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1. Count\n",
    "print(\"1. COUNT action:\")\n",
    "start = time.time()\n",
    "vip_count = vip_customers.count()\n",
    "duration = time.time() - start\n",
    "print(f\"   VIP customers: {vip_count:,} (executed in {duration:.2f}s)\")\n",
    "\n",
    "# 2. Show\n",
    "print(\"\\n2. SHOW action:\")\n",
    "print(\"   First 5 VIP customers:\")\n",
    "vip_customers.select(\"customer_id\", \"name\", \"age\").show(5)\n",
    "\n",
    "# 3. Collect\n",
    "print(\"\\n3. COLLECT action:\")\n",
    "top_3_vip = vip_customers.select(\"customer_id\", \"name\").limit(3).collect()\n",
    "print(f\"   Collected {len(top_3_vip)} rows to driver\")\n",
    "for row in top_3_vip:\n",
    "    print(f\"   - {row['customer_id']}: {row['name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafab187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATAFRAME OPERATIONS\n",
      "======================================================================\n",
      "\n",
      "1. DataFrame Info:\n",
      "   Columns: 13\n",
      "   Column names: ['customer_id', 'name', 'email', 'phone', 'address', 'city', 'region', 'country', 'postal_code', 'registration_date', 'customer_segment', 'age', 'gender']\n",
      "   Partitions: 16\n",
      "\n",
      "2. Schema:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- registration_date: date (nullable = true)\n",
      " |-- customer_segment: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "\n",
      "3. Describe (summary statistics):\n",
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|           1000000|\n",
      "|   mean|         46.506383|\n",
      "| stddev|16.733604371767242|\n",
      "|    min|                18|\n",
      "|    max|                75|\n",
      "+-------+------------------+\n",
      "\n",
      "\n",
      "4. Distinct values:\n",
      "   Customer segments:\n",
      "+----------------+\n",
      "|customer_segment|\n",
      "+----------------+\n",
      "|         Regular|\n",
      "|      Occasional|\n",
      "|             VIP|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DATAFRAME OPERATIONS\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATAFRAME OPERATIONS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Basic info\n",
    "print(\"1. DataFrame Info:\")\n",
    "print(f\"   Columns: {len(customers.columns)}\")\n",
    "print(f\"   Column names: {customers.columns}\")\n",
    "print(f\"   Partitions: {customers.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Schema\n",
    "print(\"\\n2. Schema:\")\n",
    "customers.printSchema()\n",
    "\n",
    "# Describe\n",
    "print(\"\\n3. Describe (summary statistics):\")\n",
    "customers.select(\"age\").describe().show()\n",
    "\n",
    "# Distinct\n",
    "print(\"\\n4. Distinct values:\")\n",
    "segments = customers.select(\"customer_segment\").distinct()\n",
    "print(f\"   Customer segments:\")\n",
    "segments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65782eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUERY EXECUTION PLAN\n",
      "======================================================================\n",
      "\n",
      "Chained operations:\n",
      "  1. Filter completed transactions\n",
      "  2. Filter amount > 50\n",
      "  3. Select specific columns\n",
      "  4. Add high_value column\n",
      "  5. Order by amount\n",
      "  6. Limit to top 10\n",
      "\n",
      "Result:\n",
      "+--------------+-----------+------------+----------------+----------+\n",
      "|transaction_id|customer_id|final_amount|transaction_date|high_value|\n",
      "+--------------+-----------+------------+----------------+----------+\n",
      "|   T0098657043|  C00472985|     8997.85|      2024-04-29|       Yes|\n",
      "|   T0025697854|  C00291574|     8994.05|      2024-09-18|       Yes|\n",
      "|   T0075933682|  C00968650|     8991.83|      2025-07-31|       Yes|\n",
      "|   T0088081874|  C00633627|     8982.91|      2025-03-26|       Yes|\n",
      "|   T0084912906|  C00797292|     8979.11|      2025-10-28|       Yes|\n",
      "|   T0077918434|  C00794352|     8975.02|      2025-08-03|       Yes|\n",
      "|   T0057242121|  C00663478|     8974.71|      2024-03-19|       Yes|\n",
      "|   T0030245510|  C00101347|     8964.92|      2024-12-09|       Yes|\n",
      "|   T0000997695|  C00138589|     8958.15|      2024-04-04|       Yes|\n",
      "|   T0029100685|  C00123143|     8956.97|      2024-01-30|       Yes|\n",
      "+--------------+-----------+------------+----------------+----------+\n",
      "\n",
      "\n",
      "Spark optimizes this chain before execution (Catalyst optimizer)\n",
      "\n",
      "PHYSICAL PLAN (what to do):\n",
      "----------------------------------------------------------------------\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=10, orderBy=[final_amount#1572 DESC NULLS LAST], output=[transaction_id#1564,customer_id#1565,final_amount#1572,transaction_date#1574,high_value#1877])\n",
      "+- *(1) Project [transaction_id#1564, customer_id#1565, final_amount#1572, transaction_date#1574, CASE WHEN (final_amount#1572 > 1000.0) THEN Yes ELSE No END AS high_value#1877]\n",
      "   +- *(1) Filter (((isnotnull(status#1577) AND isnotnull(final_amount#1572)) AND (status#1577 = completed)) AND (final_amount#1572 > 500.0))\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [transaction_id#1564,customer_id#1565,final_amount#1572,transaction_date#1574,status#1577,year#1578,month#1579] Batched: true, DataFilters: [isnotnull(status#1577), isnotnull(final_amount#1572), (status#1577 = completed), (final_amount#1..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://bigdata-ecommerce/raw/transactions], PartitionFilters: [], PushedFilters: [IsNotNull(status), IsNotNull(final_amount), EqualTo(status,completed), GreaterThan(final_amount,..., ReadSchema: struct<transaction_id:string,customer_id:string,final_amount:double,transaction_date:date,status:...\n",
      "\n",
      "\n",
      "\n",
      "LOGICAL PLAN (how to do it):\n",
      "----------------------------------------------------------------------\n",
      "== Parsed Logical Plan ==\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [final_amount#1572 DESC NULLS LAST], true\n",
      "      +- Project [transaction_id#1564, customer_id#1565, final_amount#1572, transaction_date#1574, CASE WHEN (final_amount#1572 > cast(1000 as double)) THEN Yes ELSE No END AS high_value#1877]\n",
      "         +- Project [transaction_id#1564, customer_id#1565, final_amount#1572, transaction_date#1574]\n",
      "            +- Filter (final_amount#1572 > cast(500 as double))\n",
      "               +- Filter (status#1577 = completed)\n",
      "                  +- Relation [transaction_id#1564,customer_id#1565,product_id#1566,quantity#1567L,unit_price#1568,total_amount#1569,discount_pct#1570,discount_amount#1571,final_amount#1572,shipping_cost#1573,transaction_date#1574,transaction_timestamp#1575,payment_method#1576,status#1577,year#1578,month#1579] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "transaction_id: string, customer_id: string, final_amount: double, transaction_date: date, high_value: string\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [final_amount#1572 DESC NULLS LAST], true\n",
      "      +- Project [transaction_id#1564, customer_id#1565, final_amount#1572, transaction_date#1574, CASE WHEN (final_amount#1572 > cast(1000 as double)) THEN Yes ELSE No END AS high_value#1877]\n",
      "         +- Project [transaction_id#1564, customer_id#1565, final_amount#1572, transaction_date#1574]\n",
      "            +- Filter (final_amount#1572 > cast(500 as double))\n",
      "               +- Filter (status#1577 = completed)\n",
      "                  +- Relation [transaction_id#1564,customer_id#1565,product_id#1566,quantity#1567L,unit_price#1568,total_amount#1569,discount_pct#1570,discount_amount#1571,final_amount#1572,shipping_cost#1573,transaction_date#1574,transaction_timestamp#1575,payment_method#1576,status#1577,year#1578,month#1579] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [final_amount#1572 DESC NULLS LAST], true\n",
      "      +- Project [transaction_id#1564, customer_id#1565, final_amount#1572, transaction_date#1574, CASE WHEN (final_amount#1572 > 1000.0) THEN Yes ELSE No END AS high_value#1877]\n",
      "         +- Filter ((isnotnull(status#1577) AND isnotnull(final_amount#1572)) AND ((status#1577 = completed) AND (final_amount#1572 > 500.0)))\n",
      "            +- Relation [transaction_id#1564,customer_id#1565,product_id#1566,quantity#1567L,unit_price#1568,total_amount#1569,discount_pct#1570,discount_amount#1571,final_amount#1572,shipping_cost#1573,transaction_date#1574,transaction_timestamp#1575,payment_method#1576,status#1577,year#1578,month#1579] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=10, orderBy=[final_amount#1572 DESC NULLS LAST], output=[transaction_id#1564,customer_id#1565,final_amount#1572,transaction_date#1574,high_value#1877])\n",
      "+- *(1) Project [transaction_id#1564, customer_id#1565, final_amount#1572, transaction_date#1574, CASE WHEN (final_amount#1572 > 1000.0) THEN Yes ELSE No END AS high_value#1877]\n",
      "   +- *(1) Filter (((isnotnull(status#1577) AND isnotnull(final_amount#1572)) AND (status#1577 = completed)) AND (final_amount#1572 > 500.0))\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [transaction_id#1564,customer_id#1565,final_amount#1572,transaction_date#1574,status#1577,year#1578,month#1579] Batched: true, DataFilters: [isnotnull(status#1577), isnotnull(final_amount#1572), (status#1577 = completed), (final_amount#1..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://bigdata-ecommerce/raw/transactions], PartitionFilters: [], PushedFilters: [IsNotNull(status), IsNotNull(final_amount), EqualTo(status,completed), GreaterThan(final_amount,..., ReadSchema: struct<transaction_id:string,customer_id:string,final_amount:double,transaction_date:date,status:...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# QUERY EXECUTION PLAN\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"QUERY EXECUTION PLAN\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Complex chain of transformations\n",
    "result = (\n",
    "    transactions\n",
    "    .filter(col(\"status\") == \"completed\")\n",
    "    .filter(col(\"final_amount\") > 500)\n",
    "    .select(\n",
    "        \"transaction_id\",\n",
    "        \"customer_id\",\n",
    "        \"final_amount\",\n",
    "        \"transaction_date\"\n",
    "    )\n",
    "    .withColumn(\"high_value\", when(col(\"final_amount\") > 1000, \"Yes\").otherwise(\"No\"))\n",
    "    .orderBy(desc(\"final_amount\"))\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "print(\"Chained operations:\")\n",
    "print(\"  1. Filter completed transactions\")\n",
    "print(\"  2. Filter amount > 50\")\n",
    "print(\"  3. Select specific columns\")\n",
    "print(\"  4. Add high_value column\")\n",
    "print(\"  5. Order by amount\")\n",
    "print(\"  6. Limit to top 10\")\n",
    "\n",
    "print(\"\\nResult:\")\n",
    "result.show()\n",
    "\n",
    "print(\"\\nSpark optimizes this chain before execution (Catalyst optimizer)\")\n",
    "\n",
    "# Show phisical plan\n",
    "print(\"\\nPHYSICAL PLAN (what to do):\")\n",
    "print(\"-\" * 70)\n",
    "result.explain(extended=False)\n",
    "\n",
    "# Show logical plan\n",
    "print(\"\\nLOGICAL PLAN (how to do it):\")\n",
    "print(\"-\" * 70)\n",
    "result.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747b102",
   "metadata": {},
   "source": [
    "# Spark SQL & Joins\n",
    "\n",
    "Spark SQL permette di eseguire query SQL distribuite su DataFrame con ottimizzazione automatica tramite Catalyst.\n",
    "\n",
    "**Temporary Views** - per registrare DataFrame come tabelle SQL:\n",
    "- **Temp View**: disponibile solo nella SparkSession corrente, si cancella al termine della sessione\n",
    "- **Global Temp View**: accessibile a tutte le sessioni Spark nell'applicazione, utile per condividere dati tra notebook\n",
    "\n",
    "**Operazioni JOIN**:\n",
    "- Supporta tutte le tipologie standard: `inner`, `left`, `right`, `outer`\n",
    "- Join standard richiedono shuffle (riorganizzazione dati tra partizioni) ed è dunque lenta\n",
    "\n",
    "**Broadcast Join Optimization**:\n",
    "Quando una tabella è piccola (< 10MB default), Spark invia una copia completa della tabella a tutti gli executors. Il join avviene localmente su ogni executor senza shuffle della tabella grande, risultando molto più veloce. Evitare di utilizzarlo con tabelle >100MB (rischio OOM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16dabcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "REGISTERING TEMPORARY VIEWS\n",
      "======================================================================\n",
      "\n",
      "Registered views:\n",
      "   - transactions\n",
      "   - customers\n",
      "   - products\n",
      "\n",
      "Available tables:\n",
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|         |   customers|      false|\n",
      "|         |    products|      false|\n",
      "|         |transactions|      false|\n",
      "+---------+------------+-----------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SIMPLE SQL QUERIES\n",
      "======================================================================\n",
      "\n",
      "1. Top 10 high-value transactions:\n",
      "+--------------+-----------+------------+----------------+\n",
      "|transaction_id|customer_id|final_amount|transaction_date|\n",
      "+--------------+-----------+------------+----------------+\n",
      "|   T0098657043|  C00472985|     8997.85|      2024-04-29|\n",
      "|   T0025697854|  C00291574|     8994.05|      2024-09-18|\n",
      "|   T0075933682|  C00968650|     8991.83|      2025-07-31|\n",
      "|   T0088081874|  C00633627|     8982.91|      2025-03-26|\n",
      "|   T0084912906|  C00797292|     8979.11|      2025-10-28|\n",
      "|   T0077918434|  C00794352|     8975.02|      2025-08-03|\n",
      "|   T0057242121|  C00663478|     8974.71|      2024-03-19|\n",
      "|   T0030245510|  C00101347|     8964.92|      2024-12-09|\n",
      "|   T0000997695|  C00138589|     8958.15|      2024-04-04|\n",
      "|   T0029100685|  C00123143|     8956.97|      2024-01-30|\n",
      "+--------------+-----------+------------+----------------+\n",
      "\n",
      "\n",
      "2. Transactions by status:\n",
      "+---------+--------+--------------------+-----------------+\n",
      "|   status|   count|       total_revenue|       avg_amount|\n",
      "+---------+--------+--------------------+-----------------+\n",
      "|completed|95001380|4.114988022378023E10|433.1503418558786|\n",
      "| refunded| 2000246|                 0.0|              0.0|\n",
      "|cancelled| 2998374|                 0.0|              0.0|\n",
      "+---------+--------+--------------------+-----------------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "GROUPBY QUERIES\n",
      "======================================================================\n",
      "\n",
      "1. Analysis by customer segment:\n",
      "+----------------+-------------+-----------------+-----------+\n",
      "|customer_segment|num_customers|          avg_age|num_regions|\n",
      "+----------------+-------------+-----------------+-----------+\n",
      "|      Occasional|       600000|         46.50292|         20|\n",
      "|         Regular|       350000|46.51125428571429|         20|\n",
      "|             VIP|        50000|         46.51384|         20|\n",
      "+----------------+-------------+-----------------+-----------+\n",
      "\n",
      "\n",
      "2. Sales by product category:\n",
      "+--------------+----------------+--------------------+---------------------+\n",
      "|      category|num_transactions|       total_revenue|avg_transaction_value|\n",
      "+--------------+----------------+--------------------+---------------------+\n",
      "|   Electronics|         8440996|1.808401946341002...|     2142.40351060586|\n",
      "|        Beauty|        18830251|     4.94147524344E9|    262.4221654528131|\n",
      "|       Fashion|        11114326| 4.485214480170002E9|    403.5525393235723|\n",
      "|        Sports|         8003459| 3.843931032420006E9|   480.28371638063066|\n",
      "|        Health|        10168864|2.6793183183899913E9|   263.48255993884777|\n",
      "|Home & Kitchen|        11388343|2.5572627128400044E9|   224.55090374780636|\n",
      "|          Toys|         7011573| 1.941004102529999E9|    276.8286235528032|\n",
      "|    Automotive|         6101310|1.6060144092299986E9|     263.224522148522|\n",
      "|          Food|         7470202| 6.596727886100025E8|    88.30722229599715|\n",
      "|         Books|         6472056| 3.519676727399997E8|    54.38266800225457|\n",
      "+--------------+----------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL QUERIES\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"REGISTERING TEMPORARY VIEWS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Register all tables\n",
    "transactions.createOrReplaceTempView(\"transactions\")\n",
    "customers.createOrReplaceTempView(\"customers\")\n",
    "products.createOrReplaceTempView(\"products\")\n",
    "\n",
    "print(\"Registered views:\")\n",
    "print(\"   - transactions\")\n",
    "print(\"   - customers\")\n",
    "print(\"   - products\")\n",
    "\n",
    "# List all temporary views\n",
    "print(\"\\nAvailable tables:\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SIMPLE SQL QUERIES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Query 1: SELECT with WHERE\n",
    "print(\"1. Top 10 high-value transactions:\")\n",
    "query1 = \"\"\"\n",
    "SELECT \n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    final_amount,\n",
    "    transaction_date\n",
    "FROM transactions\n",
    "WHERE status = 'completed' AND final_amount > 1000\n",
    "ORDER BY final_amount DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "result1 = spark.sql(query1)\n",
    "result1.show()\n",
    "\n",
    "# Query 2: Aggregation\n",
    "print(\"\\n2. Transactions by status:\")\n",
    "query2 = \"\"\"\n",
    "SELECT \n",
    "    status,\n",
    "    COUNT(*) as count,\n",
    "    SUM(final_amount) as total_revenue,\n",
    "    AVG(final_amount) as avg_amount\n",
    "FROM transactions\n",
    "GROUP BY status\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "result2 = spark.sql(query2)\n",
    "result2.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GROUPBY QUERIES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Customer segments analysis\n",
    "print(\"1. Analysis by customer segment:\")\n",
    "query3 = \"\"\"\n",
    "SELECT \n",
    "    customer_segment,\n",
    "    COUNT(*) as num_customers,\n",
    "    AVG(age) as avg_age,\n",
    "    COUNT(DISTINCT region) as num_regions\n",
    "FROM customers\n",
    "GROUP BY customer_segment\n",
    "ORDER BY num_customers DESC\n",
    "\"\"\"\n",
    "result3 = spark.sql(query3)\n",
    "result3.show()\n",
    "\n",
    "# Product categories\n",
    "print(\"\\n2. Sales by product category:\")\n",
    "query4 = \"\"\"\n",
    "SELECT \n",
    "    p.category,\n",
    "    COUNT(DISTINCT t.transaction_id) as num_transactions,\n",
    "    SUM(t.final_amount) as total_revenue,\n",
    "    AVG(t.final_amount) as avg_transaction_value\n",
    "FROM transactions t\n",
    "JOIN products p ON t.product_id = p.product_id\n",
    "WHERE t.status = 'completed'\n",
    "GROUP BY p.category\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "result4 = spark.sql(query4)\n",
    "result4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f379d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INNER JOIN\n",
      "======================================================================\n",
      "\n",
      "Transactions with customer information:\n",
      "+--------------+------------+--------------+----------------+------+\n",
      "|transaction_id|final_amount| customer_name|customer_segment|region|\n",
      "+--------------+------------+--------------+----------------+------+\n",
      "|   T0046461295|      535.99|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0021244874|      213.75|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0028990321|      113.57|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0023305546|      363.31|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0001515969|      539.96|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0036288342|      228.92|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0045058435|      132.58|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0049883716|     3508.96|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0033290666|      232.66|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0015407945|      424.87|Amadeo Cabrini|      Occasional|Molise|\n",
      "+--------------+------------+--------------+----------------+------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "LEFT JOIN\n",
      "======================================================================\n",
      "\n",
      "Top 20 customers by total spent:\n",
      "+-----------+--------------------+----------------+----------------+------------------+\n",
      "|customer_id|                name|customer_segment|num_transactions|       total_spent|\n",
      "+-----------+--------------------+----------------+----------------+------------------+\n",
      "|  C00271911|  Sig.ra Pina Golino|             VIP|             349|279918.64999999997|\n",
      "|  C00387531|     Lodovico Fusani|             VIP|             323|260226.26999999987|\n",
      "|  C00531659|     Costanzo Nordio|             VIP|             318|256164.09000000003|\n",
      "|  C00458014|Nicolò Mezzetta-B...|             VIP|             304|251168.44999999987|\n",
      "|  C00343149|Dott. Fortunata B...|             VIP|             334| 249635.8800000003|\n",
      "|  C00941483|   Valerio Tomaselli|             VIP|             327|248252.66000000024|\n",
      "|  C00984397|      Uberto Fagotto|             VIP|             332|247968.63000000012|\n",
      "|  C00371245|Sig.ra Aurora Lol...|             VIP|             343|247284.47000000003|\n",
      "|  C00479841|         Elisa Verri|             VIP|             347|246091.26999999993|\n",
      "|  C00665141|     Durante Pedroni|             VIP|             340|245274.05999999994|\n",
      "|  C00480031|   Sig.ra Dina Blasi|             VIP|             307| 244540.3899999998|\n",
      "|  C00529766|     Amleto Sabatini|             VIP|             313|243205.15999999986|\n",
      "|  C00357562|   Carmelo Tamborini|             VIP|             337|243151.69999999978|\n",
      "|  C00973129|      Stefano Ciampi|             VIP|             337|         242805.41|\n",
      "|  C00579353|Lidia Battaglia-C...|             VIP|             339|241701.30999999988|\n",
      "|  C00658949|       Virgilio Muti|             VIP|             317|241646.07999999964|\n",
      "|  C00642954|Dott. Gabriella S...|             VIP|             314|241130.11000000002|\n",
      "|  C00903647|        Priscilla Fo|             VIP|             333|241124.07999999993|\n",
      "|  C00989351|       Giulio Trotta|             VIP|             294|241079.78999999998|\n",
      "|  C00010850|Evangelista Manac...|             VIP|             304| 240939.7799999999|\n",
      "+-----------+--------------------+----------------+----------------+------------------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "BROADCAST JOIN OPTIMIZATION\n",
      "======================================================================\n",
      "\n",
      "Products table size: 50,000 rows\n",
      "Transactions table size: 100,000,000 rows\n",
      "\n",
      "1. Standard Join:\n",
      "+--------------+------------+--------------------+-----------+\n",
      "|transaction_id|final_amount|        product_name|   category|\n",
      "+--------------+------------+--------------------+-----------+\n",
      "|   T0050000090|      218.13|L'Oréal Fragrance...|     Beauty|\n",
      "|   T0050000116|      258.41|Nature's Bounty S...|     Health|\n",
      "|   T0050000120|      593.12|Zara Accessories ...|    Fashion|\n",
      "|   T0050000126|       130.3|Under Armour Acce...|     Sports|\n",
      "|   T0050000134|     1667.78|HP Accessories Oc...|Electronics|\n",
      "|   T0050000148|      612.97|Mattel Board Game...|       Toys|\n",
      "|   T0050000164|      221.57|L'Oréal Fragrance...|     Beauty|\n",
      "|   T0050000187|      206.82|Nature's Bounty W...|     Health|\n",
      "|   T0050000241|         0.0|Mattel Action Fig...|       Toys|\n",
      "|   T0050000331|       21.87|       3M Parts Unde| Automotive|\n",
      "+--------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "2. Broadcast Join:\n",
      "+--------------+------------+--------------------+-----------+\n",
      "|transaction_id|final_amount|        product_name|   category|\n",
      "+--------------+------------+--------------------+-----------+\n",
      "|   T0050000090|      218.13|L'Oréal Fragrance...|     Beauty|\n",
      "|   T0050000116|      258.41|Nature's Bounty S...|     Health|\n",
      "|   T0050000120|      593.12|Zara Accessories ...|    Fashion|\n",
      "|   T0050000126|       130.3|Under Armour Acce...|     Sports|\n",
      "|   T0050000134|     1667.78|HP Accessories Oc...|Electronics|\n",
      "|   T0050000148|      612.97|Mattel Board Game...|       Toys|\n",
      "|   T0050000164|      221.57|L'Oréal Fragrance...|     Beauty|\n",
      "|   T0050000187|      206.82|Nature's Bounty W...|     Health|\n",
      "|   T0050000241|         0.0|Mattel Action Fig...|       Toys|\n",
      "|   T0050000331|       21.87|       3M Parts Unde| Automotive|\n",
      "+--------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Broadcast join speedup: 2.23x\n",
      "   Standard join: 0.37s\n",
      "   Broadcast join: 0.16s\n",
      "Saved: 03_join_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# JOIN OPERATIONS\n",
    "\n",
    "# Inner Join\n",
    "print(\"=\"*70)\n",
    "print(\"INNER JOIN\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Inner join: transactions with customer info\n",
    "query5 = \"\"\"\n",
    "SELECT \n",
    "    t.transaction_id,\n",
    "    t.final_amount,\n",
    "    c.name as customer_name,\n",
    "    c.customer_segment,\n",
    "    c.region\n",
    "FROM transactions t\n",
    "INNER JOIN customers c ON t.customer_id = c.customer_id\n",
    "WHERE t.final_amount > 100\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "result5 = spark.sql(query5)\n",
    "print(\"Transactions with customer information:\")\n",
    "result5.show()\n",
    "\n",
    "# Left Join\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LEFT JOIN\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# All customers with their transaction count (including those with 0 transactions)\n",
    "query6 = \"\"\"\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.name,\n",
    "    c.customer_segment,\n",
    "    COUNT(t.transaction_id) as num_transactions,\n",
    "    COALESCE(SUM(t.final_amount), 0) as total_spent\n",
    "FROM customers c\n",
    "LEFT JOIN transactions t ON c.customer_id = t.customer_id \n",
    "    AND t.status = 'completed'\n",
    "GROUP BY c.customer_id, c.name, c.customer_segment\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "result6 = spark.sql(query6)\n",
    "print(\"Top 20 customers by total spent:\")\n",
    "result6.show()\n",
    "\n",
    "\n",
    "# Broadcast Join Optimization\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BROADCAST JOIN OPTIMIZATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Products table is small - good candidate for broadcast\n",
    "print(f\"Products table size: {products.count():,} rows\")\n",
    "print(f\"Transactions table size: {transactions.count():,} rows\")\n",
    "\n",
    "# Standard join (without broadcast)\n",
    "print(\"\\n1. Standard Join:\")\n",
    "standard_join = transactions.join(\n",
    "    products,\n",
    "    transactions.product_id == products.product_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    transactions.transaction_id,\n",
    "    transactions.final_amount,\n",
    "    products.product_name,\n",
    "    products.category\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "standard_join.show(10)\n",
    "standard_time = time.time() - start\n",
    "\n",
    "# Broadcast join\n",
    "print(\"\\n2. Broadcast Join:\")\n",
    "broadcast_join = transactions.join(\n",
    "    broadcast(products),\n",
    "    transactions.product_id == products.product_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    transactions.transaction_id,\n",
    "    transactions.final_amount,\n",
    "    products.product_name,\n",
    "    products.category\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "broadcast_join.show(10)\n",
    "broadcast_time = time.time() - start\n",
    "\n",
    "# Compare\n",
    "speedup = (standard_time / broadcast_time) if broadcast_time > 0 else 1\n",
    "print(f\"\\nBroadcast join speedup: {speedup:.2f}x\")\n",
    "print(f\"   Standard join: {standard_time:.2f}s\")\n",
    "print(f\"   Broadcast join: {broadcast_time:.2f}s\")\n",
    "\n",
    "# Save comparison\n",
    "join_comparison = pd.DataFrame({\n",
    "    'Join_Type': ['Standard', 'Broadcast'],\n",
    "    'Time_s': [standard_time, broadcast_time]\n",
    "})\n",
    "save_results(join_comparison, \"03_join_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a667494",
   "metadata": {},
   "source": [
    "# Performance Tuning & Monitoring\n",
    "\n",
    "L'ottimizzazione delle performance in PySpark si basa su quattro tecniche principali: **partitioning** per distribuire i dati in modo ottimale, **caching** per evitare ricomputazioni, **broadcast join** per ridurre lo shuffle, e **configuration tuning** per ottimizzare i parametri Spark.\n",
    "\n",
    "## Partitioning\n",
    "\n",
    "**Partition Size** - regola generale 128MB per partition (standard Hadoop):\n",
    "- Partizioni troppo piccole → overhead eccessivo\n",
    "- Partizioni troppo grandi → memory pressure e stragglers\n",
    "\n",
    "**Partition Count**:\n",
    "- Minimum: `num_cores * 2`\n",
    "- Optimal: `num_cores * 4`\n",
    "\n",
    "**Repartition vs Coalesce**:\n",
    "- `repartition(n)`: esegue shuffle, utile per aumentare/ridurre partizioni e load balancing\n",
    "- `coalesce(n)`: no shuffle, solo per ridurre partizioni, più veloce\n",
    "\n",
    "## Caching\n",
    "\n",
    "**Quando cachare**:\n",
    "- DataFrame usato multiple volte nella stessa sessione\n",
    "- Dopo operazioni costose (join, aggregazioni complesse)\n",
    "- Algoritmi iterativi (machine learning)\n",
    "\n",
    "**Quando non cachare**:\n",
    "- DataFrame usato una sola volta\n",
    "- Dati già ottimizzati in memoria (es. Parquet con columnar pruning)\n",
    "- Dataset troppo grande per la memoria disponibile (rischio spill to disk)\n",
    "\n",
    "## Spark UI\n",
    "\n",
    "Accessibile su **http://localhost:4040** durante una SparkSession attiva.\n",
    "\n",
    "**Tabs principali**:\n",
    "- **Jobs**: tutte le jobs eseguite, durata, stages, tasks, status success/failure\n",
    "- **Stages**: breakdown dettagliato di ogni job, metriche a livello task, shuffle read/write\n",
    "- **Storage**: RDD/DataFrame cached, utilizzo memoria, frazione cached vs spilled to disk\n",
    "- **Environment**: Spark properties, system properties, classpath\n",
    "- **Executors**: metriche per executor, memory usage, GC time, distribuzione task\n",
    "- **SQL**: execution plans (physical/logical), metriche per operatore, visualizzazione DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb049d7b",
   "metadata": {},
   "source": [
    "**Spark UI Jobs**\n",
    "\n",
    "<img src=\"./screenshots/03_spark_jobs.png\" alt=\"Spark UI Jobs\" >\n",
    "\n",
    "**Spark UI Executors**\n",
    "\n",
    "<img src=\"./screenshots/03_spark_executors.png\" alt=\"Spark UI Executors\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be1f2d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OPTIMIZATIONS\n",
      "======================================================================\n",
      "\n",
      "Running baseline query...\n",
      "\n",
      "Result:\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "|customer_segment|      category|num_transactions|       total_revenue|   avg_transaction|\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "|         Regular|   Electronics|         4547002| 9.862931500630018E9|2169.1064795287134|\n",
      "|      Occasional|   Electronics|         2595077| 4.460162046650001E9| 1718.701235705145|\n",
      "|             VIP|   Electronics|         1298917|3.7609259161299996E9|2895.4320531103986|\n",
      "|         Regular|        Beauty|        10136890| 2.694562224439984E9| 265.8174474064515|\n",
      "|         Regular|       Fashion|         5985500|2.4465271903900037E9|  408.742325685407|\n",
      "|         Regular|        Sports|         4307190|2.0952158867800007E9|486.44612538104906|\n",
      "|         Regular|        Health|         5477055|1.4617503961899996E9|266.88620000894633|\n",
      "|         Regular|Home & Kitchen|         6133733|1.3955288037499995E9|227.51704447356929|\n",
      "|      Occasional|        Beauty|         5795895| 1.219338762649999E9|210.37971920643815|\n",
      "|      Occasional|       Fashion|         3419255|1.1064785241299996E9| 323.6022244991964|\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Baseline completed:\n",
      "Time: 30.74s\n",
      "\n",
      "======================================================================\n",
      "PARTITIONING & CACHING\n",
      "======================================================================\n",
      "\n",
      "Current partitions:\n",
      "  Transactions: 35\n",
      "  Customers:    16\n",
      "  Products:     1\n",
      "\n",
      "Repartitioning transactions to 48 partitions & caching...\n",
      "  Repartitioned in     148.81s\n",
      "  New partitions:      48\n",
      "  Transactions cached: 100,000,000 row\n",
      "\n",
      "Caching Customers & Products tables...\n",
      "  Customers cached: 1,000,000 rows\n",
      "  Products cached: 50,000 rows\n",
      "\n",
      "======================================================================\n",
      "OPTIMIZED PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Running optimized query...\n",
      "\n",
      "Result:\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "|customer_segment|      category|num_transactions|       total_revenue|   avg_transaction|\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "|         Regular|   Electronics|         4547002| 9.862931500629986E9| 2169.106479528706|\n",
      "|      Occasional|   Electronics|         2595077| 4.460162046650009E9|1718.7012357051483|\n",
      "|             VIP|   Electronics|         1298917|     3.76092591613E9| 2895.432053110399|\n",
      "|         Regular|        Beauty|        10136890|2.6945622244399986E9| 265.8174474064529|\n",
      "|         Regular|       Fashion|         5985500|2.4465271903900027E9|408.74232568540685|\n",
      "|         Regular|        Sports|         4307190| 2.095215886779996E9|  486.446125381048|\n",
      "|         Regular|        Health|         5477055| 1.461750396190002E9| 266.8862000089468|\n",
      "|         Regular|Home & Kitchen|         6133733|1.3955288037500024E9|227.51704447356974|\n",
      "|      Occasional|        Beauty|         5795895|1.2193387626500003E9| 210.3797192064384|\n",
      "|      Occasional|       Fashion|         3419255|1.1064785241299994E9|323.60222449919627|\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Performance Improvement:\n",
      "  Baseline: 30.74s\n",
      "  Optimized: 25.15s\n",
      "  Speedup: 1.22x\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZATIONS\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"OPTIMIZATIONS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Execute without caching\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Baseline: Complex aggregation without optimization\n",
    "print(\"Running baseline query...\")\n",
    "start = time.time()\n",
    "result_baseline = transactions \\\n",
    "    .filter(col(\"status\") == \"completed\") \\\n",
    "    .join(customers, \"customer_id\") \\\n",
    "    .join(products, \"product_id\") \\\n",
    "    .groupBy(\"customer_segment\", \"category\") \\\n",
    "    .agg(\n",
    "        spark_count(\"*\").alias(\"num_transactions\"),\n",
    "        spark_sum(\"final_amount\").alias(\"total_revenue\"),\n",
    "        avg(\"final_amount\").alias(\"avg_transaction\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "print(\"\\nResult:\")\n",
    "result_baseline.show(10)\n",
    "baseline_time = time.time() - start\n",
    "\n",
    "print(f\"Baseline completed:\")\n",
    "print(f\"Time: {baseline_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PARTITIONING & CACHING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Check current partitions for each dataset\n",
    "print(\"Current partitions:\")\n",
    "print(f\"  Transactions: {transactions.rdd.getNumPartitions()}\")\n",
    "print(f\"  Customers:    {customers.rdd.getNumPartitions()}\")\n",
    "print(f\"  Products:     {products.rdd.getNumPartitions()}\")\n",
    "\n",
    "optimal_partitions = 48  # For local mode with 12 cores\n",
    "\n",
    "print(f\"\\nRepartitioning transactions to {optimal_partitions} partitions & caching...\")\n",
    "start = time.time()\n",
    "transactions_optimized = transactions.repartition(optimal_partitions)\n",
    "transactions_optimized.cache()\n",
    "transactions_count = transactions_optimized.count()  # Trigger caching\n",
    "repart_time = time.time() - start\n",
    "\n",
    "print(f\"  Repartitioned in     {repart_time:.2f}s\")\n",
    "print(f\"  New partitions:      {transactions_optimized.rdd.getNumPartitions()}\")\n",
    "print(f\"  Transactions cached: {transactions_count:,} row\")\n",
    "\n",
    "# Cache frequently used tables\n",
    "print(\"\\nCaching Customers & Products tables...\")\n",
    "\n",
    "# Cache customers (small, frequently joined)\n",
    "customers_optimized = customers.cache()\n",
    "customers_count = customers_optimized.count()  # Trigger caching\n",
    "print(f\"  Customers cached: {customers_count:,} rows\")\n",
    "\n",
    "# Cache products (small, frequently joined)\n",
    "products_optimized = products.cache()\n",
    "products_count = products_optimized.count()  # Trigger caching\n",
    "print(f\"  Products cached: {products_count:,} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMIZED PERFORMANCE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Same query as baseline\n",
    "print(\"Running optimized query...\")\n",
    "\n",
    "# Optimized: CACHED + REPARTITIONED + BROADCAST\n",
    "start = time.time()\n",
    "result_optimized = transactions_optimized \\\n",
    "    .filter(col(\"status\") == \"completed\") \\\n",
    "    .join(broadcast(customers_optimized), \"customer_id\") \\\n",
    "    .join(broadcast(products_optimized), \"product_id\") \\\n",
    "    .groupBy(\"customer_segment\", \"category\") \\\n",
    "    .agg(\n",
    "        spark_count(\"*\").alias(\"num_transactions\"),\n",
    "        spark_sum(\"final_amount\").alias(\"total_revenue\"),\n",
    "        avg(\"final_amount\").alias(\"avg_transaction\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "print(\"\\nResult:\")\n",
    "result_optimized.show(10)\n",
    "optimized_time = time.time() - start\n",
    "\n",
    "# Calculate improvement\n",
    "speedup = baseline_time / optimized_time if optimized_time > 0 else 1\n",
    "print(f\"\\nPerformance Improvement:\")\n",
    "print(f\"  Baseline: {baseline_time:.2f}s\")\n",
    "print(f\"  Optimized: {optimized_time:.2f}s\")\n",
    "print(f\"  Speedup: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30beb505",
   "metadata": {},
   "source": [
    "# ETL Pipeline & Delta Lake\n",
    "\n",
    "Una pipeline ETL completa prepara dati ML-ready sfruttando Delta Lake per garantire affidabilità e versioning.\n",
    "\n",
    "## Target Output per ML\n",
    "\n",
    "**ALS Data** (`user_item_interactions`) - per recommendation system:\n",
    "- Campi: `customer_id`, `product_id`, `rating`\n",
    "- Rating implicito derivato dal comportamento d'acquisto:\n",
    "\n",
    "    ```\n",
    "    rating = log(1 + purchase_count) * 2 + log(1 + total_spent) / 2\n",
    "    ```\n",
    "\n",
    "**Random Forest Data** (`customer_features`) - per classification:\n",
    "1. **Features Engineering**: \n",
    "    - **RFM**: Recency, Frequency, Monetary\n",
    "    - **Behavioral**: avg_transaction, unique_products\n",
    "    - **Temporal**: days_since_last_purchase\n",
    "    - **Demographic**: age, region\n",
    "\n",
    "2. **Labels**:\n",
    "    - `customer_segment` (multiclass)\n",
    "\n",
    "\n",
    "## Delta Lake: ACID su Data Lake\n",
    "\n",
    "Delta Lake è uno **storage layer costruito su Parquet** che aggiunge transazioni ACID, versioning e schema evolution ai data lake. Inoltre supporta sia batch che streaming in modo unificato.\n",
    "\n",
    "**Problemi dei Data Lake tradizionali**:\n",
    "- Assenza di transazioni ACID\n",
    "- Nessun versionamento dei dati\n",
    "- Inconsistenze nello schema\n",
    "- Impossibilità di rollback\n",
    "- Mancanza di audit trail\n",
    "\n",
    "**Soluzioni offerte da Delta Lake**:\n",
    "- **ACID transactions**: atomicità e consistenza garantite\n",
    "- **Time Travel**: accesso a versioni precedenti dei dati\n",
    "- **Schema enforcement & evolution**: validazione e evoluzione controllata dello schema\n",
    "- **Audit history**: transaction log completo di tutte le operazioni\n",
    "- **Unified batch & streaming**: stessa API per entrambe le modalità di elaborazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eadaafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ETL PIPELINE FOR ML\n",
      "======================================================================\n",
      "\n",
      "Pipeline Goal: Prepare data for ML models in Notebook 4\n",
      "\n",
      "Target Models:\n",
      "  1. ALS (Recommendation System)\n",
      "     - Need: user_id, item_id, rating (implicit)\n",
      "  2. Random Forest (Classification)\n",
      "     - Need: customer features + label\n",
      "\n",
      "ETL Stages:\n",
      "  EXTRACT: Load raw data from MinIO\n",
      "  TRANSFORM: Create ML-ready features\n",
      "  LOAD: Save to Delta Lake tables\n",
      "\n",
      "======================================================================\n",
      "EXTRACT\n",
      "======================================================================\n",
      "\n",
      "Loading data for ETL pipeline...\n",
      "  Transactions: 100,000,000 rows\n",
      "  Customers:    1,000,000 rows\n",
      "  Products:     50,000 rows\n",
      "\n",
      "Extract phase completed!\n",
      "\n",
      "======================================================================\n",
      "TRANSFORM - ALS DATA PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "Creating user-item interaction matrix...\n",
      "Schema after conversion:\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- purchase_count: long (nullable = false)\n",
      " |-- total_spent: double (nullable = true)\n",
      "\n",
      "Created 73,874,174 user-item interactions\n",
      "\n",
      "Sample interactions:\n",
      "+-------+-------+------+-----------+----------+--------------+------------------+\n",
      "|user_id|item_id|rating|customer_id|product_id|purchase_count|       total_spent|\n",
      "+-------+-------+------+-----------+----------+--------------+------------------+\n",
      "| 304524|      1| 12.63|  C00304524| P00000001|            49|14807.520000000002|\n",
      "| 713095|      1| 12.56|  C00713095| P00000001|            48|14162.890000000003|\n",
      "| 681654|      1| 12.54|  C00681654| P00000001|            48|          13455.59|\n",
      "| 139826|      1| 12.51|  C00139826| P00000001|            46|15117.880000000003|\n",
      "|   3361|      1| 12.48|  C00003361| P00000001|            46|14047.310000000001|\n",
      "| 766483|      1| 12.46|  C00766483| P00000001|            46|          13546.52|\n",
      "| 537479|      1| 12.42|  C00537479| P00000001|            45|13779.420000000004|\n",
      "| 154263|      1| 12.42|  C00154263| P00000001|            46|12566.039999999999|\n",
      "| 847118|      1| 12.41|  C00847118| P00000001|            45|          13327.88|\n",
      "| 843125|      1|  12.4|  C00843125| P00000001|            44|14234.349999999997|\n",
      "+-------+-------+------+-----------+----------+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TRANSFORM - RANDOM FOREST DATA PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "Creating customer features...\n",
      "\n",
      "Customer features prepared\n",
      "Schema: ['customer_id', 'recency', 'frequency', 'monetary', 'avg_transaction', 'unique_products', 'customer_segment', 'age', 'region']\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- recency: integer (nullable = true)\n",
      " |-- frequency: long (nullable = false)\n",
      " |-- monetary: double (nullable = true)\n",
      " |-- avg_transaction: double (nullable = true)\n",
      " |-- unique_products: long (nullable = false)\n",
      " |-- customer_segment: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample data:\n",
      "+-----------+-------+---------+------------------+------------------+---------------+----------------+---+-------------------+\n",
      "|customer_id|recency|frequency|monetary          |avg_transaction   |unique_products|customer_segment|age|region             |\n",
      "+-----------+-------+---------+------------------+------------------+---------------+----------------+---+-------------------+\n",
      "|C00000021  |52     |51       |16413.69          |321.8370588235294 |44             |Occasional      |53 |Trentino-Alto Adige|\n",
      "|C00000113  |63     |61       |17066.41          |279.7772131147541 |45             |Occasional      |29 |Basilicata         |\n",
      "|C00000162  |57     |109      |40201.259999999995|368.81889908256875|87             |Regular         |69 |Piemonte           |\n",
      "|C00000187  |41     |155      |73391.20999999998 |473.4916774193547 |128            |Regular         |70 |Trentino-Alto Adige|\n",
      "|C00000227  |39     |50       |14008.019999999999|280.1604          |42             |Occasional      |55 |Basilicata         |\n",
      "|C00000254  |87     |40       |20607.85000000001 |515.1962500000002 |37             |Occasional      |47 |Molise             |\n",
      "|C00000323  |37     |156      |70462.98          |451.6857692307692 |126            |Regular         |58 |Basilicata         |\n",
      "|C00000404  |37     |146      |68449.04          |468.8290410958904 |108            |Regular         |66 |Campania           |\n",
      "|C00000430  |37     |149      |61409.44999999999 |412.14395973154353|113            |Regular         |35 |Lazio              |\n",
      "|C00000472  |37     |130      |56555.08000000002 |435.0390769230771 |94             |Regular         |71 |Trentino-Alto Adige|\n",
      "+-----------+-------+---------+------------------+------------------+---------------+----------------+---+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Class distribution (customer_segment):\n",
      "+----------------+------+\n",
      "|customer_segment| count|\n",
      "+----------------+------+\n",
      "|      Occasional|600000|\n",
      "|         Regular|350000|\n",
      "|             VIP| 50000|\n",
      "+----------------+------+\n",
      "\n",
      "\n",
      "RFM Statistics:\n",
      "+-------+------------------+----------------+------------------+\n",
      "|summary|           recency|       frequency|          monetary|\n",
      "+-------+------------------+----------------+------------------+\n",
      "|  count|           1000000|         1000000|           1000000|\n",
      "|   mean|         47.379389|        95.00138| 41149.88022377997|\n",
      "| stddev|13.029851985487841|65.1370453346655| 38042.44653402155|\n",
      "|    min|                37|              20|3468.3900000000003|\n",
      "|    max|               237|             368|         279918.65|\n",
      "+-------+------------------+----------------+------------------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "LOAD\n",
      "======================================================================\n",
      "\n",
      "Delta Lake paths:\n",
      "  ALS data: s3a://bigdata-ecommerce/ml_data/user_item_interactions\n",
      "  RF data: s3a://bigdata-ecommerce/ml_data/customer_features\n",
      "\n",
      "1. Writing ALS user-item interactions to Delta...\n",
      "  Written in 82.11s\n",
      "\n",
      "2. Writing RF customer features to Delta...\n",
      "  Written in 73.20s\n",
      "\n",
      "Data loaded to Delta Lake successfully!\n"
     ]
    }
   ],
   "source": [
    "# ETL PIPELINE\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ETL PIPELINE FOR ML\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"Pipeline Goal: Prepare data for ML models in Notebook 4\")\n",
    "print(\"\\nTarget Models:\")\n",
    "print(\"  1. ALS (Recommendation System)\")\n",
    "print(\"     - Need: user_id, item_id, rating (implicit)\")\n",
    "print(\"  2. Random Forest (Classification)\")\n",
    "print(\"     - Need: customer features + label\")\n",
    "\n",
    "print(\"\\nETL Stages:\")\n",
    "print(\"  EXTRACT: Load raw data from MinIO\")\n",
    "print(\"  TRANSFORM: Create ML-ready features\")\n",
    "print(\"  LOAD: Save to Delta Lake tables\")\n",
    "\n",
    "# Extract Phase\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXTRACT\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"Loading data for ETL pipeline...\")\n",
    "\n",
    "print(f\"  Transactions: {transactions_count:,} rows\")\n",
    "print(f\"  Customers:    {customers_count:,} rows\")\n",
    "print(f\"  Products:     {products_count:,} rows\")\n",
    "\n",
    "print(\"\\nExtract phase completed!\")\n",
    "\n",
    "# Transform Phase - ALS Data Preprocessing\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRANSFORM - ALS DATA PREPROCESSING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Prepare user-item interactions for ALS\n",
    "# Use implicit feedback: number of purchases + total amount as rating\n",
    "print(\"Creating user-item interaction matrix...\")\n",
    "\n",
    "user_item_interactions = (\n",
    "    transactions_optimized\n",
    "    .filter(col(\"status\") == \"completed\")\n",
    "    .groupBy(\"customer_id\", \"product_id\")\n",
    "    .agg(\n",
    "        spark_count(\"*\").alias(\"purchase_count\"),\n",
    "        spark_sum(\"final_amount\").alias(\"total_spent\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"rating\",\n",
    "        # Implicit rating: log-scaled purchase count + amount component\n",
    "        spark_round(\n",
    "            expr(\"log(1 + purchase_count) * 2 + log(1 + total_spent) / 2\"),\n",
    "            2\n",
    "        )\n",
    "    )\n",
    "    # Convert string IDs to numeric IDs for ALS compatibility\n",
    "    .withColumn(\"user_id\", regexp_extract(col(\"customer_id\"), r\"(\\d+)\", 1).cast(\"int\"))\n",
    "    .withColumn(\"item_id\", regexp_extract(col(\"product_id\"), r\"(\\d+)\", 1).cast(\"int\"))\n",
    "    .select(\n",
    "        \"user_id\",           # numeric user ID\n",
    "        \"item_id\",           # numeric item ID\n",
    "        \"rating\",\n",
    "        \"customer_id\",       # original string ID\n",
    "        \"product_id\",        # original string ID\n",
    "        \"purchase_count\",\n",
    "        \"total_spent\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Verify conversion\n",
    "print(\"Schema after conversion:\")\n",
    "user_item_interactions.printSchema()\n",
    "\n",
    "# Count and sample\n",
    "als_count = user_item_interactions.count()\n",
    "print(f\"Created {als_count:,} user-item interactions\")\n",
    "\n",
    "print(\"\\nSample interactions:\")\n",
    "user_item_interactions.orderBy(desc(\"rating\")).show(10)\n",
    "\n",
    "\n",
    "# Transform Phase - Random Forest Data Prepocessing\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRANSFORM - RANDOM FOREST DATA PREPROCESSING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Create customer features for classification\n",
    "# Target: Predict customer segment or churn\n",
    "print(\"Creating customer features...\")\n",
    "\n",
    "# Aggregate customer behavior (RFM model)\n",
    "customer_features = (\n",
    "    transactions_optimized\n",
    "    .filter(col(\"status\") == \"completed\")\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        # RFM F: Frequency (total transactions)\n",
    "        spark_count(\"*\").alias(\"frequency\"),\n",
    "        \n",
    "        # RFM M: Monetary (total spent)\n",
    "        spark_sum(\"final_amount\").alias(\"monetary\"),\n",
    "        \n",
    "        # Average transaction value\n",
    "        avg(\"final_amount\").alias(\"avg_transaction\"),\n",
    "        \n",
    "        # RFM R: Recency (days since last transaction)\n",
    "        expr(\"datediff(current_date(), max(transaction_date))\").alias(\"recency\"),\n",
    "        \n",
    "        # Product diversity\n",
    "        expr(\"count(distinct product_id)\").alias(\"unique_products\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join with customer demographics\n",
    "customer_features = customer_features.join(\n",
    "    customers_optimized.select(\"customer_id\", \"customer_segment\", \"age\", \"region\"),\n",
    "    \"customer_id\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Select final columns with correct order and names\n",
    "customer_features = customer_features.select(\n",
    "    \"customer_id\",\n",
    "    \"recency\",           # RFM R\n",
    "    \"frequency\",         # RFM F  \n",
    "    \"monetary\",          # RFM M\n",
    "    \"avg_transaction\",   # Additional behavioral feature\n",
    "    \"unique_products\",   # Additional behavioral feature\n",
    "    \"customer_segment\",  # LABEL (multiclass)\n",
    "    \"age\",               # Demographics\n",
    "    \"region\",            # Demographics (categorical)\n",
    ")\n",
    "\n",
    "# Cast to ensure correct types\n",
    "customer_features = customer_features \\\n",
    "    .withColumn(\"recency\", col(\"recency\").cast(\"int\")) \\\n",
    "    .withColumn(\"frequency\", col(\"frequency\").cast(\"long\")) \\\n",
    "    .withColumn(\"monetary\", col(\"monetary\").cast(\"double\")) \\\n",
    "    .withColumn(\"avg_transaction\", col(\"avg_transaction\").cast(\"double\")) \\\n",
    "    .withColumn(\"unique_products\", col(\"unique_products\").cast(\"long\")) \\\n",
    "    .withColumn(\"age\", col(\"age\").cast(\"int\"))\n",
    "\n",
    "print(\"\\nCustomer features prepared\")\n",
    "print(f\"Schema: {customer_features.columns}\")\n",
    "\n",
    "# Show schema\n",
    "customer_features.printSchema()\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "customer_features.show(10, truncate=False)\n",
    "\n",
    "# Show class distribution\n",
    "print(\"\\nClass distribution (customer_segment):\")\n",
    "customer_features.groupBy(\"customer_segment\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nRFM Statistics:\")\n",
    "customer_features.select(\"recency\", \"frequency\", \"monetary\").describe().show()\n",
    "\n",
    "# Load Phase - Write to Delta Lake\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOAD\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Define Delta paths on MinIO\n",
    "als_delta_path = get_s3a_path(\"ml_data/\", \"user_item_interactions\")\n",
    "rf_delta_path = get_s3a_path(\"ml_data/\", \"customer_features\")\n",
    "\n",
    "print(\"Delta Lake paths:\")\n",
    "print(f\"  ALS data: {als_delta_path}\")\n",
    "print(f\"  RF data: {rf_delta_path}\")\n",
    "\n",
    "# Write ALS data\n",
    "print(\"\\n1. Writing ALS user-item interactions to Delta...\")\n",
    "start = time.time()\n",
    "user_item_interactions.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(als_delta_path)\n",
    "als_write_time = time.time() - start\n",
    "print(f\"  Written in {als_write_time:.2f}s\")\n",
    "\n",
    "# Write RF data\n",
    "print(\"\\n2. Writing RF customer features to Delta...\")\n",
    "start = time.time()\n",
    "customer_features.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(rf_delta_path)\n",
    "rf_write_time = time.time() - start\n",
    "print(f\"  Written in {rf_write_time:.2f}s\")\n",
    "\n",
    "print(\"\\nData loaded to Delta Lake successfully!\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492e75a",
   "metadata": {},
   "source": [
    "**MinIO Delta Lake Storage**\n",
    "\n",
    "<img src=\"./screenshots/03_minio_delta_als.png\" alt=\"MinIo Delta ALS\" >\n",
    "\n",
    "<img src=\"./screenshots/03_minio_delta_rf.png\" alt=\"MinIo Delta RF\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
