{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed1787c",
   "metadata": {},
   "source": [
    "# PySpark ETL\n",
    "\n",
    "L'obiettivo di questo Notebook è mostrare l'utilizzo di PySpark per l'analisi distribuita di Big Data, dall'introduzione all'architettura fino a pipeline ETL complete con Delta Lake.\n",
    "\n",
    "- **Dataset**: 100M transazioni e-commerce\n",
    "- **Storage**: MinIO (S3-compatible)\n",
    "- **Output**: Dati ML-ready in Delta Lake per Notebook 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16c978b",
   "metadata": {},
   "source": [
    "# Setup & Environment\n",
    "\n",
    "Configurazione ambiente Spark con Delta Lake support.\n",
    "\n",
    "1. Importare librerie necessarie\n",
    "2. Inizializzare SparkSession con configurazione MinIO\n",
    "3. Definire helper functions\n",
    "4. Verificare connessione Delta Lake\n",
    "\n",
    "### Configurazione Spark\n",
    "- **App Name**: Notebook-03-PySpark-Complete\n",
    "- **Delta Lake**: Abilitato (2.4.0)\n",
    "- **MinIO**: Configurato come S3-compatible storage\n",
    "- **Driver Memory**: 4GB\n",
    "- **Executor Memory**: 2GB\n",
    "- **Spark UI**: http://localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90871c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count as spark_count, sum as spark_sum, avg, max as spark_max, min as spark_min,\n",
    "    when, lit, round as spark_round, desc, asc, row_number, rank, dense_rank,\n",
    "    expr, broadcast, current_timestamp\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType\n",
    "from delta import DeltaTable\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import configurations\n",
    "from config.spark_config import get_spark_session\n",
    "from config.minio_config import get_s3a_path, BUCKET_NAME\n",
    "\n",
    "# Silence Warning Log\n",
    "import logging\n",
    "logging.getLogger(\"org\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340d8144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 JAR files\n",
      "Spark Session created: Notebook-03-PySpark\n",
      "Spark Version: 3.4.1\n",
      "Spark UI: http://localhost:4040\n",
      "Delta Lake + MinIO: Enabled\n",
      "  S3AFileSystem loaded successfully\n",
      "\n",
      "Key Spark Configurations:\n",
      "  spark.app.name: Notebook-03-PySpark\n",
      "  spark.driver.memory: 4g\n",
      "  spark.executor.memory: 2g\n",
      "  spark.sql.shuffle.partitions: 200\n",
      "  spark.sql.adaptive.enabled: true\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE SPARK SESSION + DELTA LAKE\n",
    "\n",
    "# Local IP configuration\n",
    "os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'\n",
    "\n",
    "spark = get_spark_session(\n",
    "    app_name=\"Notebook-03-PySpark\",\n",
    "    enable_delta=True\n",
    ")\n",
    "\n",
    "# Silence Warnings Log\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Display Spark configuration\n",
    "print(\"\\nKey Spark Configurations:\")\n",
    "conf = spark.sparkContext.getConf()\n",
    "important_configs = [\n",
    "    'spark.app.name',\n",
    "    'spark.driver.memory',\n",
    "    'spark.executor.memory',\n",
    "    'spark.sql.shuffle.partitions',\n",
    "    'spark.sql.adaptive.enabled'\n",
    "]\n",
    "for config in important_configs:\n",
    "    print(f\"  {config}: {conf.get(config, 'Not set')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69d59b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "def timer_decorator(func):\n",
    "    \"\"\"\n",
    "    Decorator to measure execution time of functions\n",
    "    \n",
    "    Args:\n",
    "        func: Function to time\n",
    "        \n",
    "    Returns:\n",
    "        Wrapped function with timing\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"\\n    {func.__name__} completed in {duration:.2f} seconds\")\n",
    "        return result, duration\n",
    "    return wrapper\n",
    "\n",
    "def count_with_time(df, description=\"Operation\"):\n",
    "    \"\"\"\n",
    "    Count DataFrame rows with timing\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        description: Operation description\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (count, duration)\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    count = df.count()\n",
    "    duration = time.time() - start\n",
    "    print(f\"    {description}: {count:,} rows in {duration:.2f}s\")\n",
    "    return count, duration\n",
    "\n",
    "def show_partitions(df, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Display partition information\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        name: DataFrame name for display\n",
    "    \"\"\"\n",
    "    num_partitions = df.rdd.getNumPartitions()\n",
    "    print(f\"{name} partitions: {num_partitions}\")\n",
    "    return num_partitions\n",
    "\n",
    "def save_results(data, filename):\n",
    "    \"\"\"\n",
    "    Save results to CSV in results directory\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary or DataFrame\n",
    "        filename: Output filename\n",
    "    \"\"\"\n",
    "    results_dir = project_root / \"results\"\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    filepath = results_dir / filename\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        pd.DataFrame([data]).to_csv(filepath, index=False)\n",
    "    else:\n",
    "        data.to_csv(filepath, index=False)\n",
    "    \n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c742168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data paths configured:\n",
      "  Transactions: s3a://bigdata-ecommerce/raw/transactions\n",
      "  Customers: s3a://bigdata-ecommerce/raw/customers.parquet\n",
      "  Products: s3a://bigdata-ecommerce/raw/products.parquet\n",
      "\n",
      "Testing Delta Lake on MinIO...\n",
      "  Delta write successful to: s3a://bigdata-ecommerce/delta-lake-test/test_table\n",
      "  Delta read successful: 2 rows\n",
      "\n",
      "  Delta Lake on MinIO is working!\n"
     ]
    }
   ],
   "source": [
    "# VERIFY PATHS & DELTA LAKE\n",
    "\n",
    "# Check MinIO paths\n",
    "transactions_path = get_s3a_path(\"raw/\", \"transactions\")\n",
    "customers_path = get_s3a_path(\"raw/\", \"customers.parquet\")\n",
    "products_path = get_s3a_path(\"raw/\", \"products.parquet\")\n",
    "\n",
    "print(\"Data paths configured:\")\n",
    "print(f\"  Transactions: {transactions_path}\")\n",
    "print(f\"  Customers: {customers_path}\")\n",
    "print(f\"  Products: {products_path}\")\n",
    "\n",
    "\n",
    "# Quick test to ensure Delta Lake works with MinIO\n",
    "print(\"\\nTesting Delta Lake on MinIO...\")\n",
    "\n",
    "test_delta_path = get_s3a_path(\"delta-lake-test/\", \"test_table\")\n",
    "\n",
    "try:\n",
    "    # Create simple test DataFrame\n",
    "    test_data = [(1, \"test\", 100), (2, \"test2\", 200)]\n",
    "    test_df = spark.createDataFrame(test_data, [\"id\", \"name\", \"value\"])\n",
    "    \n",
    "    # Write as Delta\n",
    "    test_df.write.format(\"delta\").mode(\"overwrite\").save(test_delta_path)\n",
    "    print(f\"  Delta write successful to: {test_delta_path}\")\n",
    "    \n",
    "    # Read back\n",
    "    read_df = spark.read.format(\"delta\").load(test_delta_path)\n",
    "    read_count = read_df.count()\n",
    "    print(f\"  Delta read successful: {read_count} rows\")\n",
    "    \n",
    "    print(\"\\n  Delta Lake on MinIO is working!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   Delta Lake test failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb819026",
   "metadata": {},
   "source": [
    "# Architettura PySpark\n",
    "\n",
    "PySpark utilizza la **lazy evaluation**: le trasformazioni non vengono eseguite immediatamente ma vengono registrate in un **DAG** (Directed Acyclic Graph). Il **Catalyst Optimizer** analizza il grafo per trovare il piano di esecuzione più efficiente, che viene avviato solo quando si invoca un'action.\n",
    "\n",
    "**Transformations**\n",
    "\n",
    "Creano nuovi DataFrame senza eseguire calcoli immediati. Si dividono in due categorie:\n",
    "\n",
    "1. **Narrow transformations**: operano su singole partizioni, non richiedono shuffle e network\n",
    "    - `select()`, `filter()`, `withColumn()`\n",
    "\n",
    "2. **Wide transformations**: riorganizzano dati tra partizioni, richiedono shuffle e network\n",
    "    - `groupBy()`, `join()`, `orderBy()`\n",
    "\n",
    "**Actions**\n",
    "\n",
    "Triggherano l'esecuzione del piano computazionale e ritornano risultati concreti all'utente.\n",
    "- `count()`, `collect()`, `show()`\n",
    "\n",
    "\n",
    "Due principi fondamentali caratterizzano PySpark:\n",
    "\n",
    "1. **Immutability**: ogni DataFrame è immutabile e ogni trasformazione genera un nuovo DataFrame\n",
    "\n",
    "2. **Distributed Computing**: partiziona i dati su più executors permettendo l'esecuzione parallela delle operazioni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "061095c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOAD DATA\n",
      "======================================================================\n",
      "\n",
      "Sample Customers: 1,000,000 rows\n",
      "Sample Products: 50,000 rows\n",
      "Sample Transactions: 100,000,000 rows\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOAD DATA\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Load data\n",
    "customers = spark.read.parquet(customers_path)\n",
    "products = spark.read.parquet(products_path)\n",
    "transactions = spark.read.parquet(transactions_path)\n",
    "\n",
    "print(f\"Sample Customers: {customers.count():,} rows\")\n",
    "print(f\"Sample Products: {products.count():,} rows\")\n",
    "print(f\"Sample Transactions: {transactions.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf92523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LAZY EVALUETION DEMO\n",
      "======================================================================\n",
      "\n",
      "1. Creating transformation (lazy)...\n",
      "   Transformation defined (no execution yet)\n",
      "\n",
      "2. Triggering action (execution happens now)...\n",
      "   Executed! Found 8,236,456 high-value transactions\n",
      "   Execution time: 3.49s\n"
     ]
    }
   ],
   "source": [
    "# LAZY EVALUATION DEMO\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LAZY EVALUATION DEMO\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Create transformation (no execution yet)\n",
    "print(\"1. Creating transformation (lazy)...\")\n",
    "filtered_transactions = transactions.filter(col(\"final_amount\") > 1000)\n",
    "print(\"   Transformation defined (no execution yet)\")\n",
    "\n",
    "# Trigger action (execution happens now)\n",
    "print(\"\\n2. Triggering action (execution happens now)...\")\n",
    "start = time.time()\n",
    "result_count = filtered_transactions.count()\n",
    "duration = time.time() - start\n",
    "\n",
    "print(f\"   Executed! Found {result_count:,} high-value transactions\")\n",
    "print(f\"   Execution time: {duration:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4844dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRANSFORMATION EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "1. SELECT transformation:\n",
      "   Selected 3 columns from customers\n",
      "\n",
      "2. FILTER transformation:\n",
      "   Filtered VIP customers (lazy)\n",
      "\n",
      "3. WITHCOLUMN transformation:\n",
      "   Added margin_pct column (lazy)\n",
      "\n",
      "4. GROUPBY transformation (with agg):\n",
      "   Grouped by segment (lazy)\n",
      "\n",
      "======================================================================\n",
      "ACTIONS EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "1. COUNT action:\n",
      "   VIP customers: 50,000 (executed in 0.18s)\n",
      "\n",
      "2. SHOW action:\n",
      "   First 5 VIP customers:\n",
      "+-----------+--------------------+---+\n",
      "|customer_id|                name|age|\n",
      "+-----------+--------------------+---+\n",
      "|  C00000017|       Aria Giannini| 26|\n",
      "|  C00000032|Gelsomina Renzi-P...| 26|\n",
      "|  C00000042|  Sig.ra Michela Emo| 63|\n",
      "|  C00000062|  Gustavo Spanevello| 31|\n",
      "|  C00000066|Sig.ra Lara Randazzo| 67|\n",
      "+-----------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "3. COLLECT action:\n",
      "   Collected 3 rows to driver\n",
      "   - C00000017: Aria Giannini\n",
      "   - C00000032: Gelsomina Renzi-Piane\n",
      "   - C00000042: Sig.ra Michela Emo\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMATION & ACTIONS EXAMPLES\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRANSFORMATION EXAMPLES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1. Select\n",
    "print(\"1. SELECT transformation:\")\n",
    "customer_basics = customers.select(\"customer_id\", \"name\", \"customer_segment\")\n",
    "print(f\"   Selected 3 columns from customers\")\n",
    "\n",
    "# 2. Filter\n",
    "print(\"\\n2. FILTER transformation:\")\n",
    "vip_customers = customers.filter(col(\"customer_segment\") == \"VIP\")\n",
    "print(f\"   Filtered VIP customers (lazy)\")\n",
    "\n",
    "# 3. WithColumn\n",
    "print(\"\\n3. WITHCOLUMN transformation:\")\n",
    "transactions_with_margin = transactions.withColumn(\n",
    "    \"margin_pct\",\n",
    "    spark_round((col(\"final_amount\") / col(\"total_amount\")) * 100, 2)\n",
    ")\n",
    "print(f\"   Added margin_pct column (lazy)\")\n",
    "\n",
    "# 4. GroupBy (transformation, not action!)\n",
    "print(\"\\n4. GROUPBY transformation (with agg):\")\n",
    "segment_stats = customers.groupBy(\"customer_segment\").agg(\n",
    "    spark_count(\"*\").alias(\"count\"),\n",
    "    avg(\"age\").alias(\"avg_age\")\n",
    ")\n",
    "print(f\"   Grouped by segment (lazy)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACTIONS EXAMPLES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1. Count\n",
    "print(\"1. COUNT action:\")\n",
    "start = time.time()\n",
    "vip_count = vip_customers.count()\n",
    "duration = time.time() - start\n",
    "print(f\"   VIP customers: {vip_count:,} (executed in {duration:.2f}s)\")\n",
    "\n",
    "# 2. Show\n",
    "print(\"\\n2. SHOW action:\")\n",
    "print(\"   First 5 VIP customers:\")\n",
    "vip_customers.select(\"customer_id\", \"name\", \"age\").show(5)\n",
    "\n",
    "# 3. Collect\n",
    "print(\"\\n3. COLLECT action:\")\n",
    "top_3_vip = vip_customers.select(\"customer_id\", \"name\").limit(3).collect()\n",
    "print(f\"   Collected {len(top_3_vip)} rows to driver\")\n",
    "for row in top_3_vip:\n",
    "    print(f\"   - {row['customer_id']}: {row['name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fafab187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATAFRAME OPERATIONS\n",
      "======================================================================\n",
      "\n",
      "1. DataFrame Info:\n",
      "   Columns: 13\n",
      "   Column names: ['customer_id', 'name', 'email', 'phone', 'address', 'city', 'region', 'country', 'postal_code', 'registration_date', 'customer_segment', 'age', 'gender']\n",
      "   Partitions: 12\n",
      "\n",
      "2. Schema:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- registration_date: date (nullable = true)\n",
      " |-- customer_segment: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "\n",
      "3. Describe (summary statistics):\n",
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|           1000000|\n",
      "|   mean|         46.506383|\n",
      "| stddev|16.733604371767242|\n",
      "|    min|                18|\n",
      "|    max|                75|\n",
      "+-------+------------------+\n",
      "\n",
      "\n",
      "4. Distinct values:\n",
      "   Customer segments:\n",
      "+----------------+\n",
      "|customer_segment|\n",
      "+----------------+\n",
      "|         Regular|\n",
      "|      Occasional|\n",
      "|             VIP|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DATAFRAME OPERATIONS\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATAFRAME OPERATIONS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Basic info\n",
    "print(\"1. DataFrame Info:\")\n",
    "print(f\"   Columns: {len(customers.columns)}\")\n",
    "print(f\"   Column names: {customers.columns}\")\n",
    "print(f\"   Partitions: {customers.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Schema\n",
    "print(\"\\n2. Schema:\")\n",
    "customers.printSchema()\n",
    "\n",
    "# Describe\n",
    "print(\"\\n3. Describe (summary statistics):\")\n",
    "customers.select(\"age\").describe().show()\n",
    "\n",
    "# Distinct\n",
    "print(\"\\n4. Distinct values:\")\n",
    "segments = customers.select(\"customer_segment\").distinct()\n",
    "print(f\"   Customer segments:\")\n",
    "segments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65782eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUERY EXECUTION PLAN\n",
      "======================================================================\n",
      "\n",
      "Chained operations:\n",
      "  1. Filter completed transactions\n",
      "  2. Filter amount > 50\n",
      "  3. Select specific columns\n",
      "  4. Add high_value column\n",
      "  5. Order by amount\n",
      "  6. Limit to top 10\n",
      "\n",
      "Result:\n",
      "+--------------+-----------+------------+----------------+----------+\n",
      "|transaction_id|customer_id|final_amount|transaction_date|high_value|\n",
      "+--------------+-----------+------------+----------------+----------+\n",
      "|   T0017991493|  C00429214|     8993.15|      2024-10-26|       Yes|\n",
      "|   T0025390014|  C00225504|     8971.79|      2022-01-27|       Yes|\n",
      "|   T0013820685|  C00150095|     8970.68|      2022-03-02|       Yes|\n",
      "|   T0098879494|  C00375959|     8970.06|      2023-12-07|       Yes|\n",
      "|   T0004061594|  C00961063|     8969.79|      2023-05-05|       Yes|\n",
      "|   T0053544641|  C00039748|     8965.36|      2022-08-01|       Yes|\n",
      "|   T0005112796|  C00687537|     8963.48|      2025-01-08|       Yes|\n",
      "|   T0028746018|  C00180126|     8959.42|      2022-02-27|       Yes|\n",
      "|   T0040512331|  C00461006|     8958.44|      2024-12-27|       Yes|\n",
      "|   T0077952413|  C00829607|     8956.87|      2022-09-06|       Yes|\n",
      "+--------------+-----------+------------+----------------+----------+\n",
      "\n",
      "\n",
      "Spark optimizes this chain before execution (Catalyst optimizer)\n",
      "\n",
      "PHYSICAL PLAN (what to do):\n",
      "----------------------------------------------------------------------\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=10, orderBy=[final_amount#2743 DESC NULLS LAST], output=[transaction_id#2735,customer_id#2736,final_amount#2743,transaction_date#2745,high_value#3117])\n",
      "+- *(1) Project [transaction_id#2735, customer_id#2736, final_amount#2743, transaction_date#2745, CASE WHEN (final_amount#2743 > 1000.0) THEN Yes ELSE No END AS high_value#3117]\n",
      "   +- *(1) Filter (((isnotnull(status#2748) AND isnotnull(final_amount#2743)) AND (status#2748 = completed)) AND (final_amount#2743 > 500.0))\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [transaction_id#2735,customer_id#2736,final_amount#2743,transaction_date#2745,status#2748,year#2749,month#2750] Batched: true, DataFilters: [isnotnull(status#2748), isnotnull(final_amount#2743), (status#2748 = completed), (final_amount#2..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://bigdata-ecommerce/raw/transactions], PartitionFilters: [], PushedFilters: [IsNotNull(status), IsNotNull(final_amount), EqualTo(status,completed), GreaterThan(final_amount,..., ReadSchema: struct<transaction_id:string,customer_id:string,final_amount:double,transaction_date:date,status:...\n",
      "\n",
      "\n",
      "\n",
      "LOGICAL PLAN (how to do it):\n",
      "----------------------------------------------------------------------\n",
      "== Parsed Logical Plan ==\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [final_amount#2743 DESC NULLS LAST], true\n",
      "      +- Project [transaction_id#2735, customer_id#2736, final_amount#2743, transaction_date#2745, CASE WHEN (final_amount#2743 > cast(1000 as double)) THEN Yes ELSE No END AS high_value#3117]\n",
      "         +- Project [transaction_id#2735, customer_id#2736, final_amount#2743, transaction_date#2745]\n",
      "            +- Filter (final_amount#2743 > cast(500 as double))\n",
      "               +- Filter (status#2748 = completed)\n",
      "                  +- Relation [transaction_id#2735,customer_id#2736,product_id#2737,quantity#2738L,unit_price#2739,total_amount#2740,discount_pct#2741,discount_amount#2742,final_amount#2743,shipping_cost#2744,transaction_date#2745,transaction_timestamp#2746,payment_method#2747,status#2748,year#2749,month#2750] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "transaction_id: string, customer_id: string, final_amount: double, transaction_date: date, high_value: string\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [final_amount#2743 DESC NULLS LAST], true\n",
      "      +- Project [transaction_id#2735, customer_id#2736, final_amount#2743, transaction_date#2745, CASE WHEN (final_amount#2743 > cast(1000 as double)) THEN Yes ELSE No END AS high_value#3117]\n",
      "         +- Project [transaction_id#2735, customer_id#2736, final_amount#2743, transaction_date#2745]\n",
      "            +- Filter (final_amount#2743 > cast(500 as double))\n",
      "               +- Filter (status#2748 = completed)\n",
      "                  +- Relation [transaction_id#2735,customer_id#2736,product_id#2737,quantity#2738L,unit_price#2739,total_amount#2740,discount_pct#2741,discount_amount#2742,final_amount#2743,shipping_cost#2744,transaction_date#2745,transaction_timestamp#2746,payment_method#2747,status#2748,year#2749,month#2750] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [final_amount#2743 DESC NULLS LAST], true\n",
      "      +- Project [transaction_id#2735, customer_id#2736, final_amount#2743, transaction_date#2745, CASE WHEN (final_amount#2743 > 1000.0) THEN Yes ELSE No END AS high_value#3117]\n",
      "         +- Filter ((isnotnull(status#2748) AND isnotnull(final_amount#2743)) AND ((status#2748 = completed) AND (final_amount#2743 > 500.0)))\n",
      "            +- Relation [transaction_id#2735,customer_id#2736,product_id#2737,quantity#2738L,unit_price#2739,total_amount#2740,discount_pct#2741,discount_amount#2742,final_amount#2743,shipping_cost#2744,transaction_date#2745,transaction_timestamp#2746,payment_method#2747,status#2748,year#2749,month#2750] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=10, orderBy=[final_amount#2743 DESC NULLS LAST], output=[transaction_id#2735,customer_id#2736,final_amount#2743,transaction_date#2745,high_value#3117])\n",
      "+- *(1) Project [transaction_id#2735, customer_id#2736, final_amount#2743, transaction_date#2745, CASE WHEN (final_amount#2743 > 1000.0) THEN Yes ELSE No END AS high_value#3117]\n",
      "   +- *(1) Filter (((isnotnull(status#2748) AND isnotnull(final_amount#2743)) AND (status#2748 = completed)) AND (final_amount#2743 > 500.0))\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [transaction_id#2735,customer_id#2736,final_amount#2743,transaction_date#2745,status#2748,year#2749,month#2750] Batched: true, DataFilters: [isnotnull(status#2748), isnotnull(final_amount#2743), (status#2748 = completed), (final_amount#2..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://bigdata-ecommerce/raw/transactions], PartitionFilters: [], PushedFilters: [IsNotNull(status), IsNotNull(final_amount), EqualTo(status,completed), GreaterThan(final_amount,..., ReadSchema: struct<transaction_id:string,customer_id:string,final_amount:double,transaction_date:date,status:...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# QUERY EXECUTION PLAN\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"QUERY EXECUTION PLAN\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Complex chain of transformations\n",
    "result = (\n",
    "    transactions\n",
    "    .filter(col(\"status\") == \"completed\")\n",
    "    .filter(col(\"final_amount\") > 500)\n",
    "    .select(\n",
    "        \"transaction_id\",\n",
    "        \"customer_id\",\n",
    "        \"final_amount\",\n",
    "        \"transaction_date\"\n",
    "    )\n",
    "    .withColumn(\"high_value\", when(col(\"final_amount\") > 1000, \"Yes\").otherwise(\"No\"))\n",
    "    .orderBy(desc(\"final_amount\"))\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "print(\"Chained operations:\")\n",
    "print(\"  1. Filter completed transactions\")\n",
    "print(\"  2. Filter amount > 50\")\n",
    "print(\"  3. Select specific columns\")\n",
    "print(\"  4. Add high_value column\")\n",
    "print(\"  5. Order by amount\")\n",
    "print(\"  6. Limit to top 10\")\n",
    "\n",
    "print(\"\\nResult:\")\n",
    "result.show()\n",
    "\n",
    "print(\"\\nSpark optimizes this chain before execution (Catalyst optimizer)\")\n",
    "\n",
    "# Show phisical plan\n",
    "print(\"\\nPHYSICAL PLAN (what to do):\")\n",
    "print(\"-\" * 70)\n",
    "result.explain(extended=False)\n",
    "\n",
    "# Show logical plan\n",
    "print(\"\\nLOGICAL PLAN (how to do it):\")\n",
    "print(\"-\" * 70)\n",
    "result.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747b102",
   "metadata": {},
   "source": [
    "# Spark SQL & Joins\n",
    "\n",
    "Spark SQL permette di eseguire query SQL distribuite su DataFrame con ottimizzazione automatica tramite Catalyst.\n",
    "\n",
    "**Temporary Views** - per registrare DataFrame come tabelle SQL:\n",
    "- **Temp View**: disponibile solo nella SparkSession corrente, si cancella al termine della sessione\n",
    "- **Global Temp View**: accessibile a tutte le sessioni Spark nell'applicazione, utile per condividere dati tra notebook\n",
    "\n",
    "**Operazioni JOIN**:\n",
    "- Supporta tutte le tipologie standard: `inner`, `left`, `right`, `outer`\n",
    "- Join standard richiedono shuffle (riorganizzazione dati tra partizioni) ed è dunque lenta\n",
    "\n",
    "**Broadcast Join Optimization**:\n",
    "Quando una tabella è piccola (< 10MB default), Spark invia una copia completa della tabella a tutti gli executors. Il join avviene localmente su ogni executor senza shuffle della tabella grande, risultando molto più veloce. Evitare di utilizzarlo con tabelle >100MB (rischio OOM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16dabcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "REGISTERING TEMPORARY VIEWS\n",
      "======================================================================\n",
      "Registered views:\n",
      "   - transactions\n",
      "   - customers\n",
      "   - products\n",
      "\n",
      "Available tables:\n",
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|         |   customers|      false|\n",
      "|         |    products|      false|\n",
      "|         |transactions|      false|\n",
      "+---------+------------+-----------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SIMPLE SQL QUERIES\n",
      "======================================================================\n",
      "1. Top 10 high-value transactions:\n",
      "+--------------+-----------+------------+----------------+\n",
      "|transaction_id|customer_id|final_amount|transaction_date|\n",
      "+--------------+-----------+------------+----------------+\n",
      "|   T0017991493|  C00429214|     8993.15|      2024-10-26|\n",
      "|   T0025390014|  C00225504|     8971.79|      2022-01-27|\n",
      "|   T0013820685|  C00150095|     8970.68|      2022-03-02|\n",
      "|   T0098879494|  C00375959|     8970.06|      2023-12-07|\n",
      "|   T0004061594|  C00961063|     8969.79|      2023-05-05|\n",
      "|   T0053544641|  C00039748|     8965.36|      2022-08-01|\n",
      "|   T0005112796|  C00687537|     8963.48|      2025-01-08|\n",
      "|   T0028746018|  C00180126|     8959.42|      2022-02-27|\n",
      "|   T0040512331|  C00461006|     8958.44|      2024-12-27|\n",
      "|   T0077952413|  C00829607|     8956.87|      2022-09-06|\n",
      "+--------------+-----------+------------+----------------+\n",
      "\n",
      "\n",
      "2. Transactions by status:\n",
      "+---------+--------+--------------------+-----------------+\n",
      "|   status|   count|       total_revenue|       avg_amount|\n",
      "+---------+--------+--------------------+-----------------+\n",
      "|completed|95000239|4.115641359173029E10|433.2243163275652|\n",
      "| refunded| 2000800|                 0.0|              0.0|\n",
      "|cancelled| 2998961|                 0.0|              0.0|\n",
      "+---------+--------+--------------------+-----------------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "GROUPBY QUERIES\n",
      "======================================================================\n",
      "1. Analysis by customer segment:\n",
      "+----------------+-------------+-----------------+-----------+\n",
      "|customer_segment|num_customers|          avg_age|num_regions|\n",
      "+----------------+-------------+-----------------+-----------+\n",
      "|      Occasional|       600000|         46.50292|         20|\n",
      "|         Regular|       350000|46.51125428571429|         20|\n",
      "|             VIP|        50000|         46.51384|         20|\n",
      "+----------------+-------------+-----------------+-----------+\n",
      "\n",
      "\n",
      "2. Sales by product category:\n",
      "+--------------+----------------+--------------------+---------------------+\n",
      "|      category|num_transactions|       total_revenue|avg_transaction_value|\n",
      "+--------------+----------------+--------------------+---------------------+\n",
      "|   Electronics|         8445444|1.809022743041000...|   2142.0102282852154|\n",
      "|        Beauty|        18821578| 4.939064441909997E9|   262.41500271178097|\n",
      "|       Fashion|        11115049| 4.487250488629994E9|    403.7094653051007|\n",
      "|        Sports|         7999986| 3.842552285380004E9|    480.3198762322839|\n",
      "|        Health|        10170897|2.6801031439899993E9|    263.5070578327555|\n",
      "|Home & Kitchen|        11377812| 2.554228032620002E9|   224.49202294957956|\n",
      "|          Toys|         7014877|1.9422837553799899E9|    276.8806574056808|\n",
      "|    Automotive|         6107682|1.6088681359299996E9|   263.41714187641065|\n",
      "|          Food|         7475713| 6.598855614100006E8|    88.27058521508258|\n",
      "|         Books|         6471201| 3.519503160699996E8|   54.387171109350426|\n",
      "+--------------+----------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL QUERIES\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"REGISTERING TEMPORARY VIEWS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Register all tables\n",
    "transactions.createOrReplaceTempView(\"transactions\")\n",
    "customers.createOrReplaceTempView(\"customers\")\n",
    "products.createOrReplaceTempView(\"products\")\n",
    "\n",
    "print(\"Registered views:\")\n",
    "print(\"   - transactions\")\n",
    "print(\"   - customers\")\n",
    "print(\"   - products\")\n",
    "\n",
    "# List all temporary views\n",
    "print(\"\\nAvailable tables:\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SIMPLE SQL QUERIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Query 1: SELECT with WHERE\n",
    "print(\"1. Top 10 high-value transactions:\")\n",
    "query1 = \"\"\"\n",
    "SELECT \n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    final_amount,\n",
    "    transaction_date\n",
    "FROM transactions\n",
    "WHERE status = 'completed' AND final_amount > 1000\n",
    "ORDER BY final_amount DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "result1 = spark.sql(query1)\n",
    "result1.show()\n",
    "\n",
    "# Query 2: Aggregation\n",
    "print(\"\\n2. Transactions by status:\")\n",
    "query2 = \"\"\"\n",
    "SELECT \n",
    "    status,\n",
    "    COUNT(*) as count,\n",
    "    SUM(final_amount) as total_revenue,\n",
    "    AVG(final_amount) as avg_amount\n",
    "FROM transactions\n",
    "GROUP BY status\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "result2 = spark.sql(query2)\n",
    "result2.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GROUPBY QUERIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Customer segments analysis\n",
    "print(\"1. Analysis by customer segment:\")\n",
    "query3 = \"\"\"\n",
    "SELECT \n",
    "    customer_segment,\n",
    "    COUNT(*) as num_customers,\n",
    "    AVG(age) as avg_age,\n",
    "    COUNT(DISTINCT region) as num_regions\n",
    "FROM customers\n",
    "GROUP BY customer_segment\n",
    "ORDER BY num_customers DESC\n",
    "\"\"\"\n",
    "result3 = spark.sql(query3)\n",
    "result3.show()\n",
    "\n",
    "# Product categories\n",
    "print(\"\\n2. Sales by product category:\")\n",
    "query4 = \"\"\"\n",
    "SELECT \n",
    "    p.category,\n",
    "    COUNT(DISTINCT t.transaction_id) as num_transactions,\n",
    "    SUM(t.final_amount) as total_revenue,\n",
    "    AVG(t.final_amount) as avg_transaction_value\n",
    "FROM transactions t\n",
    "JOIN products p ON t.product_id = p.product_id\n",
    "WHERE t.status = 'completed'\n",
    "GROUP BY p.category\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "result4 = spark.sql(query4)\n",
    "result4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f379d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INNER JOIN\n",
      "======================================================================\n",
      "Transactions with customer information:\n",
      "+--------------+------------+--------------+----------------+------+\n",
      "|transaction_id|final_amount| customer_name|customer_segment|region|\n",
      "+--------------+------------+--------------+----------------+------+\n",
      "|   T0059903320|      162.34|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0033333619|      255.45|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0082668660|      271.21|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0079579088|      147.42|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0035322381|       180.7|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0066798242|      508.06|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0070841305|      349.94|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0016617238|      224.44|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0003034843|      161.76|Amadeo Cabrini|      Occasional|Molise|\n",
      "|   T0093220517|     1934.48|Amadeo Cabrini|      Occasional|Molise|\n",
      "+--------------+------------+--------------+----------------+------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "LEFT JOIN\n",
      "======================================================================\n",
      "Top 20 customers by total spent:\n",
      "+-----------+--------------------+----------------+----------------+------------------+\n",
      "|customer_id|                name|customer_segment|num_transactions|       total_spent|\n",
      "+-----------+--------------------+----------------+----------------+------------------+\n",
      "|  C00200763|Annamaria Fittipaldi|             VIP|             315|264387.58999999985|\n",
      "|  C00060848|      Liberto Turati|             VIP|             308|258969.30000000008|\n",
      "|  C00911113|     Fedele Giannini|             VIP|             330| 256586.0700000001|\n",
      "|  C00908037|     Amalia Vecellio|             VIP|             297|255335.33000000005|\n",
      "|  C00206223|Sig.ra Marissa Vi...|             VIP|             316| 254792.0899999999|\n",
      "|  C00776848|   Rosaria Biagiotti|             VIP|             357|250920.06999999998|\n",
      "|  C00297972| Antonino Rusticucci|             VIP|             335|250174.99000000014|\n",
      "|  C00896604|  Dott. Luisa Oscuro|             VIP|             315| 247821.1199999999|\n",
      "|  C00237958|    Ermenegildo Donà|             VIP|             342|247335.55000000008|\n",
      "|  C00204792|    Priscilla Paruta|             VIP|             304|246492.75999999986|\n",
      "|  C00364561| Elena Finzi-Marrone|             VIP|             334|246443.44000000006|\n",
      "|  C00290910|    Paulina Pasolini|             VIP|             343|         246257.36|\n",
      "|  C00834789|Dante Segrè-Cesar...|             VIP|             326|          245935.3|\n",
      "|  C00155762| Graziano Stefanelli|             VIP|             299| 245338.2399999999|\n",
      "|  C00384388|  Federica Comeriato|             VIP|             319|245231.46000000017|\n",
      "|  C00391865|Claudia Condoleo-...|             VIP|             283|245203.53999999995|\n",
      "|  C00908421| Bernardo Modigliani|             VIP|             324|243730.17000000007|\n",
      "|  C00967929|Dott. Flavia Bonatti|             VIP|             334| 243199.9999999999|\n",
      "|  C00094838|        Amedeo Calvo|             VIP|             321|242956.81999999995|\n",
      "|  C00903743|    Renzo Spanevello|             VIP|             311|         242848.44|\n",
      "+-----------+--------------------+----------------+----------------+------------------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "BROADCAST JOIN OPTIMIZATION\n",
      "======================================================================\n",
      "Products table size: 50,000 rows\n",
      "Transactions table size: 100,000,000 rows\n",
      "\n",
      "1. Standard Join:\n",
      "+--------------+------------+--------------------+--------------+\n",
      "|transaction_id|final_amount|        product_name|      category|\n",
      "+--------------+------------+--------------------+--------------+\n",
      "|   T0060000055|     1183.32|Dell Accessories ...|   Electronics|\n",
      "|   T0060000068|      106.05|L'Oréal Fragrance...|        Beauty|\n",
      "|   T0060000140|        47.1|Estée Lauder Hair...|        Beauty|\n",
      "|   T0060000196|      1758.5|   Dell Tablets Quia|   Electronics|\n",
      "|   T0060000268|       22.33|Mondadori Fiction...|         Books|\n",
      "|   T0060000273|      128.53|Optimum Wellness ...|        Health|\n",
      "|   T0060000292|         0.0|Philips Kitchenwa...|Home & Kitchen|\n",
      "|   T0060000303|      883.37|Clinique Haircare...|        Beauty|\n",
      "|   T0060000341|      497.04|   H&M Jewelry Eaque|       Fashion|\n",
      "|   T0060000386|     1408.22|  Dell Tablets Vitae|   Electronics|\n",
      "+--------------+------------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "2. Broadcast Join:\n",
      "+--------------+------------+--------------------+--------------+\n",
      "|transaction_id|final_amount|        product_name|      category|\n",
      "+--------------+------------+--------------------+--------------+\n",
      "|   T0060000055|     1183.32|Dell Accessories ...|   Electronics|\n",
      "|   T0060000068|      106.05|L'Oréal Fragrance...|        Beauty|\n",
      "|   T0060000140|        47.1|Estée Lauder Hair...|        Beauty|\n",
      "|   T0060000196|      1758.5|   Dell Tablets Quia|   Electronics|\n",
      "|   T0060000268|       22.33|Mondadori Fiction...|         Books|\n",
      "|   T0060000273|      128.53|Optimum Wellness ...|        Health|\n",
      "|   T0060000292|         0.0|Philips Kitchenwa...|Home & Kitchen|\n",
      "|   T0060000303|      883.37|Clinique Haircare...|        Beauty|\n",
      "|   T0060000341|      497.04|   H&M Jewelry Eaque|       Fashion|\n",
      "|   T0060000386|     1408.22|  Dell Tablets Vitae|   Electronics|\n",
      "+--------------+------------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Broadcast join speedup: 1.80x\n",
      "   Standard join: 0.29s\n",
      "   Broadcast join: 0.16s\n",
      "Saved: 03_join_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# JOIN OPERATIONS\n",
    "\n",
    "# Inner Join\n",
    "print(\"=\"*70)\n",
    "print(\"INNER JOIN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Inner join: transactions with customer info\n",
    "query5 = \"\"\"\n",
    "SELECT \n",
    "    t.transaction_id,\n",
    "    t.final_amount,\n",
    "    c.name as customer_name,\n",
    "    c.customer_segment,\n",
    "    c.region\n",
    "FROM transactions t\n",
    "INNER JOIN customers c ON t.customer_id = c.customer_id\n",
    "WHERE t.final_amount > 100\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "result5 = spark.sql(query5)\n",
    "print(\"Transactions with customer information:\")\n",
    "result5.show()\n",
    "\n",
    "# Left Join\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LEFT JOIN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# All customers with their transaction count (including those with 0 transactions)\n",
    "query6 = \"\"\"\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.name,\n",
    "    c.customer_segment,\n",
    "    COUNT(t.transaction_id) as num_transactions,\n",
    "    COALESCE(SUM(t.final_amount), 0) as total_spent\n",
    "FROM customers c\n",
    "LEFT JOIN transactions t ON c.customer_id = t.customer_id \n",
    "    AND t.status = 'completed'\n",
    "GROUP BY c.customer_id, c.name, c.customer_segment\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "result6 = spark.sql(query6)\n",
    "print(\"Top 20 customers by total spent:\")\n",
    "result6.show()\n",
    "\n",
    "\n",
    "# Broadcast Join Optimization\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BROADCAST JOIN OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Products table is small - good candidate for broadcast\n",
    "print(f\"Products table size: {products.count():,} rows\")\n",
    "print(f\"Transactions table size: {transactions.count():,} rows\")\n",
    "\n",
    "# Standard join (without broadcast)\n",
    "print(\"\\n1. Standard Join:\")\n",
    "standard_join = transactions.join(\n",
    "    products,\n",
    "    transactions.product_id == products.product_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    transactions.transaction_id,\n",
    "    transactions.final_amount,\n",
    "    products.product_name,\n",
    "    products.category\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "standard_join.show(10)\n",
    "standard_time = time.time() - start\n",
    "\n",
    "# Broadcast join\n",
    "print(\"\\n2. Broadcast Join:\")\n",
    "broadcast_join = transactions.join(\n",
    "    broadcast(products),\n",
    "    transactions.product_id == products.product_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    transactions.transaction_id,\n",
    "    transactions.final_amount,\n",
    "    products.product_name,\n",
    "    products.category\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "broadcast_join.show(10)\n",
    "broadcast_time = time.time() - start\n",
    "\n",
    "# Compare\n",
    "speedup = (standard_time / broadcast_time) if broadcast_time > 0 else 1\n",
    "print(f\"\\nBroadcast join speedup: {speedup:.2f}x\")\n",
    "print(f\"   Standard join: {standard_time:.2f}s\")\n",
    "print(f\"   Broadcast join: {broadcast_time:.2f}s\")\n",
    "\n",
    "# Save comparison\n",
    "join_comparison = pd.DataFrame({\n",
    "    'Join_Type': ['Standard', 'Broadcast'],\n",
    "    'Time_s': [standard_time, broadcast_time]\n",
    "})\n",
    "save_results(join_comparison, \"03_join_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a667494",
   "metadata": {},
   "source": [
    "# Performance Tuning & Monitoring\n",
    "\n",
    "L'ottimizzazione delle performance in PySpark si basa su quattro tecniche principali: **partitioning** per distribuire i dati in modo ottimale, **caching** per evitare ricomputazioni, **broadcast join** per ridurre lo shuffle, e **configuration tuning** per ottimizzare i parametri Spark.\n",
    "\n",
    "## Partitioning\n",
    "\n",
    "**Partition Size** - regola generale 128MB per partition (standard Hadoop):\n",
    "- Partizioni troppo piccole → overhead eccessivo\n",
    "- Partizioni troppo grandi → memory pressure e stragglers\n",
    "\n",
    "**Partition Count**:\n",
    "- Minimum: `num_cores * 2`\n",
    "- Optimal: `num_cores * 4`\n",
    "\n",
    "**Repartition vs Coalesce**:\n",
    "- `repartition(n)`: esegue shuffle, utile per aumentare/ridurre partizioni e load balancing\n",
    "- `coalesce(n)`: no shuffle, solo per ridurre partizioni, più veloce\n",
    "\n",
    "## Caching\n",
    "\n",
    "**Quando cachare**:\n",
    "- DataFrame usato multiple volte nella stessa sessione\n",
    "- Dopo operazioni costose (join, aggregazioni complesse)\n",
    "- Algoritmi iterativi (machine learning)\n",
    "\n",
    "**Quando non cachare**:\n",
    "- DataFrame usato una sola volta\n",
    "- Dati già ottimizzati in memoria (es. Parquet con columnar pruning)\n",
    "- Dataset troppo grande per la memoria disponibile (rischio spill to disk)\n",
    "\n",
    "## Spark UI\n",
    "\n",
    "Accessibile su **http://localhost:4040** durante una SparkSession attiva.\n",
    "\n",
    "**Tabs principali**:\n",
    "- **Jobs**: tutte le jobs eseguite, durata, stages, tasks, status success/failure\n",
    "- **Stages**: breakdown dettagliato di ogni job, metriche a livello task, shuffle read/write\n",
    "- **Storage**: RDD/DataFrame cached, utilizzo memoria, frazione cached vs spilled to disk\n",
    "- **Environment**: Spark properties, system properties, classpath\n",
    "- **Executors**: metriche per executor, memory usage, GC time, distribuzione task\n",
    "- **SQL**: execution plans (physical/logical), metriche per operatore, visualizzazione DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be1f2d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "OPTIMIZATIONS\n",
      "======================================================================\n",
      "\n",
      "Baseline Performance\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  Running baseline query...\n",
      "  Result:\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "|customer_segment|      category|num_transactions|       total_revenue|   avg_transaction|\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "|         Regular|   Electronics|         4548638|  9.87136232544999E9| 2170.179804471138|\n",
      "|      Occasional|   Electronics|         2598525| 4.463711123720002E9|1717.7864841477385|\n",
      "|             VIP|   Electronics|         1298281|3.7551539812400007E9| 2892.404634466653|\n",
      "|         Regular|        Beauty|        10132594|2.6933833683999906E9|265.81380527039676|\n",
      "|         Regular|       Fashion|         5984928|2.4484364331599975E9| 409.1003990624444|\n",
      "|         Regular|        Sports|         4306959| 2.095409849840001E9| 486.5172503011988|\n",
      "|         Regular|        Health|         5473316|1.4614029682600017E9| 267.0050419635924|\n",
      "|         Regular|Home & Kitchen|         6122928|1.3924833118100028E9|227.42114749838686|\n",
      "|      Occasional|        Beauty|         5789740|1.2181996686100001E9|210.40662769139894|\n",
      "|      Occasional|       Fashion|         3419609|1.1070541987799964E9| 323.7370701679626|\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "  Baseline completed:\n",
      "    Time: 33.86s\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Partitioning & Caching\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  Current partitions:\n",
      "    Transactions: 41\n",
      "    Customers:    12\n",
      "    Products:     1\n",
      "\n",
      "  Repartitioning transactions to 48 partitions & caching...\n",
      "    Repartitioned in     160.95s\n",
      "    New partitions:      48\n",
      "    Transactions cached: 100,000,000 row\n",
      "\n",
      "  Caching Customers & Products tables...\n",
      "    Customers cached: 1,000,000 rows\n",
      "    Products cached: 50,000 rows\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Optimized Performance\n",
      "----------------------------------------------------------------------\n",
      "  Running optimized query...\n",
      "  Result:\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "|customer_segment|      category|num_transactions|       total_revenue|   avg_transaction|\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "|         Regular|   Electronics|         4548638|  9.87136232544998E9|2170.1798044711363|\n",
      "|      Occasional|   Electronics|         2598525| 4.463711123720001E9| 1717.786484147738|\n",
      "|             VIP|   Electronics|         1298281|3.7551539812399993E9|2892.4046344666517|\n",
      "|         Regular|        Beauty|        10132594|2.6933833684000006E9| 265.8138052703977|\n",
      "|         Regular|       Fashion|         5984928| 2.448436433160001E9|  409.100399062445|\n",
      "|         Regular|        Sports|         4306959|2.0954098498399951E9|486.51725030119746|\n",
      "|         Regular|        Health|         5473316|1.4614029682599993E9|267.00504196359196|\n",
      "|         Regular|Home & Kitchen|         6122928|     1.39248331181E9|227.42114749838638|\n",
      "|      Occasional|        Beauty|         5789740|1.2181996686100008E9|210.40662769139908|\n",
      "|      Occasional|       Fashion|         3419609|1.1070541987799964E9| 323.7370701679626|\n",
      "+----------------+--------------+----------------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "  Performance Improvement:\n",
      "    Baseline: 33.86s\n",
      "    Optimized: 17.58s\n",
      "    Speedup: 1.93x\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZATIONS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Execute without caching\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "print(\"\\nBaseline Performance\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Baseline: Complex aggregation without optimization\n",
    "print(\"\\n  Running baseline query...\")\n",
    "start = time.time()\n",
    "result_baseline = transactions \\\n",
    "    .filter(col(\"status\") == \"completed\") \\\n",
    "    .join(customers, \"customer_id\") \\\n",
    "    .join(products, \"product_id\") \\\n",
    "    .groupBy(\"customer_segment\", \"category\") \\\n",
    "    .agg(\n",
    "        spark_count(\"*\").alias(\"num_transactions\"),\n",
    "        spark_sum(\"final_amount\").alias(\"total_revenue\"),\n",
    "        avg(\"final_amount\").alias(\"avg_transaction\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "print(\"  Result:\")\n",
    "result_baseline.show(10)\n",
    "baseline_time = time.time() - start\n",
    "\n",
    "print(f\"  Baseline completed:\")\n",
    "print(f\"    Time: {baseline_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Partitioning & Caching\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Check current partitions for each dataset\n",
    "print(\"\\n  Current partitions:\")\n",
    "print(f\"    Transactions: {transactions.rdd.getNumPartitions()}\")\n",
    "print(f\"    Customers:    {customers.rdd.getNumPartitions()}\")\n",
    "print(f\"    Products:     {products.rdd.getNumPartitions()}\")\n",
    "\n",
    "optimal_partitions = 48  # For local mode with 12 cores\n",
    "\n",
    "print(f\"\\n  Repartitioning transactions to {optimal_partitions} partitions & caching...\")\n",
    "start = time.time()\n",
    "transactions_optimized = transactions.repartition(optimal_partitions)\n",
    "transactions_optimized.cache()\n",
    "transactions_count = transactions_optimized.count()  # Trigger caching\n",
    "repart_time = time.time() - start\n",
    "\n",
    "print(f\"    Repartitioned in     {repart_time:.2f}s\")\n",
    "print(f\"    New partitions:      {transactions_optimized.rdd.getNumPartitions()}\")\n",
    "print(f\"    Transactions cached: {transactions_count:,} row\")\n",
    "\n",
    "# Cache frequently used tables\n",
    "print(\"\\n  Caching Customers & Products tables...\")\n",
    "\n",
    "# Cache customers (small, frequently joined)\n",
    "customers_optimized = customers.cache()\n",
    "customers_count = customers_optimized.count()  # Trigger caching\n",
    "print(f\"    Customers cached: {customers_count:,} rows\")\n",
    "\n",
    "# Cache products (small, frequently joined)\n",
    "products_optimized = products.cache()\n",
    "products_count = products_optimized.count()  # Trigger caching\n",
    "print(f\"    Products cached: {products_count:,} rows\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Optimized Performance\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Same query as baseline\n",
    "print(\"  Running optimized query...\")\n",
    "\n",
    "# Optimized: CACHED + REPARTITIONED + BROADCAST\n",
    "start = time.time()\n",
    "result_optimized = transactions_optimized \\\n",
    "    .filter(col(\"status\") == \"completed\") \\\n",
    "    .join(broadcast(customers_optimized), \"customer_id\") \\\n",
    "    .join(broadcast(products_optimized), \"product_id\") \\\n",
    "    .groupBy(\"customer_segment\", \"category\") \\\n",
    "    .agg(\n",
    "        spark_count(\"*\").alias(\"num_transactions\"),\n",
    "        spark_sum(\"final_amount\").alias(\"total_revenue\"),\n",
    "        avg(\"final_amount\").alias(\"avg_transaction\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "print(\"  Result:\")\n",
    "result_optimized.show(10)\n",
    "optimized_time = time.time() - start\n",
    "\n",
    "# Calculate improvement\n",
    "speedup = baseline_time / optimized_time if optimized_time > 0 else 1\n",
    "print(f\"\\n  Performance Improvement:\")\n",
    "print(f\"    Baseline: {baseline_time:.2f}s\")\n",
    "print(f\"    Optimized: {optimized_time:.2f}s\")\n",
    "print(f\"    Speedup: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30beb505",
   "metadata": {},
   "source": [
    "# ETL Pipeline & Delta Lake\n",
    "\n",
    "Una pipeline ETL completa prepara dati ML-ready sfruttando Delta Lake per garantire affidabilità e versioning.\n",
    "\n",
    "## Target Output per ML\n",
    "\n",
    "**ALS Data** (`user_item_interactions`) - per recommendation system:\n",
    "- Campi: `customer_id`, `product_id`, `rating`\n",
    "- Rating implicito derivato dal comportamento d'acquisto:\n",
    "\n",
    "    ```\n",
    "    rating = log(1 + purchase_count) * 2 + log(1 + total_spent) / 2\n",
    "    ```\n",
    "\n",
    "**Random Forest Data** (`customer_features`) - per classification:\n",
    "1. **Features Engineering**: \n",
    "    - **RFM**: Recency, Frequency, Monetary\n",
    "    - **Behavioral**: avg_transaction, unique_products\n",
    "    - **Temporal**: days_since_last_purchase\n",
    "    - **Demographic**: age, region\n",
    "\n",
    "2. **Labels**:\n",
    "    - Opzione 1: `customer_segment` (multiclass)\n",
    "    - Opzione 2: `is_churned` (binary)\n",
    "\n",
    "## Delta Lake: ACID su Data Lake\n",
    "\n",
    "Delta Lake è uno **storage layer costruito su Parquet** che aggiunge transazioni ACID, versioning e schema evolution ai data lake. Inoltre supporta sia batch che streaming in modo unificato.\n",
    "\n",
    "**Problemi dei Data Lake tradizionali**:\n",
    "- Assenza di transazioni ACID\n",
    "- Nessun versionamento dei dati\n",
    "- Inconsistenze nello schema\n",
    "- Impossibilità di rollback\n",
    "- Mancanza di audit trail\n",
    "\n",
    "**Soluzioni offerte da Delta Lake**:\n",
    "- **ACID transactions**: atomicità e consistenza garantite\n",
    "- **Time Travel**: accesso a versioni precedenti dei dati\n",
    "- **Schema enforcement & evolution**: validazione e evoluzione controllata dello schema\n",
    "- **Audit history**: transaction log completo di tutte le operazioni\n",
    "- **Unified batch & streaming**: stessa API per entrambe le modalità di elaborazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5eadaafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ETL PIPELINE FOR ML\n",
      "======================================================================\n",
      "Pipeline Goal: Prepare data for ML models in Notebook 4\n",
      "\n",
      "Target Models:\n",
      "  1. ALS (Recommendation System)\n",
      "     - Need: user_id, item_id, rating (implicit)\n",
      "  2. Random Forest (Classification)\n",
      "     - Need: customer features + label\n",
      "\n",
      "ETL Stages:\n",
      "  EXTRACT: Load raw data from MinIO\n",
      "  TRANSFORM: Create ML-ready features\n",
      "  LOAD: Save to Delta Lake tables\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Extract Phase\n",
      "----------------------------------------------------------------------\n",
      "  Loading data for ETL pipeline...\n",
      "    Transactions: 100,000,000 rows\n",
      "    Customers:    1,000,000 rows\n",
      "    Products:     50,000 rows\n",
      "\n",
      "  Extract phase completed!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Transform Phase - ALS Data Preparation\n",
      "----------------------------------------------------------------------\n",
      "  Creating user-item interaction matrix...\n",
      "  Created 73,881,272 user-item interactions\n",
      "\n",
      "  Sample interactions:\n",
      "+-----------+----------+------+--------------+------------------+\n",
      "|customer_id|product_id|rating|purchase_count|       total_spent|\n",
      "+-----------+----------+------+--------------+------------------+\n",
      "|  C00761177| P00000001| 12.64|            49|15189.010000000002|\n",
      "|  C00921997| P00000001| 12.62|            48|16005.279999999999|\n",
      "|  C00069906| P00000001| 12.61|            49|14248.029999999999|\n",
      "|  C00549215| P00000001| 12.52|            46|          15461.52|\n",
      "|  C00825262| P00000001|  12.5|            46|          14780.95|\n",
      "|  C00508643| P00000001|  12.5|            46|14861.980000000001|\n",
      "|  C00805320| P00000001| 12.47|            46|          13993.59|\n",
      "|  C00514843| P00000001| 12.46|            46|          13593.99|\n",
      "|  C00785674| P00000001| 12.45|            45|          14475.97|\n",
      "|  C00236578| P00000001| 12.45|            45|          14492.34|\n",
      "+-----------+----------+------+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Transform Phase - Random Forest Data Preparation\n",
      "----------------------------------------------------------------------\n",
      "  Creating customer features...\n",
      "  Created features for 1,000,000 customers\n",
      "\n",
      "  Sample features:\n",
      "+-----------+------------------+------------------+---------------------+---------------+---------------+------------------------+----------------------+---------------+----------------+---+---------+----------+\n",
      "|customer_id|total_transactions|total_spent       |avg_transaction_value|max_transaction|min_transaction|days_since_last_purchase|transactions_per_month|unique_products|customer_segment|age|region   |is_churned|\n",
      "+-----------+------------------+------------------+---------------------+---------------+---------------+------------------------+----------------------+---------------+----------------+---+---------+----------+\n",
      "|C00000004  |58                |20204.10999999999 |348.34672413793083   |3888.97        |16.54          |392                     |1.611111              |53             |Occasional      |35 |Molise   |1         |\n",
      "|C00000028  |54                |14402.44          |266.71185185185186   |2199.56        |20.76          |462                     |1.500000              |39             |Occasional      |26 |Sicilia  |1         |\n",
      "|C00000067  |46                |12190.349999999997|265.0076086956521    |1322.63        |15.45          |392                     |1.277778              |41             |Occasional      |60 |Veneto   |1         |\n",
      "|C00000125  |141               |60911.38          |431.9956028368794    |5031.89        |17.62          |381                     |3.916667              |101            |Regular         |28 |Liguria  |1         |\n",
      "|C00000136  |144               |55566.88000000002 |385.88111111111124   |5274.55        |8.27           |402                     |4.000000              |117            |Regular         |59 |Lombardia|1         |\n",
      "+-----------+------------------+------------------+---------------------+---------------+---------------+------------------------+----------------------+---------------+----------------+---+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "  Feature statistics:\n",
      "+-------+------------------+------------------+------------------------+----------+\n",
      "|summary|total_transactions|       total_spent|days_since_last_purchase|is_churned|\n",
      "+-------+------------------+------------------+------------------------+----------+\n",
      "|  count|           1000000|           1000000|                 1000000|   1000000|\n",
      "|   mean|         95.000239| 41156.41359173008|              392.088544|       1.0|\n",
      "| stddev| 65.15002093221534|38030.230792202296|      19.819113874170533|       0.0|\n",
      "|    min|                21|           3908.33|                     376|         1|\n",
      "|    max|               362|         264387.59|                     650|         1|\n",
      "+-------+------------------+------------------+------------------------+----------+\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Load Phase - Write to Delta Lake\n",
      "----------------------------------------------------------------------\n",
      "  Delta Lake paths:\n",
      "    ALS data: s3a://bigdata-ecommerce/ml_data/user_item_interactions\n",
      "    RF data: s3a://bigdata-ecommerce/ml_data/customer_features\n",
      "\n",
      "  1. Writing ALS user-item interactions to Delta...\n",
      "    Written in 65.38s\n",
      "\n",
      "2. Writing RF customer features to Delta...\n",
      "    Written in 69.27s\n",
      "\n",
      "  Data loaded to Delta Lake successfully!\n"
     ]
    }
   ],
   "source": [
    "# ETL PIPELINE\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ETL PIPELINE FOR ML\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"Pipeline Goal: Prepare data for ML models in Notebook 4\")\n",
    "print(\"\\nTarget Models:\")\n",
    "print(\"  1. ALS (Recommendation System)\")\n",
    "print(\"     - Need: user_id, item_id, rating (implicit)\")\n",
    "print(\"  2. Random Forest (Classification)\")\n",
    "print(\"     - Need: customer features + label\")\n",
    "\n",
    "print(\"\\nETL Stages:\")\n",
    "print(\"  EXTRACT: Load raw data from MinIO\")\n",
    "print(\"  TRANSFORM: Create ML-ready features\")\n",
    "print(\"  LOAD: Save to Delta Lake tables\")\n",
    "\n",
    "# Extract Phase\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Extract Phase\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"  Loading data for ETL pipeline...\")\n",
    "\n",
    "print(f\"    Transactions: {transactions_count:,} rows\")\n",
    "print(f\"    Customers:    {customers_count:,} rows\")\n",
    "print(f\"    Products:     {products_count:,} rows\")\n",
    "\n",
    "print(\"\\n  Extract phase completed!\")\n",
    "\n",
    "# Transform Phase - ALS Data Preparation\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Transform Phase - ALS Data Preparation\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Prepare user-item interactions for ALS\n",
    "# Use implicit feedback: number of purchases + total amount as rating\n",
    "print(\"  Creating user-item interaction matrix...\")\n",
    "\n",
    "user_item_interactions = (\n",
    "    transactions_optimized\n",
    "    .filter(col(\"status\") == \"completed\")\n",
    "    .groupBy(\"customer_id\", \"product_id\")\n",
    "    .agg(\n",
    "        spark_count(\"*\").alias(\"purchase_count\"),\n",
    "        spark_sum(\"final_amount\").alias(\"total_spent\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"rating\",\n",
    "        # Implicit rating: log-scaled purchase count + amount component\n",
    "        spark_round(\n",
    "            expr(\"log(1 + purchase_count) * 2 + log(1 + total_spent) / 2\"),\n",
    "            2\n",
    "        )\n",
    "    )\n",
    "    .select(\"customer_id\", \"product_id\", \"rating\", \"purchase_count\", \"total_spent\")\n",
    ")\n",
    "\n",
    "# Count and sample\n",
    "als_count = user_item_interactions.count()\n",
    "print(f\"  Created {als_count:,} user-item interactions\")\n",
    "\n",
    "print(\"\\n  Sample interactions:\")\n",
    "user_item_interactions.orderBy(desc(\"rating\")).show(10)\n",
    "\n",
    "\n",
    "# Transform Phase - Random Forest Data Preparation\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Transform Phase - Random Forest Data Preparation\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Create customer features for classification\n",
    "# Target: Predict customer segment or churn\n",
    "print(\"  Creating customer features...\")\n",
    "\n",
    "# Aggregate customer behavior\n",
    "customer_features = (\n",
    "    transactions_optimized\n",
    "    .filter(col(\"status\") == \"completed\")\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        spark_count(\"*\").alias(\"total_transactions\"),\n",
    "        spark_sum(\"final_amount\").alias(\"total_spent\"),\n",
    "        avg(\"final_amount\").alias(\"avg_transaction_value\"),\n",
    "        spark_max(\"final_amount\").alias(\"max_transaction\"),\n",
    "        spark_min(\"final_amount\").alias(\"min_transaction\"),\n",
    "        # Recency: days since last transaction\n",
    "        expr(\"datediff(current_date(), max(transaction_date))\").alias(\"days_since_last_purchase\"),\n",
    "        # Frequency: transactions per month (assuming ~3 years of data)\n",
    "        expr(\"count(*) / 36.0\").alias(\"transactions_per_month\"),\n",
    "        # Diversity: number of unique products\n",
    "        expr(\"count(distinct product_id)\").alias(\"unique_products\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join with customer demographics\n",
    "customer_features = customer_features.join(\n",
    "    customers_optimized.select(\"customer_id\", \"customer_segment\", \"age\", \"region\"),\n",
    "    \"customer_id\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Create label (for classification)\n",
    "# Option 1: Predict segment (multiclass)\n",
    "# Option 2: Predict churn (binary) - define churn as no purchase in last 180 days\n",
    "customer_features = customer_features.withColumn(\n",
    "    \"is_churned\",\n",
    "    when(col(\"days_since_last_purchase\") > 180, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "rf_count = customer_features.count()\n",
    "print(f\"  Created features for {rf_count:,} customers\")\n",
    "\n",
    "print(\"\\n  Sample features:\")\n",
    "customer_features.show(5, truncate=False)\n",
    "\n",
    "# Check feature statistics\n",
    "print(\"\\n  Feature statistics:\")\n",
    "customer_features.select(\n",
    "    \"total_transactions\", \"total_spent\", \"days_since_last_purchase\", \"is_churned\"\n",
    ").describe().show()\n",
    "\n",
    "# Load Phase - Write to Delta Lake\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Load Phase - Write to Delta Lake\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Define Delta paths on MinIO\n",
    "als_delta_path = get_s3a_path(\"ml_data/\", \"user_item_interactions\")\n",
    "rf_delta_path = get_s3a_path(\"ml_data/\", \"customer_features\")\n",
    "\n",
    "print(\"  Delta Lake paths:\")\n",
    "print(f\"    ALS data: {als_delta_path}\")\n",
    "print(f\"    RF data: {rf_delta_path}\")\n",
    "\n",
    "# Write ALS data\n",
    "print(\"\\n  1. Writing ALS user-item interactions to Delta...\")\n",
    "start = time.time()\n",
    "user_item_interactions.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(als_delta_path)\n",
    "als_write_time = time.time() - start\n",
    "print(f\"    Written in {als_write_time:.2f}s\")\n",
    "\n",
    "# Write RF data\n",
    "print(\"\\n2. Writing RF customer features to Delta...\")\n",
    "start = time.time()\n",
    "customer_features.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(rf_delta_path)\n",
    "rf_write_time = time.time() - start\n",
    "print(f\"    Written in {rf_write_time:.2f}s\")\n",
    "\n",
    "print(\"\\n  Data loaded to Delta Lake successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
